<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Seil Park" /></head>
<style>@import url(/public/css/syntax/monokai.css);</style>
  <title>Seil Park</title>
  <!-- <link href="/public/css/bootstrap.min.css" rel="stylesheet"> -->

  <link href="/public/css/style.css" rel="stylesheet">
  <body>
  	<div class="container"> 
		<div class="sidebar">
			<div class="sidebar-item sidebar-header">
	<div class='sidebar-brand'>
		<a href="/">Seil Park</a>
	</div>
	<p class="lead"></p></div>

<div class="sidebar-item sidebar-nav">
	<ul class="nav">
      <li class="nav-title" style="text-align: center;">Introduction</li>
	  <li>
	  	<a class="nav-item" href="/">CV</a>
	  </li>
	  <li>
		<a class="nav-item" href="/research">Research</a>
	  </li>
	</ul>
</div>

<div class="sidebar-item sidebar-nav">
  	<ul class="nav">
			<li class="nav-title" style="text-align: center;">Articles</li>
		
	    <li>
	    	<a class="nav-item" href="/category/#History">
				<span class="name">History</span>
				<span class="badge">5</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Market">
				<span class="name">Market</span>
				<span class="badge">10</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Miscellaneous">
				<span class="name">Miscellaneous</span>
				<span class="badge">3</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Process">
				<span class="name">Process</span>
				<span class="badge">11</span>
	    	</a>
 		</li>
	    
	  </nav>
	</ul>
</div>

<div class="sidebar-item sidebar-footer">
	<p>Powered by <a href="https://github.com/jekyll/jekyll">Jekyll</a></p>
</div>

		</div>
		<div class="content">
			<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="post">
	<header class="post-header">
		<div class="post-title"> 
			기업 - 팹리스 - 인공지능 반도체
		</div>
		<time class="post-date dt-published" datetime="2023-10-04T19:31:29+09:00" itemprop="datePublished">2023/10/04
		</time>		
	</header>

	<div class="post-content">
		<p>특정 연산에 특화된 하드웨어를 가속기(Accelerator) 또는 ASIC(Application Specific IC)라 부른다.
인공지능 반도체 팹리스 업체는 인공지능 연산에 특화된 가속기를 설계하는 업체다.</p>

<p>현재 인공지능의 문제: training을 할때 전력을 엄청나게 먹는다.
지금도 그런데, 우리 생활 속에 AI가 다 들어오면 전력소모가 진짜 감당 안될거다.
그래서 더 효율적인 연산을 하는 하드웨어가 필요하다.</p>

<p>현재 인공지능 연산은 대부분 엔비디아의 하드웨어에서 실행되고 있다.
인공지능도 모델이 여러가지인데,
내가 쓸 그 인공지능 모델 연산에서만 빠른 하드웨어가 있으면 좋을 것이다.</p>

<p>2016년에 알파고가 이세돌을 이겼다. 그 뒤로 인공지능이 계속 언급되며 발전했는데, 인공지능 학습을 위해 새로운 하드웨어들이 필요해졌다.<br /></p>

<p>인공지능 연산을 하는 하드웨어에는 몇가지가 필요하다.
메모리:
인공지능의 학습능력은 초당 얼마나 많은 자료를 학습시킬 수 있는가다.
그래서 고용량, 고대역폭 메모리가 필요하다.
메모리 회사들이 연구하던 HBM이 여기 적합했다.</p>

<p>핸드폰에서도 AI 연산을 한다. 안면인식, 지문인식에 인공지능 모델이 사용된다.
근데, 스마트폰은 PC와 환경이 많이 다르다.
엔비디아가 만들던건 대형 기판에서 100W 이상 먹는 거대한 칩들이라,<br />
핸드폰에 들어가는 NPU들은 엔비디아가 아니라 삼성전자, 애플 등 대형 스마트폰 제조사와 퀄컴같은 팹리스 업체들이 만들게 됐다.<br /></p>

<p><br />
CPU에도 정수 연산장치가 있긴 하지만, GPU 정수연산 성능이 압도적이다.<br />
<br />
모니터의 각 픽셀들은 서로에게 영향을 주지 않기 때문에, 1천만개 픽셀이 있다면 1천개씩 1만번 계산해도 된다.<br />
<br /></p>

<p>엔비디아는 개발자들이 엔비디아 제품을 쓰도록 생태계를 조성하기 위해 CUDA를 출시했다.<br />
CUDA는 개발자들이 GPU 기반 프로그래밍을 할 때, CPU 위 C언어 등에서 개발할 때같은 익숙한 느낌으로 개발할 수 있게 해놨다.<br />
CUDA 문법이 C 문법이랑 비슷하다.<br />
<br />
CUDA 덕분에, 프로그래머들은 자신이 어떤 칩을 사용하는지 신경쓰지 않아도 되게 되었다.<br />
프로그래머 입장에서는 엔비디아가 제조한 VGA만 사용하면 된다.<br />
일단 CUDA 기반으로 프로그램을 짜면, 이후에 GPU를 교체하더라도 추가 작업을 할 필요가 없게 된 것이다.<br />
<br />
CUDA 덕분에 코드 참조나 이직도 쉬워졌다.<br />
<br /></p>

<p>AI반도체 회사들이 많지만, 다들 집중하는 연산, 분야가 다 다르다.
한국의 유명한 AI반도체 회사 3개를 중심으로 예시를 들려고 한다.
리벨리온, 퓨리오사, 사피온
모빌린트, 딥엑스</p>

<p>tenstorrent</p>

<p>https://v.daum.net/v/20240207165405603
https://tenstorrent.com/thinking/</p>

<p>리벨리온은 throughput보다 latency에 집중한다</p>

<p>NPU에 필요한건 throughput, latency, efficiency</p>

<p>인공지능이 성능은 좋은데 느리더라. 연산량이 너무 많았다
그래서 리벨리온은 latency쪽으로 갔다.</p>

<p>미국 보면 데이터센터를 냉각을 위해 바다에 짓고 있다.
microsoft underwater data center</p>

<p>퓨리오사:
데이터센터, 인프라 영역에 집중하고 있다.
-&gt; power efficiency를 중점적으로 보고 있다</p>

<p>반도체보다도 AI를 더 중요하게 생각한다</p>

<p>Neural Scaling Law</p>

<p>hyperscaler 중심으로 AI가 먼저 도입되기 시작했다</p>

<p>tradeoff of programmability, performance and power efficiency
performance and power efficiency are hard problems,
but programmability is even harder</p>

<p>SW: Compiler
HW: Silicon Architecture</p>

<p>퓨리오사 warboy: 삼성전자 14nm공정, 컴퓨터비전 특화</p>

<p>MLPerf: 반도체 성능 대회</p>

<p>edge 영역, server 영역?</p>

<p>말해보카 OCR이 Warboy 쓰인다 한다
Chip의 안정성 평가가 중요했다.
AI 모델들이 보통 엔비디아 GPU에서 개발되다보니, 그걸 퓨리오사 칩 쪽으로 가져온느게 중요했다.
AI반도체는 테스트가 그렇게 어렵지는 않다. 모델이 잘 돌아가면 된다.</p>

<p>리벨리온:</p>

<p>설계는 대기업의 영역이라기보다, 스타트업의 엘리트비즈니스다.
미국만 봐도, 설계는 스타트업이 앞서나가고 대기업들이 따라가는 구조였다.</p>

<p>AI반도체면 AI도 알아야 하고 반도체도 알아야 하는데, 펀딩 따오기가 어려웠다</p>

<p>deep tech 회사들은 항상 시간이 부족하다. 경쟁자들이 계속 달리고 있다.</p>

<p>stac-ML은 월스트리트쪽에서 성능확인</p>

<p>Latency-Critical AI - Latency만큼은 정말 좋은 AI, HFT
커다란 규모의 상업화 - KT랑 함께 데이터센터도 도전중</p>

<p>LightTrader는 월스트리트에서 한번 써보고 있다
KT 언어모델들도 돌려보고 있다</p>

<p>RebelFLOW: 리벨리온의 소프트웨어 suite
그냥 쿠버네틱스 위에 올리면 바로 돌아간다? 고 한다
ML OPs?</p>

<p>시카고에서 선물데이터가 변하면 뉴욕에서도 현물데이터가 변한다</p>

<p>주가예측은 LSTM으로 하는데, 이걸 GPU로 돌린느것보다 빠르게 돌릴 수 있다</p>

<p>정보를 조금이라도 빠르게 가져오는 기술은 정말 중요하다.</p>

<p>리벨리온은 삼성전자 5nm공정을 썼다.
삼성전자의 배려로 test chip을 만들 수 있었다.
2024년 양산 계획이 있다.</p>

<p>엔비디아 DGX</p>

<p><br />
FPGA의 원래 목적은, 웨이퍼로 실물 칩을 만들기 전에 설계를 검증해보는거다.<br />
만약 칩 설계 단계에서 실수가 일어나게 되면 제조 공정을 통째로 엎어버려야 하기 때문에 실수의 대가가 크다.<br />
심할 경우에는, 제조사가 문제 원인을 알면서도 해결을 포기한다.<br />
하드웨어 한두개 크기가 바뀌어 배선도 바꿔야 하고, 결국 칩 형태가 크게 변해 칩 특성과 수율을 처음부터 다시 맞춰야 할 수도 있다.<br />
<br />
인텔도 멜트다운, 스펙터 결함 수정을 거부한 바 있다.<br />
<br />
어쨌든, 이런 설계 실수는 막아야 한다. 이를 위해 수많은 방법론들이 나왔다.<br />
일단 컴퓨터 시뮬레이션이 있다. 편하긴 한데, 문제는 시뮬레이션이 아주 느리다는 거다. 1초 보려고 며칠 돌리는 식이다.<br />
<br />
게다가, 칩의 동작은 그 칩에서 동작하는 소프트웨어와 큰 연관이 있다.<br />
그렇다고 이미 느린 시뮬레이션에 거대한 소프트웨어까지 올려서 시뮬레이션하면 더 느려져서 아무것도 확인 못한다.<br />
<br />
이건 팹리스 업체들에게 큰 부담이다. 팹리스 업체들이라고 해도 칩만 만들어 파는게 아니라, 해당 칩에서 사용 가능한 펌웨어까지는 같이 내놔야 하기 때문이다.<br />
근데 컴퓨터 시뮬레이션이 너무 느리면, 시뮬레이션으로 소프트웨어를 개발하거나 동작을 확인하는게 불가능하다.<br />
<br />
그렇다고 실제로 칩을 만들어봐? 이건 비싸고, 오래걸리고, 뭐 하나 수정하면 또 만들어봐야 하는데 그건 현실적으로 말이 안된다.<br />
<br />
그래서 FPGA를 쓴다. PC에서는 프로그램 형태로 시뮬레이션이 돌아가지만, FPGA에서는 실제 웨이퍼에서 로직이 동작하듯 돌아간다.<br />
물론 실제 칩보다는 느리긴 하지만, 그래도 이걸 써서 소프트웨어를 돌려보고, 구조를 바꿔서도 돌려보는 것으로 설계를 검증하고 펌웨어를 개발할 수 있다.<br />
하지만 가격이 CPU보다 비싸다.<br />
<br />
원래 FPGA는 이렇게 칩 시뮬레이션을 위해 쓰는 것이었는데, 이걸 AI 연산 등 특정 연산을 위해 쓰려는 사람들이 나타났다.<br />
<br />
2017년, 중국계 채굴 업체 BitMain은 채굴 전용 칩 Antminer를 설계해, 1600W 전력으로 14TH/s를 달성했다.<br />
엔비디아 GPU를 8개 연결한 채굴기들은 비슷한 전력으로 1GH/s밖에 달성하지 못했다. 1만배가 넘는 차이다.<br />
<br />
그럼 무조건 ASIC(Application Specific Intergrated Circuit)이 이득인가?<br />
위험할 수 있는게, 채굴에 필요한 알고리즘이 변하면 원래 쓰던 ASIC를 쓸 수 없게 된다.<br />
이더리움도 채굴용 하드웨어, 전용 칩 사용을 막기 위해 알고리즘을 바꾼 적이 있다.<br />
<br />
그래서, 하드웨어 수정이 필요할 수도 있는 곳에는 FPGA를 쓴다. 아예 ASIC로 만들면 수정이 안되니까.<br />
전무님이 Hardwire로 구워버리면 싸다고 한게 이 얘기인가?<br />
결국 FPGA가 쓰이게 된건 ASIC 대신이고, ASIC는 GPU보다 특정 연산을 위해 쓰는거였다.<br />
<br />
결국, 컴퓨팅 성능 발전이 느려지고 AI가 대두되어 GPU를 많이 쓰는 거였으니,<br />
컴퓨팅 성능 발전이 느려져 FPGA를 쓰게 됐다고도 볼 수 있다.<br />
<br />
클라우드 서비스가 나오면서(아마존 AWS, 마이크로소프트 Azure), 기업들이 모두 HPC(고성능컴퓨터)를 살 필요 없이 클라우드를 이용하면 되게 되었다.<br />
<br />
RDIMM(고성능 DRAM 모듈)의 수요가 증가했고, 서버용 SSD 수요가 늘어나 낸드플래시 매출이 올라갔다.<br />
메모리 회사들도 돈벌고, 가상화 호스팅 업체들도 돈 많이 벌었다.<br />
<br />
휴대용 디바이스 수요 증가-&gt; 그거에 맞는 칩 설계하는 팹리스 성장 -&gt; 파운드리 매출도 성장<br />
<br />
전통적인 CPU 성능 증가에 한계, AI 등 응용프로그램 등장 -&gt; GPU, FPGA 등 특정 연산에 강점을 보이는 칩들이 연산용 반도체의 주도권을 갖게 됐다.<br />
<br />
기존 회사들은 CPU에 최적화된 프로그램을 버리고, 전용 가속기에 맞는 프로그램을 설계하게 됐다.<br />
<br />
메모리 구조가 바뀔 필요는 없었다. 메모리 회사들은 그냥 팔면 됐고,<br />
대규모 병렬 처리가 많이 필요해지면서 HBM같은 고부가가치 고성능 메모리에 대한 수요가 생겼다.<br />
<br />
전통적인 로직 반도체 회사들은 모바일 플랫폼 발전+GPU 수요 상승에 편승 못하고 매출에 타격을 입었지만,<br />
AWS 등 서버 서비스가 확장되어 서버용 CPU를 팔며 이득을 보긴 했다.<br />
<br />
그럼에도, 인텔은 ARM 서버들의 도전을 모두 물리치고 여전히 최강자로 군림하고 있다.<br />
여전히 기업 서버 시장에서 인텔을 대체할 회사는 없다.<br />
<br />
인텔은 FPGA 회사 Altera를 인수했다. 이게 반격의 실마리가 될 수도 있다.<br />
어쩌면 FPGA도 가상화하여 전세계에 임대하는 사업 모델을 만들어볼 수도 있을 것이다.<br />
이렇게 되면 칩 설계 회사들도 앱 개발 회사들처럼 작은 규모로도 운영할 수 있게 될거다.</p>

<p>스마트폰에서 돌아가는 ARM 기반 프로그램들은 x86환경에서 크로스 컴파일러를 이용해 만들어진다.<br />
저전력 환경에서 돌아가는 수많은 프로그램들은 저전력 환경에서는 못만드는 것이다.<br />
<br />
ARM의 모바일 GPU는 성능, 시장 점유율에서 퀄컴, 파워VR을 이기지 못하고,<br />
AMD의 GPU 설계인 RDNA는 삼성전자와의 협업을 통해 모바일에 진출하려 한다.<br />
<br />
결국 ARM은 모바일 CPU 말고는 제대로 먹은 시장이 없는데, 스마트폰 출하량은 감소하기 시작했다.<br />
그리고 퀄컴, 화웨이, 삼성전자같이 AP 대부분을 스스로 설계하는 회사들 비중이 커지면 ARM도 위험해질 수 있다.<br />
ISA만 라이센싱하는건 설계 자체를 라이센싱하는것보다 훨씬 이익이 적다.<br />
<br />
ARM은 어쨌든 새로운 시장을 찾아내야 하는 상황이다.<br />
그래도 ARM은 자체 칩을 제조하지 않기 때문에 경쟁하기보다는 협력하려는 회사가 많다.<br />
엔비디아가 ARM을 합병하려 하기도 했었는데, 잘 안된 것으로 알고 있다.<br />
<br />
원래 GPU는 프로그램 수행의 결과대로 움직여 영상을 띄워주는 정도의 일을 했다.<br />
그러다가 CPU 성능 향상이 한계에 다다르고, 대안으로 떠오른 기계학습이 주목받게 됐다. 그런데 GPU가 기계학습에 적합했다.<br />
CPU도 4~20개 등 다중코어 시스템이 되긴 했지만, 여전히 거대한 디코더와 비순차 수행기 등 거대한 하드웨어가 필요했다.<br />
<br />
엔비디아가 행운을 공짜로 얻은건 아니고, 오래 전부터 GPU로 대규모 연산이 필요한 시장에 들어가려는 노력을 해 왔다.<br />
<br />
엔비디아는 2008년 Ageia라는 물리연산 가속기 전문 회사를 인수해 대규모 물리학 시뮬레이션이 필요한 분야에 진출하려고 했다.<br />
물리 연산도 특정 정지된 시간에 공간상에 표시된 물체들의 속도 등을 각자 따로 계산하면 되는 작업이기 때문이다.<br />
<br />
CUDA는 2006년에 처음 출시됐다. 엔비디아 그래픽카드기만 하면 쓸 수 있었고, 물리학 시뮬레이션과 인코딩/디코딩 등에 쓰였다.<br />
그러다가 AI가 주목받자 CUDA가 더 중요해졌고, 엔비디아는 GPU를 GPGPU라고 재명명하기도 했다.<br />
GPU는 단순히 모니터에 그림을 띄우는 칩이 아니라는 엔비디아의 선언이었다.<br />
<br />
GPU 경쟁사로는 CPU와 GPU 둘 다 하는 AMD 정도가 있었는데, AMD는 CPU를 말아먹고 죽어가던 상태였다.<br />
<br />
인텔, ARM도 GPU에서 엔비디아를 이길 수 없었다. ARM은 대형 하드웨어를 두고 고성능 컴퓨팅을 시도해본 적이 없었고,<br />
인텔은 Larrabee라는 x86 기반 병렬 프로세서를 개발하려 했지만, 제대로 된 성능을 내지 못했다.<br />
<br />
인텔의 그래픽카드는 2017년 라자 코두리가 AMD에서 옮겨오고 나서야 개선되기 시작했고,<br />
2020년 들어서야 쓸만한 물건이 나오게 된다.<br />
<br />
이렇게 엔비디아는 그래픽카드에 있어 독보적인 위치를 갖게 되었으며, 돈을 많이 벌어 TSMC의 최첨단 공정을 쓰게 됐다.<br />
<br />
GPU 가격은 100~200만원 정도인데, 머신러닝 전용 카드들의 가격은 천만원이 넘는다.<br />
그리고 이제는 단일카드만 파는게 아니라, 일종의 소형 슈퍼컴퓨터같은 형태로 팔기도 한다.<br />
여러개의 엔비디아 A100 카드를 엮어 만드는 엔비디아 DGX A100의 초기 출시가는 19만9천달러였다.<br />
근데 이런 비싼 제품들도 늘 공급부족이다.<br />
<br />
이런 가격대가 가능한건, 머신러닝이 엄청난 부가가치를 창출해 여기 참여한 회사들의 매출, 실적이 폭등해 부자가 되었기 때문이다.<br />
그러니 비싼돈 내고 이런것들을 사간다.<br />
<br />
엔비디아는 기존 PC 게이밍 시장은 그대로 유지한 채로, 슈퍼컴퓨팅, 서버, 자율주행차 등 수많은 분야들을 차지했다.<br />
Tegra라는 모바일 시장용 칩은 실패하긴 했다.<br />
<br />
근데 Tegra는 아예 뒤진건 아니고, 모바일 시장 말고 인공지능 에지(Edge AI) 시장을 개척하고 있다.<br />
엔비디아의 머신러닝 솔루션인 Jetson 시리즈의 칩으로 사용된 것이다.<br />
<br />
현재 Tegra는 초저전력부터 고성능까지 다양한 형태로 설계되어,<br />
전자는 머신러닝 입문자들이 좋아하는 Jetson 나노에,<br />
후자는 자율주행에도 사용 가능한 고급형 모델인 Xavier에 투입하고 있다.<br />
<br />
그리고, 지금 엔비디아가 갖춘 수많은 개발 인프라와 개발자 집단, 하드웨어 성능을 따라잡는 회사가 나오기는 힘들어 보인다.<br />
직업 시장에는 CUDA 프로그래밍을 전문으로 하는 프로그래머들이 이미 많이 자리잡았다.<br />
<br />
이 지위를 유지하기 위해 엔비디아는 지속적으로 CUDA에 새로운 기능들을 도입하고 있으며,<br />
Jetson Nano같이 CUDA를 사용하는 개발자 보드를 개발해 초보자들이 계속 들어오도록 만들고 있다.<br />
Jetson Nano는 카메라까지 사도 20만원이면 살 수 있다고 한다.<br />
<br />
엔비디아는 Jetson Nano에 약 10줄정도 코드만 짜면 이미지 인식 등을 시킬 수 있도록 ‘Jetson Inference’의 예시를 무료로 제시하고 있다.<br />
우리와 함께하면 인공지능 쉽다! 같은 느낌이다.<br />
<br />
하지만 엔비디아에게도 위협은 존재한다. 엔비디아의 새로운 상품들은 대부분 GPGPU를 중심으로 하고 있으며,<br />
필요한 경우 기존 GPGPU 근처에 ARM 프로세서를 결합하여 제어 능력을 부여한 것이 대부분이다.<br />
엔비디아는 이런 소형 프로세서의 자체 설계를 갖고 있지 않다.<br />
<br />
그래서, 만약 인텔같은 거대한 기업이 고성능 로직 프로세서와 기존 x86 생태계에 FPGA나 GPGPU를 결합하고,<br />
이를 통해 강력한 부가가치를 만들어내기 시작하면 엔비디아에게 큰 위협이 될 수 있다.<br />
<br />
지금은 GPU 분야에서 힘을 쓰지는 못하고 있지만, x86의 2인자 AMD도 엔비디아에게 위협이 될 수 있다.<br />
<br />
아니면 구글같은 강력한 소프트웨어 기업이 알파고에서 했던 것처럼 자체 가속기를 설계해 사용하고,<br />
나아가 해당 가속기에 맞는 소프트웨어 환경을 구축하기 시작할 수도 있다.<br />
이렇게 되면 엔비디아는 고객도 잃어버리고 생태계 주도권도 잃어버리게 된다.<br />
<br />
엔비디아 역시 텐서 연산기를 칩에 내장하는 등의 방식으로 성능 우위를 유지하려 한다.<br />
하지만, 결국 필요한 연산의 종류를 결정하는건 소프트웨어 기업이라 언제나 한발 늦게 될 수도 있다.<br />
<br />
그래서 엔비디아가 ARM 인수를 시도했을 것이다.<br /></p>

<p>삼성전자도 소프트웨어는 제대로 못만들고 있다.
스마트폰용 바다OS는 시장에 제대로 나가보지도 못하고 사라졌다.
타이젠은 일부 스마트워치 제품들에서만 라이센스 비용 절약 목적으로 사용된다.
원래 목적이었던 사물인터넷 진출이나 디바이스간 연결은 제대로 안되고 있다.</p>

<p>삼성 그룹으로 보면 자체 브랜드 아파트까지 있기 때문에, 집-가전제품-스마트폰 연결성을 실현하기에 좋은 환경을 갖고 있다.
하지만 제대로 못해내고 있다.</p>

<p>빅스비도 제대로 안되고 있다.
OS, AI 모두 시장 초기 진입이 중요한 소프트웨어들이다보니, 삼성전자의 fast follower 전략의 한계일 수도 있다.</p>

<p>소프트웨어 개발자는 OS를 통한 유통망을 쓰게 된다. 소프트웨어 개발자 혼자 유통망을 만들기는 너무 어렵다.
개발자는 안드로이드, iOS에서만 잘 작동하게 만들면 유통은 플레이스토어, 앱스토어에서 해준다.</p>

<p>새로운 OS를 만들어서 여기 끼어들려고 해도, 소프트웨어 개발자들은 얼마나 버틸지도 모르는 새로운 OS를 위해 소프트웨어를 만들 이유가 없다.</p>

<p>AI는 쌓인 데이터가 그 성능을 결정하게 된다.
신경망 구조에 대한 연구는 거의 끝났고, 이제는 학습용 데이터의 양과 질이 중요하다.
특히 구글은 검색 엔진을 다 먹어버렸기에, 데이터에서는 경쟁자들이 구글을 따라가기 어렵다.</p>

<p>구글은 CAPTCHA(로봇이 아닙니다)같은 무료 보안 프로그램, QuickDraw같은 놀이용 프로그램으로 AI 능력을 향상시키고 있다.
이렇게 전세계 사람들이 구글의 AI성능을 올려주고 있는데, 삼성전자가 AI 인력 몇명을 채용해봐야 이걸 뒤집기는 어려울거다.</p>

<p>대리인 문제: 일을 시키는 사람과 일을 하는 사람 사이 정보 비대칭으로 생기는 업무 효율성 감소, 감시비용 증가
구글은 대리인 문제도 없이 AI를 키우고 있다.</p>

<p>그래도, 삼성전자는 OS, AI를 제외한 애플리케이션들에서는 성과가 있다.
삼성전자는 일반적인 어플 개발 회사들과는 달리, 강력한 하드웨어적 지원이 가능하다.</p>

<p>설계 분야에서, 모바일 분야에서는 중국이 이미 상당한 수준에 올라와 있다.
대표적인 회사는 화웨이의 자회사인 HiSilicon으로, 이미 Kirin이라는 자체 AP도 갖고 있다.
다만, 얘네는 삼성전자 퀄컴과 다르게 자체 ARM 코어 설계를 만들지는 않고, ARM이 제공하는 코어 설계를 쓴다.
그래서 칩 판매당 순이익이 더 적을 것으로 추정된다.</p>

<p>HiSilicon은 하이엔드부터 로우엔드, 최근에는 서버까지 커버할 정도의 라인업을 갖고 있다.
Kirin이라는 AP, Kunpeng이라는 서버 CPU, Ascend라는 AI 가속기까지 있다.
이렇게 정말 다양한 칩들을 설계하고 있다.</p>

<p>설계가 어려운 일이긴 하지만, 그렇다고 공장처럼 대규모 투자가 필요한 일은 아니니까.</p>

<p>스마트폰이 PC보다 안좋은 점 중 하나는 입력장치다. 그래서 비밀번호 입력 등이 PC보다 힘들다.<br />
그래서 스마트폰은 개인 인증을 위해 홍채, 지문 등 생체 인식과 영상, 음성 인식 등 AI 인식을 도입했다.<br />
이런 인증에서 중요한건 반응 속도와 정확도인데, 스마트폰 회사들은 AI 연산 가속용 칩은 NPU를 탑재해 전력을 아끼면서도 인식이 잘 되도록 했다.<br /></p>

<p>NPU:<br />
Machine Learning은 인공지능의 한 분야고, Deep Learning은 Machine Learning에서 활용되는 알고리즘이다.<br />
<br />
요즘 인공지능 반도체라고 NPU라는게 나오는데, 왜 GPU를 두고 NPU라는걸 새로 만든걸까?<br />
1. 내장 AI 알고리즘, 2. 데이터 전송 속도, 3. 가격<br /></p>

<p>GPGPU로는 연산만 가능하고, 연산 결과를 분석하려면 별도의 소프트웨어가 필요하다.
그리고 AI연산을 위한 고속 데이터 전송 구조(아키텍처)가 없어 속도가 느리다.</p>

<p>그래서, AI 알고리즘이 탑재되어있고, 대규모 병렬 연산 후 고속 데이터 전송까지 가능한 NPU가 만들어지게 된다.
AI 알고리즘이 회로로 구현되어 있어 속도가 빠르다.</p>

<p>가격 관점에서는, GPU가 갖고 있던 그래픽 처리 관련 하드웨어를 다 버려서 그만큼 경제적이 되었다.</p>

<p>NPU는 다수의 CPU가 한 칩에 탑재된 구조다. 각 CPU는 독립적으로 연산을 처리하면서도 서로의 연산 결과에 영향을 준다.</p>

<p>NPU 만드는 기업으로는 애플, 화웨이, 삼성전자, 퀄컴 등이 있다.
다들 AP 만드는 업체들인데, 지들 AP 안에 NPU를 넣어 성능을 테스트함과 도시에 데이터를 수집하고 있다.</p>

<p>시리, 빅스비, 구글 어시스턴트 모두 AP에 NPU 들어간 후 성능이 향상됐고,
카메라 자동보정도 NPU 도입 이후 성능이 좋아졌다.</p>

<p>FPGA:
FPGA에는 코어의 구조를 원하는 대로 프로그래밍할 수 있는 프로그래머블 코어가 쓰인다.
그래서 코어의 구조를 연산에 맞게 최적화된 형태로 만들 수 있다.</p>

<p>코어가 연산에 맞게 최적화되면 명령어 해석 단계가 짧아진다.
그래서 특정 연산에서 FPGA가 CPU보다 빠를 수 있다.</p>

<p>FPGA는 특정 용도로 사용하기 위한 반도체를 개발할 때, 최적의 설계를 찾기 위해 반도체 개발자들이 많이 사용한다.</p>

<p>하드웨어 가속: 소프트웨어를 거치지 않고 하드웨어에서 바로 처리하는 것</p>

<p>ASIC:
FPGA를 통해 특정 상황에 최적화된 반도체를 설계했다면, 그 형태를 고정해서 대량생산하는 것으로 제작 단가를 낮출 수 있다.
이런 방식으로 만든 반도체가 ASIC다.</p>

<p>ASIC는 구성만 보면 CPU랑 비슷해 보이지만, 특정 연산만 수행하도록 만들어졌다는 차이가 있다.</p>

<p>ASIC는 특화된 기능만 수행하기에, 동작 속도가 빠르고, 크기가 작고, 소비전력이 적다.
물론 CPU에 비해 범용성은 매우 떨어진다.</p>

<p>그래서 ASIC가 코인 채굴에 많이 쓰였다.</p>

<p>2021년 기준으로, FPGA 시장은 약 8조원(69.6억 USD)규모로 전체 시스템 반도체 시장의 2.3%를 차지했다.</p>

<p>FPGA 1위 기업은 점유율 50%를 먹은 Xilinx고, 2위는 업계 2위였던 Altera를 2015년 인수한 인텔로, 30~40% 점유율을 차지하고 있다.</p>

<p>그리고 2020년에는 AMD가 Xilinx를 인수했다.</p>

<p>ASIC는 어떤 기업이든 지들 필요한 대로 알아서 만들어 쓰는거라, 대표 기업을 말하기 어렵다.</p>

<p>AI반도체는 현재 1세대, 2세대, 3세대로 구분한다.
1세대: GPU 병렬연산능력 + CPU, 소프트웨어로 AI알고리즘 구현
2세대: GPU에서 필요없는 기능 빼고, 하드웨어로 AI 알고리즘 구현
3세대: 뉴로모픽(Neuromorphic) 반도체</p>

<p>CPU는 데이터들을 순차적으로 처리하는데, 이러면 2차원 정보에 대한 패턴 분석이나, 실시간 데이터 처리같은 영역에서 데이터 처리 지연이 발생한다.</p>

<p>그래서 신경망을 모방해, 메모리 기록과 데이터 연산을 동시에 진행하는 뉴로모픽 반도체가 연구되고 있다.</p>

<p>최근에는 memristor를 뉴로모픽 반도체에 적용해 소비전력을 획기적으로 줄이려고 하고 있다.</p>

<p>memristor는 여기서는 메모리 특성을 갖는 저항 소자를 말한다.</p>

<p>FPGA = Field Programmable Gate Array
ASIC = Application-Specific IC</p>

<p>AI에 중요한건 데이터, 알고리즘, 연산력이다. 이 중 중국이 가지지 못한건 연산력이다.<br />
연산력에 쓰이는 하드웨어 다 미국이 설계하고 대만이 만들기 때문이다.<br /></p>

<p>6G: AI가 되게 중요하게 여겨지고 있다</p>

<p>카이스트 인공지능반도체 대학원:</p>

<p>혁신 기초 연구</p>

<p>Processing-in-Memory (PIM) 연구
PIM을 통해 기존 폰 노이만 프로세싱이 가지고 있던 memory bottleneck 문제를 해결하여 저전력으로 높은 성능을 달성할 수 있게 되었다.
SRAM의 cross-coupled 셀 구조를 사용하여 높은 연산 처리 속도를 가지는 SRAM-PIM과 DRAM의 높은 셀 집적도를 이용하여 높은 면적 효율을 가지는 DRAM-PIM을 설계한다.</p>

<p>뉴로모픽 시스템 연구
뉴로모픽 시스템은 인간의 뇌 구조와 동작을 모방한 시스템으로, 기존의 폰 노이만이 가지고 있는 순차적인 프로세싱, 연산기와 메모리의 분리, clock-driven operation이라는 특징 대신 병렬 프로세싱, 연산기와 메모리의 통합, event-driven operation이라는 특징을 가진다.
Event-based gesture recognition, always-on Internet-of-Things device, health care 등 저전력이 요구되는 application에 응용될 수 있는 뉴로모픽 시스템을 설계한다.</p>

<p>차세대 Non-Volatile Memory 기반 시스템 연구
비휘발성 메모리 중에서 비휘발성 자기 메모리(MRAM)는 터널 자기저항(Tunneling Magneto Resistance, TMR) 효과를 이용하여 자기터널접합(Magnetic Tunnel Junction, MTJ)에 데이터를 저장한다.
MRAM PIM을 기반으로 한 event-driven SoC를 설계한다.</p>

<p>메모리반도체 사이클:
일단 시스템반도체는 종류가 다양하고, 사용처가 정해져 있다.
PC에는 CPU, 스마트폰에는 AP, 채굴/인공지능에는 GPU, 전력관리에는 PMIC 이런 식으로 다 정해져 있다.
그리고 생산 전에 사용처와 계약을 먼저 체결하고, 사용처가 원하는 대로 제품을 만들어준다.</p>

<p>근데, 메모리반도체는 DRAM, NAND플래시 두가지밖에 없고, 규격화도 되어 있어 아무거나 끼워 쓰면 된다.
그래서 만드는 쪽에서도 일단 생산을 한다. 그 다음에 어떻게든 파는 식이다.
그렇기에 많은 재고를 안고 가게 되고, 가격 변동성에 크게 노출된다.
이게 메모리반도체 사이클의 원인이다.</p>

<p>시스템반도체의 경쟁력: 특화기업
메모리반도체의 경쟁력: 규모의 경제</p>

	</div>
</article>
		</div>
	</div>
  </body>
</html>