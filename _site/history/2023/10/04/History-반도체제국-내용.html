<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Seil Park" /></head>
<style>@import url(/public/css/syntax/monokai.css);</style>
  <title>Seil Park</title>
  <!-- <link href="/public/css/bootstrap.min.css" rel="stylesheet"> -->

  <link href="/public/css/style.css" rel="stylesheet">
  <body>
  	<div class="container"> 
		<div class="sidebar">
			<div class="sidebar-item sidebar-header">
	<div class='sidebar-brand'>
		<a href="/">Seil Park</a>
	</div>
	<p class="lead"></p></div>

<div class="sidebar-item sidebar-nav">
	<ul class="nav">
      <li class="nav-title" style="text-align: center;">Introduction</li>
	  <li>
	  	<a class="nav-item" href="/">About</a>
	  </li>
	  <li>
		<a class="nav-item" href="/research">Research</a>
	  </li>
	</ul>
</div>

<div class="sidebar-item sidebar-nav">
  	<ul class="nav">
			<li class="nav-title" style="text-align: center;">Articles</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Electromagnetics">
				<span class="name">Electromagnetics</span>
				<span class="badge">3</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Circuits">
				<span class="name">Circuits</span>
				<span class="badge">4</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#History">
				<span class="name">History</span>
				<span class="badge">21</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Market">
				<span class="name">Market</span>
				<span class="badge">10</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Miscellaneous">
				<span class="name">Miscellaneous</span>
				<span class="badge">11</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Process">
				<span class="name">Process</span>
				<span class="badge">12</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Time-Frequency">
				<span class="name">Time-Frequency</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	  </nav>
	</ul>
</div>

<div class="sidebar-item sidebar-footer">
	<p>Powered by <a href="https://github.com/jekyll/jekyll">Jekyll</a></p>
</div>

		</div>
		<div class="content">
			<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="post">
	<header class="post-header">
		<div class="post-title"> 
			반도체제국 내용
		</div>
		<time class="post-date dt-published" datetime="2023-10-04T19:31:29+09:00" itemprop="datePublished">2023/10/04
		</time>		
	</header>

	<div class="post-content">
		<p>HiSilicon: 화웨이의 칩 설계 회사<br />
<br />
메모리 회사: 어느 정도 용량의 메모리가 가장 수요가 많을지, 어느 정도 전력 소비까지 고객이 감내할 수 있는지 확인<br />
<br />
인텔같은 로직 회사: 고객이 어느 정도의 CPU 성능을 요구할지, 노트북 고객들은 어느 정도의 배터리 수명을 요구하고 CPU로부터 어느 정도의 전력 및 발열까지 감내할 수 있는지를 조사한다.<br />
<br />
D램은 사용자로부터 명령과 주소를 받는 부분, 주소를 해독하는 부분, 데이터 저장소, 읽어온 데이터를 잠시 보관해두는 latch로 비교적 간단하게 구성된다.<br />
<br />
D램은 설계 기술이 상대적으로 적게 필요하고, 대신 면적의 대부분을 차지하는 데이터 저장소의 면적을 줄이는게 중요하다.<br />
<br />
현대 CPU들은 다층의 캐시메모리, 디코더, ROB, 대기소, 연산포트, ALU등 다양한 기능을 하는 많고 작은 하드웨어들로 연결되어 있다.<br />
<br />
chip의 설계를 도와주는 EDA, 설계를 제조기술에 맞춰주는 PDK<br />
<br />
하드웨어 버그 예시로는 인텔 CPU의 보안 결함?<br />
<br />
테스트 후에는 원래 패키징해서 팔지만, 공간이 중요해지며 아예 웨이퍼 단위로 사고팔기도 한다.<br />
<br />
요즘은 여러개 CPU나 CPU+GPU 등의 구조로 Heterogeneous 칩을 고속 인터커넥트로 연결해 한 패키지로 팔기도 한다.<br />
<br />
Dhrystone: 1984년에 제안된 컴퓨터 성능 측정의 지표<br />
<br />
인텔 11900K는 1초에 4,110억개 명령어를 처리한다(Dhrystone 기준)<br />
<br />
컴퓨터: CPU, 메모리, 보조기억장치(HDD, SSD, USB메모리 등)<br />
<br />
반도체 제조업이 다른 제조업드로가 구분되는 가장 큰 차이점은 기술력이 미치는 영향이 막대하다는 점이다.<br />
<br />
1980년 1MB 메모리 가격: 6480달러<br />
2015년 1MB 메모리 가격: 0.0042달러 (백만배정도 싸졌다)<br />
다른 제조업들은 이정도로 못줄인다<br />
<br />
Dennard Scaling: 같은 면적에 집적된 트랜지스터는 전력 소모량이 같다.<br />
그래서 기술이 앞서는 회사는 전력소모 특성도 앞서나간다. 같은 면적 안에 트랜지스터가 100개든 100만개든 전력 소모는 같으니까.<br />
<br />
그리고 반도체는 다른 제조업에 비해 부피가 작고 부가가치가 높은 편이라서, 한 회사가 만들어 전 세계로 수출할 수 있다.<br />
그래서 빵집마냥 다른 동네로 도망가서 장사하는게 안된다. 전세계랑 경쟁해야된다.<br />
<br />
제조원가에서 설비 투자 비용이 차지하는 비중이 압도적으로 크다는 점도 다른 제조업들과 다르다.<br />
<br />
그래서 매년 엄청난 돈을 장비에 써야 한다. 즉, 고정비용이 크다.<br />
고정비용이 크다는건, 그 해 생산을 안하더라도 아낄 수 있는 비용이 얼마 안된다는 뜻이다.<br />
<br />
게다가, 반도체 공장은 공장 재가동에 시간이 오래 걸린다.<br />
<br />
일반적으로 반도체는 포토마스크 한장을 처리하는 데에 하루 이상이 걸리며,<br />
구조가 간단하다는 DRAM도 2000년에 이미 20장 넘는 마스크를 사용했다.<br />
<br />
2013년에는 DRAM 웨이퍼 한장이 필요로 하는 마스크 수가 40장을 넘었다. 그만큼 오래 걸린다.<br />
<br />
결국, 제품 가격이 떡락했다고 해도, 생산을 멈추면 손해다. 변동비용이 매출액보다 커지면 그때는 생산을 멈춰야 하겠지만, 반도체 생산에서 변동비용은 상당히 작다. 고정비용이 그만큼 커서 그렇다.<br />
<br />
여기서 말하는 변동비용은 인건비, 웨이퍼 가격, 재료 가격<br />
<br />
그럼 제품 가격이 떡락하면 회사를 팔아버리는건? 회사마다 공장에서 쓰는 장비가 많이 달라서 어렵다.<br />
삼성전자는 지금까지 단 한번도 메모리 회사를 인수한 적이 없다.<br />
<br />
그래서, 물량으로 시장 점유율을 가져왔더라도, 가격이 떡락하면 떡락을 막을 수가 없다.<br />
어떻게어떻게 버텨서 살아남으면? 바닥회사들이 죽어서 싸움이 끝난건데, 이 경우 1등 기업과 겨우 살아남은 회사의 사정은 크게 차이난다. 기술력에 의한 원가 절감 차이가 엄청나니까. 그래서 1등을 절대 못따라잡는다.<br />
<br />
그래서 메모리 반도체 시장에서는, 설비투자를 해버렸으면 연구개발과 재무운영 외에는 해볼 수 있는게 없다.<br />
<br />
인텔 4004: 최초의 마이크로프로세서. ALU, 레지스터 등을 모두 웨이퍼조각 하나 위에 올려놨다.<br />
4bit연산까지밖에 안됐지만, 이거 이후로 컴퓨터 가격이 떨어지기 시작했다.<br />
<br />
1975: IBM 5100(1.9MHz CPU, 64kB 메모리)<br />
1987: IBM XT(4.77MHz CPU, 640kB 메모리)<br />
<br />
1983년 삼성전자의 도쿄 선언: 메모리 사업 시작하겠다<br />
<br />
일본 메모리는 품질 위주, 삼성전자는 원가 위주 -&gt; 딱 시장에 필요한 성능만 내고 나머지는 원가절감 하겠다.<br />
일본 메모리는 품질을 위해 제조공정이 길고, 장비 종류도 많았다.<br />
<br />
1989년, DRAM 용량이 4MBit를 넘어가자 DRAM의 형태가 문제되기 시작했다.<br />
<br />
지금까지는 평면에 트랜지스터와 DRAM을 함께 늘어놓는 식이었는데, 밀도를 높이려면 다른 방식을 써야 했다.<br />
<br />
Trench형과 Stack형을 두고 논쟁이 많았는데, 진대제, 권오현이 Stack이 낫다고 해서 이건희가 Stack으로 가자고 했다.<br />
<br />
이때 IBM, 도시바, NEC 등이 다 트렌치 하는동안 삼성전자만 Stack을 했고, 옳은 결정이었기에 4MBit Dram이 대박을 친다.<br />
*Stack방식의 장점 적어놓기. 불량분석 이득이었던듯?<br />
<br />
반도체 산업이 처음 시작됐을때는 100mm 웨이퍼를 썼다. 1980년대 들어서는 150mm, 90년대 들어서는 200mm가 주류가 됐다.<br />
웨이퍼가 커질수록 버려지는 부분도 줄어드니까, 면적비 이상의 이득이 난다.<br />
100mm 웨이퍼와 300mm웨이퍼는 웨이퍼당 칩 갯수가 10배 넘게 차이난다.<br />
<br />
유명한 빅 칩중 하나인 IBM의 POWER9는 25x27mm다.<br />
<br />
2001년, 웨이퍼 업체들이 300mm 양산기술을 확보했다. 메모리업체들 입장에서는 당연히 바꾸는게 장기적으로 이득이지만, 불황 시기에 엄청 비싼 장비들을 사와야 한다는게 부담으로 다가왔다. 공정 최적화도 또 해야 하는데.<br />
<br />
이 불황은 2000년 10월의 닷컴버블 붕괴 때문이다. 버블 고점에서는 64MBit DRAM이 20달러였는데, 2001년 2월에에는 3.8달러가 됐다.<br />
하여간, 회사들은 돈이 없어 300mm를 바로 도입 못했다. 안그래도 불황인데 여기서 또 문제생기면 그땐 진짜 망할 수도 있으니까.<br />
<br />
근데 삼성전자는 빠꾸 안치고 2001년 10월에 300mm 웨이퍼 공정을 도입했다. 그래서 2001년에는 메모리 업체들 중 삼성전자만 흑자였다는데, 10월에 도입했는데??<br />
<br />
DRAM Cell은 원래 8F^2 구조였다가 6F^2 구조로 개선됐다. 6F^2구조가 밀도가 더 높다.<br />
마이크론이 2006년 처음 했고, 바로 삼성전자도 시작했다.<br />
<br />
일본 기업들은 1등을 뺏겼지만, 삼성전자는 6F^2 cell, 300mm공장등을 해내며 계속 1등을 이어갔다.<br />
<br />
결국 원가를 깎아서 삼성전자가 이긴거다. 비싸고 오래가는 메모리는 필요 없는 시대가 됐다.<br />
삼성전자가 기술이 훌륭했다기보다는 시대 흐름을 잘 탄거다.<br />
<br />
컴퓨터의 구성 요소: CPU, 메모리, 보조기억장치(HDD, SSD)<br />
보조기억장치는 초기 컴퓨터 이론에 존재하지 않았다. 폰 노이만 구조에는 메모리, CPU만 존재한다.<br />
(그림)<br />
뭐 전원 꺼지면 메모리가 날아간다 라고 폰 노이만 구조에 적혀 있지는 않은데,<br />
실제로는 DRAM, SRAM으로 메모리를 만들게 되어 전원이 꺼지면 메모리가 날아가게 되었다. 이건 좀 문제가 됐다.<br />
<br />
전원이 꺼져도 데이터를 보존하는 기억장치가 필요해졌다.<br />
<br />
이때, CPU가 메모리 이외 공간에 데이터를 보낸다는건 그 데이터는 당분간 필요 없다는 뜻일거다.<br />
따라서, 데이터 보존 장치의 성능은 좀 떨어져도 될 것이다.<br />
<br />
DRAM은 CPU가 직접 접근해야 하기 때문에, 메모리의 모든 방이 CPU에서 바로 접근 가능해야 했다.<br />
이는 메모리 설계에서 큰 부담으로 작용했다.<br />
모든 데이터 방에 금속을 설치해서 연결해야 하기 때문이다.<br />
<br />
결국 큰 단위로 데이터를 저장하고, 데이터가 필요할 때는 그 단위를 통째로 불러와 필요한 것만 찾아 쓰는 방식이 되었다. 이게 더 원가가 낮았다.<br />
<br />
처음에는 메모리로 자기 테이프를 썼는데, 원하는 데이터가 테이프 시작과 끝에 흩어져 있으면 테이프를 여러번 감았다 풀었다 하며 읽어야 했다.<br />
이 경우 성능이 안좋았다.<br />
<br />
1956년, IBM이 하드디스크를 내놓는다.<br />
하드디스크는 구조상 최고성능-최악성능 차이가 적다. 테이프는 엄청 크다. 그림으로 설명 가능<br />
즉, 카세트테이프는 순차접근만 가능, 하드디스크는 무작위 접근이 가능하다.<br />
<br />
HDD는 자기 테이프보다 비쌌지만, 성능도 좋고 관리도 쉬웠다.<br />
세계 HDD 출하량: 1996년 1억대, 2013년 8억대<br />
<br />
근데, CPU 성능은 2001년(펜티엄 1.2GHz) ~ 2012년(샌디브리지 3.3GHz)동안 24배 증가(코어성능 6배, 코어개수 4배)했고,<br />
메모리 가격 대비 용량은 128배, 전송속도는 12배, 접근속도는 4배(2001년 PC-133, 2012년 DDR3-1600) 증가했다.<br />
<br />
CPU와 메모리의 발전은 이렇게 빨랐는데, HDD의 발전은 느렸다.<br />
웨스턴디지털의 HDD 성능, 용량 변화:<br />
2001(WB100EB): 전송속도 40, 반응속도 12.1, 용량 10GB<br />
2012(WD10EZEX): 전송속도 150, 반응속도 8.9, 용량 1TB *단위 확인 필요<br />
<br />
하드디스크를 보면 용량은 100배 늘었지만 최대 전송속도가 3.75배밖에 안올랐고,<br />
반응속도는 그냥 모터를 5400RPM에서 7200RPM으로 높인게 다다.<br />
<br />
그리고, 데이터 안정성 때문에 3.5인치 이상 디스크에서는 회전속도를 7200RPM 이상으로 올리는게 힘들었다.<br />
<br />
게다가 컴퓨팅 기술이 발전해서 각종 프로그램, OS가 발전해 파일들의 크기가 커졌고,<br />
그러면 사용자가 프로그램을 실행하면 하드디스크 여러곳을 탐색하며 다음 파일을 읽어야 하는데, 여기에서 큰 성능 하락이 발생했다.<br />
<br />
1995년에는 MS-DOS를 썼고, 2010년에는 윈도우7을 썼다.<br />
1995년에는 MS-DOS를 용량 1.44MB짜리 3.5인치 플로피디스크에 담아 썼는데(MS-DOS의 용량이 몇인지는 모르겠다)<br />
2010년 윈도우7은 용량이 2GB가 넘는다.<br />
<br />
DOS때는 부팅시 3~10개 파일만 로드하면 됐었는데, 윈도우 부팅에는 수천개 파일이 필요하다.<br />
DOS 시절 게임 용량은 MB단위였지만, 2000년대 중반 게임들은 로딩 시간이 1분은 필요하게 되었다.<br />
결국 HDD가 데이터 입출력 병목을 만드는 상황이 되었다.<br />
<br />
그리고, HDD는 기계장치 때문에 소형화하면 성능 저하가 크게 일어났고, 저전력 상태로 만드는 데에도 오래 걸려서 저전력 상태를 유지하기 어려웠다.<br />
결국 HDD는 저전력과 고성능 양쪽에 방해됐다.<br />
<br />
플래시메모리는 1980년 도시바에서 마스오카 후지오 박사가 만들었다.<br />
DRAM, SRAM은 전하를 매우 작은 도체로 구성된 공간에 집어넣어 1과 0을 구분했다.<br />
플래시메모리는 더 출입이 어려운 절연 구역에 고전압을 가해 전자를 터널링시켜 가두는 것으로 1과 0을 구분했다.<br />
<br />
당연히, 플래시메모리는 SRAM, DRAM보다 읽기쓰기 속도가 훨씬 느렸다. SRAM, DRAM은 도체랑 바로 붙어있으니까.<br />
<br />
DRAM은 수십나노초면 읽기쓰기가 가능했지만, 플래시메모리는 전압펌프를 가동해서 절연 공간에 전하를 집어넣을 때까지 수십~수천us가 필요했다.<br />
대신, 전자가 갇혀있게 되어 전원을 꺼도 메모리가 유지됐다.<br />
<br />
플래시메모리는 Block 단위로 데이터를 읽고 쓴다. Block 단위로 데이터가 뿅 하고 사라지기 때문에 ‘플래시’메모리라 부른다.<br />
<br />
근데 플래시메모리는 공정상의 한계때문에, 칩 내의 block 중 1% 이상이 결함을 갖고 나온다.<br />
그래서 에러 정정 장치를 붙여서 사용해야 했다.<br />
플래시메모리 제조사는 ‘0번 block은 결함이 없을 것’과 ‘칩 내 불량이 몇개 이하일 것’만 보장해 판매한다.<br />
<br />
플래시메모리는 컨트롤러와 결합하여 운영되며, 컨트롤러 안에 작은 소프트웨어를 탑재해 문제들을 해결한다.<br />
값싼 원가의 대가로 얻은 낮은 질을 소프트웨어로 해결하려는거다.<br />
<br />
NAND, NOR가 있는데 NOR는 Block이 작다. 수 바이트정도 된다.<br />
NAND는 Block이 수kB 수준이다.<br />
그래서 NOR는 각 방에 연결되는 도체를 더 많이 깔아야 하고, 그래서 더 비싸다.<br />
결국 반응속도는 NOR가 더 빠르고, 가격은 NAND가 더 싸다.<br />
쓰기는 NAND가 한번에 뭉텅이로 써서 더 빠르다. 읽기가 NOR가 더 빠른거다.<br />
<br />
후지오 마스오카 박사는 플래시메모리로 마그네틱 기반 저장소를 모두 대체하고 싶어했으나, 도시바는 그 아이디어에 관심이 없었다.<br />
<br />
인텔이 더 먼저 행동했다. 인텔은 CPU 사업을 하고 있었는데, CPU가 구동하기 위해서는 일련의 구동 코드들 및 BIOS를 마더보드에 저장해둘 외부 공간이 필요했다.<br />
그때까지의 컴퓨터들은 이 코드들을 PROM, EPROM, EEPROM에 저장해 왔다.<br />
<br />
하지만 PROM은 물리적으로 퓨즈를 끊어 0과 1을 구분하는 방식이었기 때문에, 실수하면 칩을 못쓰게 될 위험이 존재했다.<br />
EPROM은 데이터를 적기는 쉬운데, 지우려면 자외선을 조사하는 과정이 필요했다.<br />
EEPROM은 사용하기는 쉬웠지만 용량이 너무 작았다.<br />
<br />
플래시메모리는 이런 단점들이 없단느걸 인텔이 알아챘다.<br />
<br />
이 코드들은 컴퓨터 부팅시에만 사용되며, 프로그램 성능에는 영향을 주지 않ㄴ느다. 용량은 수십kB ~ 수십MB만 있으면 된다.<br />
데이터를 바꾸는 일은 BIOS 업데이트를 할 때 정도고, 사용자 환경과 분리되어 운영되기 때문에 데이터 접근 성능에 대한 걱정도 필요없다.<br />
물론 매년 필요한 용량이 증가하긴 했지만 얼마 안됐다. 결국, 그냥 간단하게 사용 가능하고 비휘발성이면 되는거였다.<br />
여기에 NOR 플래시메모리가 쓰이게 된다. 읽는게 빠르니까.<br />
<br />
NAND 플래시의 메모리 특성이 더 안좋았기 때문에, 컨트롤러는 NAND 플래시용 컨트롤러가 더 컸다. 그래도 가격은 NAND가 더 쌌다.<br />
<br />
인텔, 암드는 NOR 플래시메모리에 집중했다. 마더보드 위의 ROM을 대체하기 위함이었으니까.<br />
<br />
도시바는 후발주자가 되어버렸는데, 1991년 낸드플래시 개발을 완료하고 시장 진출을 선언했다.<br />
그리고 1992년, 삼성전자에게 낸드플래시 기술을 라이센싱해주게 된다.<br />
<br />
그 후, 삼성전자는 미래 디지털 제품 기술 확보를 위해 DRAM, NAND, CDMA에 역량을 쏟아붓겠다고 선언한다.<br />
지금 보면 꽤 성공했다.<br />
<br />
Sandisk는 메모리를 직접 제조하지는 않고, 플래시메모리에 들어갈 컨트롤러만 만들었다.<br />
당시 이름은 Sundisk였는데, 나중에 Sandisk로 바꾼거다.<br />
<br />
핸드폰도 발전하기 시작해서, 저장장치가 필요해졌다.<br />
플로피디스크는 성능 향상이 힘들었고, 신뢰하기 힘든 매체가 사용됐다.<br />
CD-ROM은 한번 적은 내용을 고치기 어려웠다.<br />
휴대용 HDD는 크기가 너무 컸고, 충격으로 데이터를 잃어버릴 수도 있었다.<br />
<br />
여기에도 플래시메모리가 쓰이게 된다.<br />
<br />
도시바는 SD(Secure Digital)라는 새로운 규격을 만들었고,<br />
이스라엘의 M-systems는 최초의 휴대용 USB를 만들었다. 자신들의 컨트롤러에 샌디스크의 낸드를 붙였다고 한다.<br />
근데 아까 샌디스크는 컨트롤러만 만들었다며? 뭐지<br />
<br />
당시 USB 용량은 8~16MB라 CD-ROM보다는 용량이 훨씬 작았지만, 휴대성과 안정성은 훨씬 좋았다.<br />
<br />
삼성전자는 DRAM 시장에서 승리한 후, 낸드플래시 비트당 가격이 DRAM보다 높은걸 보고 낸드플래시에 대규모 투자를 하게 된다.<br />
낸드플래시가 DRAM보다 싸지게 됐고, 가격경쟁이 심해지기 시작했다.<br />
<br />
이때쯤, 애플은 iPod보다 발전한 iPod Nano를 만들고 싶어했다.<br />
원래 아이팟에는 소형 HDD가 있었는데, 더 소형화하기 위해 플래시메모리를 쓰게 됐다.<br />
당시 NOR 대비 NAND 플래시의 칩당 밀도가 10배정도 높아서, 애플은 NAND 플래시를 쓰게 됐다. 이때, 삼성전자의 NAND플래시를 쓰게 된다.<br />
<br />
이 아이팟 나노가 엄청난 히트를 치게 되어, CD나 HDD 기반 음악감상 장치들이 도태되고 플래시메모리 기반 제품들이 많이 나오게 된다.<br />
결국 플래시메모리가 많이 필요해졌고, 삼성전자는 엄청난 이득을 보게 된다.<br />
<br />
다른 낸드 제조사들도 이득을 봤지만, 삼성전자에게 특히 유리했던 점은 삼성전자는 도시바와 달리 DRAM 사업부, 파운드리사업부도 있었다는 점이다.<br />
아이팟 나노는 모바일 SD램과 일종의 CPU인 미디어 프로세서도 필요했는데, 삼성전자는 이걸 다 만들어줄 수 있었다.<br />
<br />
그래서 애플에게 미디어 프로세서를 공급하던 팹리스 업체인 PortalPlayer는 2006년 애플과의 계약이 종료됐음을 발표하고, 이후 엔비디아에 인수된다.<br />
그 계약들이 다 삼성전자한테 넘어간거다.<br />
<br />
물론 모든 낸드 제조사들이 이득을 본건 아니었다. 일본의 르네사스 반도체는 2010년 12월, 더이상 낸드 개발을 하지 않겠다고 선언한다.<br />
<br />
2005년, 애플 아이팟 나노 덕분에 돈이 많아진 삼성전자는 갑자기 새로운 사업을 발표한다.<br />
1.8인치, 2.5인치 SSD시장에 진출하겠다는거다.<br />
<br />
SSD 1TB, HDD 10TB여도 SSD 1TB를 고를 수 있으니까. 10TB까지 필요 없잖아?<br />
<br />
그리고 SSD는 내충격성, 성능이 높으니 충격 방지 설계 비용, CPU에 소모될 예산을 아낄 수 있으니 전체 비용이 줄고 경량화가 가능하다.<br />
<br />
그리고 HDD컨트롤러보다 SSD컨트롤러가 더 만들기 쉬웠다. 기계적인거 신경 안써도 되니까.<br />
<br />
2006년: 삼성전자가 최초의 양산형 SSD를 699달러에 발표한다. 32GB짜리였다.<br />
<br />
이후 비슷한 SSD들이 컨트롤러 업체들에서 등장한다. 플렉스터, 퓨전IO, OCZ, Silicon Motion 등 업체들이었다.<br />
이 팹리스 업체들은 낸드를 사와서 자사 컨트롤러에 붙여 팔기 시작했다.<br />
<br />
UBER: 수정 불가능한 에러가 발생할 확률. Uncorrectable Bit Error Rate<br />
<br />
삼성전자는 DRAM때는 원가경쟁으로 경쟁업체들을 말려죽였고, 낸드 시장에서는 낸드와 다른 하드웨어들을 결합한 솔루션(노트북, 아이팟나노 등)들을 제공했다. NOR 대신 NAND쪽을 선택한 것도 옳은 선택이었다.<br />
(107페이지 삼성전자 연표)<br />
<br />
ISA: Instruction Set Architecture. 인텔, AMD CPU는 인텔의 ISA인 x86, x86-64를 쓴다.<br />
ARM CPU는 ARM V8을 쓴다.<br />
<br />
exe: 실행 파일. 사용자가 직접 클릭해서 실행할 수 있는 파일<br />
dll: 동적 라이브러리. 다른 실행파일이 불러와서 사용할 수 있는 형태의 파일.<br />
<br />
Compiler별로 프로그램 성능이 많이 달라질 수 있다. How are you?를 ‘요즘 어떻게 지내니?’, ‘요즘 어때?’ 등으로 번역 가능한 것처럼<br />
<br />
Compiler, 기계어는 인간이 다루기 어렵다.<br />
<br />
1977년 발사된 보이저1,2호의 메모리는 68kB였다. 보이저의 제어 프로그램은 기계어와 포트란으로 만들어졌다. 이 정도는 인간이 기계어로 만들 수 있다.<br />
<br />
근데, 예를 들어 윈도우 7은 4기가짜리다. 이런건 인간이 기계어로 못만든다.<br />
<br />
옛날 컴퓨터에는 확장 슬롯 자체가 없거나, 공인된 하드웨어만 붙일 수 있었다.<br />
<br />
1981년 IBM이 XT라는 컴퓨터를 만들었는데, 서드파티에게 컴퓨터를 오픈하고, 보조기억장치로 HDD와 플로피디스크를 갖추고, 확장 가능한 램 슬롯을 가진 구조였다.<br />
CPU는 인텔 8088, OS는 MS-DOS였다.<br />
<br />
당시에는 컴퓨터 관련 표준이 거의 없어서, 당시 컴퓨터들은 CPU가 동일한 ISA를 사용하더라도, 아니면 아예 같은 CPU를 사용하더라도 프로그램이 제대로 안돌아가는 경우가 많았다.<br />
요즘은 어느 회사 RAM을 쓰든, 어느 회사 GPU를 쓰든 잘 작동하잖아? 그때는 공식적으로 인정된 부품만 써야 했다.<br />
<br />
그래서 IBM PC가 혁신이었다. 맘대로 하드웨어 갈아끼울 수 있다니!<br />
<br />
당시 인텔8088은 가격이 저렴하고 성능이 뛰어났다. 그리고 인텔 x86-16은 CISC 명령어 체계를 사용하고 있었는데, x86-16에는 범용 레지스터(CPU 내부 연산 처리를 위한 초고속 저장공간)가 훨씬 많아서, 코드를 잘 짜면 경쟁자들을 압도할 수 있었다.<br />
<br />
그리고 8088은 내부로는 16비트를 사용해서 성능을 끌어올렸지만, 외부로는 8비트 버스를 갖고 있었다.<br />
당시 시장에 나온 하드웨어들은 다 8Bit였기 때문에 이게 나았다.<br />
<br />
그 후 모든 회사들이 달려들어 IBM PC를 리버스엔지니어링 했다. 모두가 IBM PC에 맞는 규격의 하드웨어, 소프트웨어를 만들게 되었다.<br />
<br />
그 전에는 다 규격이 달라서 거래처가 바뀌면 싹 다 다시 해야 했다.<br />
하지만 이제는 IBM PC 규격이 표준이 되어 이거대로 만들면 되는 상황이 됐다.<br />
<br />
그래서 IBM은 이득을 봤나? 그건 아니다. IBM도 경쟁자 중 하나가 되어버리고 말았다. 그래서 별로 이득 못봤다.<br />
<br />
그래도 PC 사업은 IBM의 사업들 중 하나였기 때문에 큰 타격은 없었다. IBM은 지금도 메인프레임 시장의 절대자다.<br />
<br />
이 시점에, 인텔은 두가지 결정을 내린다.<br />
1. 일본과의 DRAM 경쟁을 포기하고 CPU에 집중한다.<br />
2. 프로세서 생산을 타사에 맡기지 않는다.<br />
<br />
인텔을 왕좌에 올린 8088 프로세서를 위탁 생산하던 회사는 AMD, NEC, 후지츠 등 10개 가까이 되었다.<br />
이렇게 아웃소싱을 돌려서 웨이퍼 공장을 작게 유지하고 있었지만, Value chain의 상당 부분을 다른 회사들과 나눠 가져야 했고, 기술이 새어나갈 위험도 있었다.<br />
<br />
인텔은 자기 제품을 타사에 맡기지 않게 되며, 전세계 PC CPU의 설계부터 제조까지 모든 Value Chain을 장악하게 됐다.<br />
<br />
인텔이 자체 생산으로 물량을 돌릴 때, AMD는 인텔의 x86 ISA만 라이센싱하고 설계는 자체적으로 하기로 했다.<br />
인텔이 CPU 시장을 먹어버리긴 했지만, CPU는 수명이 길기 때문에 인텔은 과거 제품보다 더 좋은 제품들을 계속 만들어내야 했다.<br />
게다가 AMD도 쫓아오고 있었다.<br />
<br />
그래서, 독점시장이어도 인텔은 가만히 있을 수가 없었다.<br />
<br />
데너드 스케일링: 미세공정 발전으로 면적당 트랜지스터가 늘어나도 전력 소모는 늘어나지 않는다<br />
인텔은 전력 소비량을 유지하며 더 많은 부품을 CPU에 빽빽히 꽂아넣어 성능을 늘릴 수 있었다.<br />
또, 약간 밀도를 낮춰 동작 마진을 주는 것으로, 더 높은 clock으로 동작할 수 있게 했다.<br />
<br />
인텔은 과거 CPU에서 동작하는 프로그램들은 모두 상위 CPU에서도 동작하게 만들었다.<br />
뭐 ISA를 갈아엎는다거나 이런 행동을 하지 않은거다.<br />
<br />
컴퓨터가 발전하고 메모리 용량이 커지자, 인텔은 x86-16에서 x86-32로 넘어갔다. x86-16은 64kB 이상 메모리를 인식할 수 없기 때문이다.<br />
인텔 80386에서 이 변화가 발생했는데, 옛날 프로그램들 잘 돌아가도록 ‘Virtual 8086 mode’를 만들어놨다.<br />
지금도 가상환경에서는 8086 명령어 다 돌아간다.<br />
8086 프로세서는 트랜지스터 3만개인데, 요즘 프로세서들은 1억개 이상이다. 사실 아예 프로세서를 집어넣을 수도 있는거다.<br />
<br />
이런 호환성 유지는 쉬운 일이 아니다. 요즘 잘 안쓰는 명령어도 들고 가야 하니 웨이퍼 면적, 전력 낭비가 어느 정도씩 발생하게 된다.<br />
인텔은 이걸 감내하면서도 성능을 향상시킬 방법을 찾아나갔다.<br />
<br />
성능을 높이는 한가지 방법은 clock을 높이는 것이었다.<br />
최신 제조장비를 도입해서 미세공정 품질을 높이고, 이를 통해 cell의 크기를 줄이고 누설전류와 발열을 억제해 CPU의 최대 스위칭속도를 높이는거다.<br />
이걸로 clock이 2배 높아지면 CPU 성능도 2배가 된다.<br />
<br />
하지만 switching 속도가 너무 빨라지면 원래 같이 동작하는걸 전제로 만들었던 회로가 따로 동작할 수도 있다.<br />
(그림)<br />
<br />
1GHz면 30cm 이동하니까, 길이가 30cm 넘어가면 다른 clock이 걸리게 된다. 4GHz면 7.5cm<br />
그래서 한 덩어리였던 하드웨어 블록을 여러 덩어리로 쪼개야 한다. 이게 파이프라인이다.<br />
<br />
다른 방법은 아키텍처를 넓히는 것이다. 인텔 Sandy Bridge, Haswell, Skylake 등이 이런 아키텍처의 코드명이다.<br />
한개 CPU core에서 병렬로 동시에 처리될 수 있는 작업들이 생기는데, 이런걸 ‘ILP(Instruction Level Parallelism)’이라 부른다.<br />
이 ILP들을 찾아내 성능을 높이는 CPU를 Superscalar Processor라 한다.<br />
물론, 이 ILP 찾는건 아주 빠르게 이뤄져야 하기 때문에 만들기 Superscalar Processor를 만드는게 어렵다.<br />
하지만 이걸 해내면, clock도 안바꿨는데 CPU가 빨라진다!<br />
<br />
마지막 방식은 새로운 명령어를 ISA에 추가하는거다. 특정 상황에서 아주 빠르게 동작할 수 있는 신규 명령어,<br />
그걸 위한 하드웨어들을 추가하는거다.<br />
예시로는 인텔 펜티엄의 MMX, 펜티엄3의 SSE, 코어 시리즈의 AVX등의 SIMD(Single Instruction Multiple Data) 명령어들이 있다.<br />
한개 Instruction으로 여러개 숫자열 상태가 바뀐다.<br />
<br />
근데 이 방식은 다른 두 방식과 다르게, 프로그래머가 이 새로운 명령어를 써줘야 동작이 빨라진다.<br />
그리고, 과거 CPU들은 이 새로운 명령어를 이해 못하기에 프로그램을 돌릴 수 없게 된다.<br />
ex) 386은 MMX가 적용된 프로그램을 못돌린다.<br />
<br />
새로운 명령어가 추가되면 Compiler를 새로 만들어야 하는데, 인텔이 스스로 Compiler(ICC)를 만들어 팔았다.<br />
그래서 프로그램 개발이 그렇게 어렵지 않았고, 어차피 이런거까지 써야 하는 하이엔드 성능이 필요한 프로그램이면 옛날 CPU에서 안돌아가는게 문제가 되지 않았다.<br />
<br />
그래픽 작업이 병렬인 이유: 모니터에 표시되는 픽셀들은 서로 상호작용하지 않기 때문에 병렬이다.<br />
<br />
인텔은 램버스와 협력해 RD램을 만들어 다시 한번 메모리 시장에 들어가 PC 플랫폼에 대한 영향력을 늘리려 했지만, 메모리 기업들과의 원가 싸움에서 또 털리고 만다.<br />
<br />
20세기 말, 컴퓨터 성능과 용량이 기하급수적으로 늘어나고 있었다.<br />
32비트 기반이었던 x86은 한번에 접근 가능한 메모리 영역이 4GB였는데, 메모리 용량이 점점 커져서 곧 메모리에 한번에 접근하지 못하게 될 위기였다.<br />
접근 자체는 가능한데, 여러 단계를 거쳐야 하니 효율성이 떨어지는 상황이었다.<br />
<br />
인텔은 CPU를 64비트로 바꾸고 최대 메모리 주소를 늘려야 하는 상황이었는데, 20년간 유지해온 하위호환 정책에 대해 다시 생각해보게 됐다.<br />
<br />
신형 칩들 clock은 계속 올라서 2000년에는 1GHz를 넘었고, 실리콘 웨이퍼가 버틸 수 있는 최대 clock인 4~5GHz 근처로 빠르게 다가가고 있었다.<br />
<br />
그리고, 만들어진지 20년이 넘은 인텔의 x86은 각 명령어마다 길이가 달랐기 때문에(CISC),<br />
그 후 생긴 모든 명령어의 길이가 같은(RISC) ARM 프로세서보다 비순차 수행에 적합하지 못했다.<br />
<br />
비순차수행에서는 명령어들의 순서를 바꾸는 과정이 필요한데, 명령어들의 길이가 같으면 쉽지만 다르면 어렵다.<br />
<br />
그리고, CPU가 추출해낼 수 있는 동시 수행 가능성도 한계였다.<br />
<br />
인텔은 여기서, x86을 포기하고 앞으로 나아가기로 한다. 이때 HP와 협력한다.<br />
<br />
HP는 1980년부터 RISC도 CISC도 아닌 대안 아키텍처를 고민하고 있었다.<br />
<br />
2001년, 인텔은 HP와 협력하여 새로운 CPU 아키텍처인 Itanium을 발표했다.<br />
기존 CPU들은 비순차 수행 방식으로 성능을 끌어올렸는데, 인텔의 새로운 CPU는 그런 복잡한 비순차 수행 엔진(명령어 배치 장치)을 만들 웨이퍼 면적을 추가 연산장치에 투자하는 방향으로 갔다.<br />
<br />
그럼 Itanium은 동시에 명령어 수행을 못하나?<br />
Compiler에서 동시에 실행할 수 있는 명령어를 모아서 실행파일을 만들게 했다.<br />
이러면 Compiler의 부담이 커지긴 하지만, Compiler는 하넌 고생해서 만들어두면 수십년 쓰니까.<br />
<br />
이렇게 비순차 수행 엔진 치우고 그 자리에 연산장치를 놓으면 성능 좋아지고, 원가 내려가고, 전력소모가 내려갈 것이다!<br />
<br />
근데 이렇게 하려면, compiler는 CPU가 바뀌면 그때마다 매번 새로운 실행파일을 만들어줘야 한다.<br />
<br />
ex) CPU에 원래 연산장치가 4개였는데, 신형은 6개가 됐다 -&gt; 기존 실행파일 쓰면 2개가 논다. 그래서 실행파일을 새로 만들어줘야 한다.<br />
기존 슈퍼스칼라 프로세서였다면 알아서 새로운 2개 잘 썼을텐데!<br />
<br />
소프트웨어 회사들은 Itanium 아키텍처를 쓰려면 CPU마다 다른 실행파일을 만들어야 한다.<br />
<br />
하위호환성도 있기는 했지만, 영 좋지 않았다.<br />
기존 x86 실행파일 -&gt; CPU 에뮬레이터 -&gt; IA-64(아이태니엄 언어) -&gt; CPU 본체 과정으로 실행해야 했다.<br />
이렇게 복잡하게 실행하니, 당연히 성능이 기존 대비 아주 안좋았다.<br />
<br />
결국 아이태니엄은 잘 안팔렸다.<br />
<br />
2003년, 지금까지 인텔의 ISA를 라이센싱해서 사업하던 AMD가 완벽한 하위호환을 유지하며 64비트 확장을 성공시키고, 성능까지 높인 새로운 명령어 세트인 AMD 64 (= x86-64)를 내놓고, 서버용 CPU 슬렛지해머를 출시했다.<br />
이 CPU는 AMD의 서버용 CPU 라인업인 Opteron에 포함되는 제품이다. Opteron은 x86으로 구성된 기존 명령들을 완벽히 수행할 수 있었다.<br />
<br />
인텔 아이태니엄 생태계가 자리잡지 못한 상태에서, 기업들이 아이태니엄 말고 x86-64로 갈아타버리면 인텔은 망하고 말 것이다.<br />
인텔은 위기를 느끼고, x86-64를 받아들이며 CPU 개발계획을 크게 전환했다.<br />
그럼에도, 아직 HP와의 계약이 남아 아이태니엄을 완전히 접을 수는 없었다.<br />
<br />
그래서 이제는 인텔이 AMD에 x86-64의 사용료를 지급해야 하게 됐고, 지금까지도 내고 있다.<br />
<br />
짐 켈러가 AMD에 있을때 만든게 AMD64랑 하이퍼트랜스포트다. 하이퍼트랜스포트는 멀티코어 프로세서의 핵심 기술이다.<br />
<br />
아이태니엄은 인텔에게 계속 비용을 발생시켰다. HP에게 HP-UX 시스템용 CPU를 공급하는 장기 계약을 했기 때문이다.<br />
<br />
2004년, 인텔의 신형 CPU인 펜티엄4 프레스캇이 나왔는데, 초기에는 x86-64 지원 안한다고 하더니 얼마 후부터 갑자기 x86-64를 지원한다는 문구를 달고 나오기 시작했다.<br />
<br />
그 사이에 x86-64 지원하도록 재설계했다고 보기에는 너무 짧은 기간이고, 애초에 x86-64를 위한 공간을 만들어뒀던 것으로 보인다.<br />
<br />
AMD는 이후 듀얼코어 맨체스터를 개발하며 시장의 우위를 점했고, 인텔은 비순차 처리 엔진보다 clock 상승에 집중했다가 clock에 비해 낮은 성능을 냈고, 발열도 심해져 펜티엄4 프레스캇은 ‘프레스핫’이라는 별명까지 얻었다.<br />
<br />
2005년 AMD의 시장 점유율은 40%가 넘었으며, CPU 성능은 최저가 라인업부터 최고가 라인업까지 모두 AMD가 인텔보다 높았다.<br />
<br />
고가 라인업에서는 펜티엄이 애슬론FX에 밀렸고, 저가에서는 셀러론이 셈프론에 밀렸다.<br />
<br />
인텔은 정신차리고 ‘펜티엄’이라는 브랜드를 폐기하고, ‘인텔 코어2 프로세서’라는 브랜드를 내놓고, 2006년 신형 마이크로아키텍처 Conree를 내놓는다.<br />
<br />
인텔의 Conree는 clock이 2.4GHz밖에 되지 않았지만, 성능이 상당히 좋았다.<br />
Conree는 33만원이었는데, 성능은 당시 100만원이 넘던 AMD의 최고급 CPU인 애슬론 64 FX62를 밀어냈다.<br />
인텔이 개발 방향을 clock 중심에서 아키텍처 확장으로 전환한 덕분이었다.<br />
<br />
인텔이 정신차리자 AMD가 시장점유율을 잃어버리기 시작했다.<br />
게다가 TLB 버그라는 하드웨어 결함까지 발견되며 시장을 거의 잃어버린다.<br />
<br />
2000년대 초, AMD와 인텔 모두 한개 코어 성능을 키우는 데에는 한계가 있다는 것을 알고 있었다.<br />
그래서 CPU 하나에 여러개 코어를 박는 멀티코어를 만들어야 했다.<br />
<br />
근데 문제는, 코어끼리 통신하는 데에 시간이 엄청 걸린다는거다.<br />
공유해야 하는 정보가 CPU 공유 캐시에 있으면 명령어 100개 이상, D램에 있으면 1만개 이상이 필요하다.<br />
그래서 멀티코어가 어렵다. 통신이 오래 걸린다.<br />
<br />
4코어 사용 프로그램을 열어보면 4개 코어가 1~100% 사이에서 다양한 사용률을 보인다.<br />
다른 코어와 협업하기 위해 답장을 기다리는 동안 아무것도 못하니까.<br />
<br />
AMD는 승부수를 던져보기로 했다.<br />
AMD가 쓰던 아키텍처인 K10은 이미 수명을 다해가고 있었고, 대형 CPU 설계에서 인텔의 노하우를 이기는건 어려우니<br />
계속 커져가고 있던 CPU 코어의 크기를 줄이고, CPU 코어의 갯수를 늘리자는 것이었다.<br />
<br />
이렇게 하면 비순차 실행장치의 크기도 줄어들고, 연결되어야 하는 회로의 갯수도 줄어들어 설계의 어려움이 감소했다.<br />
비순차 실행장치의 성능은 크기에 정비례하지 않으니, 크기 감소로 인한 코어당 성능 감소가 작을 것이라고 생각한 것이다.<br />
<br />
프로그래밍 시장도 점점 많은 코어를 활용하는 방향으로 가고 있었고, 애초에 서버 시장에서는 코어당 성능보다 코어 갯수가 중요했다.<br />
<br />
AMD는 큰 코어를 작은 코어 2개로 쪼개고, 일부 하드웨어를 공유하는 ‘모듈’개념을 도입했다. 이걸 CMT(Cluster Multithreading)라 부른다.<br />
이렇게 개발한 마이크로아키텍처가 Bulldozer였다. Bulldozer는 INT가 2개고, DEC과 FP를 공유한다.<br />
*CPU 안의 DEC, DISPATCH, INT, FP가 뭔지 확인<br />
<br />
인텔은 암달의 법칙(Amdahl’s law)에 주목했다. 프로그램 실행 성능은 단일코어 성능과 멀티코어 성능 중 나쁜 쪽이 결정한다는거다.<br />
한 코어가 아주 빠르다고 해서 전체가 빠른게 아니다. CPU 코어들이 나눠서 할 수 없는 작업들도 있을거니까.<br />
<br />
그래서, 인텔은 코어를 늘리기보다는 코어당 성능을 올리고, 한개의 고성능 코어가 다른 작업도 처리할 수 있게 했다. 이 방식은 SMT라 불렸다.<br />
이 분야는 IBM이 1968년에 연구했던 분야다. 인텔은 이걸 hyperthreading이라 불렀다.<br />
<br />
이렇게, 두 회사의 CPU 설계 방향이 크게 갈리게 되었다.<br />
<br />
2011년, 최초의 Bulldozer 마이크로아키텍처 CPU인 4모듈, 8코어 잠베지가 출시됐지만, 성능이 아주 안좋았다.<br />
<br />
인텔은 4개 코어로 구성된 차기 마이크로아키텍처 샌디브리지를 발표했다.<br />
이전 세대 대비 CPU 단일코어 성능이 30%까지 높아져 있었다.<br />
<br />
잠베지랑 샌디브리지는 단일코어 성능이 2배 가까이 차이났다.<br />
이건 단일 코어에서만 돌아가는 옛날 프로그램을 실행하면 성능 차이가 2배 난다는 이야기다.<br />
<br />
그럼 서버용 CPU 시장에서는 AMD가 이겼나? 서버는 코어 수가 중요하다고 했잖아.<br />
서버에서 멀티코어가 중요한건 트래픽이 몰릴때 이야기고, 평소에는 서버용 CPU도 간단한 일만 하니까 이때는 단일코어 성능이 중요하다.<br />
서버 PC들은 굳이 AMD로 넘어갈 이유를 찾지 못했다.<br />
<br />
AMD의 서버시장 점유율은 계속 하락해서, 2012년에는 거의 끝장났다.<br />
그 후에는 개발되는 서버용 프로그램들은 모두 인텔 CPU에 최적화되고 호환성도 맞춰져 재진입이 어려워졌다.<br />
<br />
AMD가 거의 망해버리자, AMD 칩 제조를 담당하던 파트너사 글로벌파운드리즈도 경영 위기에 들어간다.<br />
AMD의 암흑기는 CMT를 포기하고 인텔의 SMT를 적용한 Zen이 등장하고서야 끝났다.<br />
<br />
인텔은 이렇게 두번의 위기를 극복해냈다.<br />
인텔의 강력한 단일코어 성능은 모바일 혁명 이후 기세가 붙은 ARM의 서버 시장 도전을 막아내는 강력한 방패로 작용했다.<br />
<br />
인텔은 마이크로소프트와 더불어 반도체시장을 독점하는 기업들 중 하나였지만, 눌러앉이 않고 계속 제품개발을 했다.<br />
162페이지에 인텔 연대표<br />
<br />
삼성전자, 인텔은 IDM(Integrated Device Manufacturer)이다. 하나의 회사가 반도체 설계, 생산을 모두 하는게 IDM이다.<br />
설계에 쓰는 프로그램은 EDA라 부른다.<br />
<br />
고성능 로직은 마스크가 20장 이상 필요하기도 하다.<br />
<br />
삼성전자는 메모리, 인텔은 CPU회사라 미세화 및 성능 향상의 이득이 그대로 들어나는 분야에 있다.<br />
그래서 1등기업이어도 계속 연구개발에 투자해야 한다. CPU는 수명도 길어서, 계속 더 좋은걸 만들어 팔지 않으면 굶어죽는다.<br />
<br />
MCU도 CPU와 비슷한 구조를 갖고 있지만, 고성능 연산을 포기하고 더 작은 연산장치나 간소화된 메모리 시스템을 갖고 있다.<br />
SSD, HDD 안에도 MCU가 있다.<br />
<br />
MCU가 하는 일은 간단하다. ‘A버튼이 눌리면 어떻게 행동한다’ 정도다. 자판기 생각하면 된다.<br />
그리고 자판기에는 고성능 연산이 필요 없다. 조금 빨라져서 뭐하나.<br />
<br />
MCU는 성능 요구치가 낮으니 단가도 낮다. MCU는 칩 하나에 메모리, CPU 등 전부 합친거고, DSP는 MCU보다는 비교적 낮은 처리능력을 가진 로직이다.<br />
하여간 얘네 시장규모는 메모리 시장의 20% 정도다.<br />
<br />
이런 제품들은 가격도 높지 않고, 추가 성능에 대해 고객들이 더 많은 돈을 지급할 용의도 없다.<br />
그래서 회사들이 굳이 설계~제조라인을 완벽히 갖추고 원가와 성능을 쥐어짜낼 이유가 없다.<br />
<br />
반도체시장 초기에는 많은 회사들이 자신의 fab을 갖고 있었지만, 미세공정이 점점 힘들어지자 회사들은 고민에 빠졌다.<br />
이제 미세공정 구현 능력과 원가 경쟁이 중요해졌는데,<br />
이걸 못따라간 회사들은 공장을 포기하고 팹리스 회사가 되거나,<br />
아예 매각/흡수합병되는 회사들도 많았다.<br />
<br />
소형 로직/마이크로컨트롤러 제조업체들의 경쟁력은 설계 노하우 및 인력이었고, 용도가 제한적이거나 특수했기 때문에 매각이나 인수가 쉬웠다.<br />
근데 메모리 업계는 전세계 회사들이 같은 제품을 서로 원가 깎아가며 만드는 경쟁이라,<br />
메모리 업계에서는 부도난 회사의 경쟁력 없는 노하우를 원하는 인수자가 없다.<br />
<br />
2011년, 일본 메모리업체 엘피다는 10조 부채를 남기고 파산했다.<br />
<br />
하여간, 이렇게 팹리스/파운드리 구조로 시장이 변했다. 각자의 이득은 p.174에 있다.<br />
팹리스: 공장에 투자하는 부담이 사라졌다.<br />
파운드리: 공정에 집중할 수 있다, 과거 공정을 계속 쓸 수 있다.<br />
<br />
ARM은 칩 설계의 일부만, 또는 칩 설계 없이 ISA만 팔았다. 설계하다 막히는 곳 있으면 우리꺼 사다 써라!<br />
약간 팹리스들의 팹리스가 되고 싶어 한다.<br />
<br />
TSMC는 첨단~옛날 공정 다 유지한다. 100nm 넘는 공정을 지금도 갖고 있다. 시장이 작아서 새로운 설계를 하는게 부담인 제품들도 있으니까.<br />
<br />
원래는 TSMC 첨단공정이 인텔보다 훨씬 안좋았는데, 나중에 역전되게 된다.<br />
<br />
21세기에 들어서자, 데너드 스케일링이 흔들리기 시작했다.<br />
수십nm 수준으로 미세화되자, Cell과 배선 사이의 parasitic effect의 영향이 늘어나 leakage current가 증가하게 됐다.<br />
전성비(전력 대 성능비)가 잘 늘어나지 않게 됐고, 반도체의 열밀도가 올라가게 됐다.<br />
<br />
이제 반도체 회사들은 집적도를 올릴 때마다 동일 면적에서 일어나는 발열이 커지는 것을 감내해야 했다.<br />
데너드 스케일링 말대로 집적도 올리면 트랜지스터당 사용하는 전력이 감소하는게 맞긴 한데,<br />
이제는 leakage current가 늘어나서 옛날만큼 감소하지도 않고, 옛날이랑 같은 면적이어도 열이 더 많이 발생하게 됐다.<br />
그래서 반도체 특성 관리, 냉각, 전력공급이 어려워졌다.<br />
그래서 칩 전체를 가동하는게 점점 힘들어졌고, 당장 필요없는 회로는 잠깐 꺼야 할 수도 있었다.<br />
<br />
노광장치의 발전도 느려지기 시작했다. 노광장치의 핵심은 빛의 파장을 최대한 줄여 해상도를 높이는 것이었고,<br />
거대한 볼록거울로 그 짧은 파장의 빛을 모으는 방식이었다.<br />
<br />
근데 문제는, 빛의 파장이 짧아질수록 흡수율이 올라간다는 것이었다.<br />
거울에서 반사가 잘 안되고 뚫고들어가버린단 뜻인듯? X선이 그렇듯이 말이다<br />
근데 밑에서 렌즈도 못쓴다는거 보니까 진짜 말 그대로 흡수인가?<br />
<br />
하여간, 이제 광원에서 발생한 빛이 웨이퍼에 전달되는 비율이 내려가고 있었다.<br />
<br />
Hglamp -&gt; KrF -&gt; ArF로 바뀌며 짧은 파장 광원을 열심히 찾아다니긴 했지만, ArF 레이저로는 20nm 근처 lithography가 불가능했다.<br />
그래서 광원 파장은 그대로 두고, 렌즈 아래에 굴절률 높은 액체를 배치해 광원의 파장을 줄이는 Immersion(액침) 방식을 사용했다.<br />
<br />
이렇게 해서 얼마동안은 노광기를 발전시킬 수 있었다.<br />
<br />
하지만 이후에는 EUV 영역으로 나아가야 했는데, EUV는 볼록렌즈를 쓸 수 없을 정도로 흡수율이 높다.<br />
파장이 너무 짧아서 인간이 마주치는 모든 물질이 극자외선을 흡수할 수 있다.<br />
<br />
그래서 EUV 장치 내부는 거의 완벽한 진공이어야 하고, 볼록렌즈 대신 특수하게 설계된 다중 반사판을 써야 한다.<br />
근데 뭐 어떻게 만들어도 loss가 심해서, EUV 장비 개발은 계속 늦어지고 있었다.<br />
<br />
그 동안에도 반도체 회사들은 미세공정이 필요했다. 그래서 ‘멀티 패터닝’이라는걸 사용했다.<br />
한개 패턴을 생산할 때 한개 마스크 대신 여러개 마스크를 써서 가느다란 배선을 만드는 방식이었다.<br />
<br />
근데 이렇게 하면 필요한 마스크의 수가 엄청나게 많아졌고, 사실 그렇게 정확하지도 않았다.<br />
lithography가 정확하지 않으면 제품 성능이 균일하지가 못하니, 결국 EUV를 쓰긴 해야 했다.<br />
<br />
그래서 EUV 장비를 사오면?<br />
EUV는 loss가 심해서 출력이 아주 낮다. 그래서 여러개를 사와야 하는데, 개당 2천억원 이상이다.<br />
그니까, 이거 안사오면 성능/품질에 문제가 생기고, 이거 사왔다가 제대로 가동 못하면 진짜 망하는거다.<br />
<br />
삼성전자는 칩 전체에 EUV를 쓰지 않고, 고해상도가 필요한 핵심 회로에만 EUV를 썼다.<br />
EUV만 쓰면 웨이퍼 처리가 너무 느리기 때문이다. 출력이 낮으니까.<br />
<br />
전체를 EUV로 찍는것보다, 일부만 EUV로 찍는게 필요한 기계 수도 적었다.<br />
<br />
인텔은 칩간 2차원 연결을 지원하는 EMIB 기술과 3차원 연결을 하는 포베로스 계획을 발표했다.<br />
미세공정의 영향을 덜 받는 I/O는 구세대 공정으로 만들고, 고성능 인터커넥트인 EMIB로 칩 사이를 연결하는 Chiplet 방식을 함께 도입하겠다는거다.<br />
<br />
CPU, 그래픽 등 성능이 중요한 부분에는 고성능의 10나노 공정, 입출력단이나 메모리 컨트롤러 등에는 구세대 공정을 사용하고 다 연결하는거다.<br />
영어로는 monolithic chip 대신 chiplet들의 결합이다.<br />
모든 부분에 비싸고 느린 공정을 쓸 필요는 없으니까. 구세대 공정 재활용도 가능하고.<br />
<br />
하여간, 이렇게 해서 노광장비와 함께하는 반도체 장비들의 가격도 함께 엄청나게 올라버려서, IDM, 파운드리 분야에 새로운 회사가 들어오는건 어렵게 되었다.<br />
<br />
설계 분야에서도 어려움이 생겼다. 트랜지스터가 많아지긴 했는데, 이걸 다 써먹는 것은 어려운 일이었다.<br />
일단 코어 갯수를 늘렸는데, 코어를 다 활용할 수 없을 정도로 코어가 많아지자 내장 VGA를 넣게 되었다.<br />
<br />
코어가 많다고 왜 못쓰냐? 식당에 손님이 8명 올때 요리사 1-&gt;2명은 차이 크지만 2-&gt;16명은 의미가 없다? 책 다시 봐야될듯 여기는<br />
<br />
설계는 공정의 한계도 어느정도 떠맡아야 했다. 미세공정 때문에 SRAM의 신뢰성이 급격히 떨어져서,<br />
전압의 순간적인 변화, 온도에 의한 특성 변화, 우주 방사선 등에 의해 값이 바뀌는 문제들이 생겼다.<br />
<br />
미세 공정때문에 Cell의 크기가 줄어들었기 때문에, 1개 bit를 표현하기 위해 쓰는 전자 수도 줄어들었다.<br />
옛날에는 전자가 50개 이상이면 1, 아니면 0이었던게 전자 10개로 기준이 바뀌는 식이었다.<br />
이러면 외부에 의해 전자가 1개 추가됐을때, 오차 2%였던게 cell이 작아졌기 때문에 10% 오차가 되는거다. 당연히 오차가 더 생기게 됐다.<br />
<br />
SRAM 말고, DRAM도 회로 미세화에 의해 Rowhammer라는 결함을 겪게 된다.<br />
특정 위치 cell에 접근할 때 흐르는 전류때문에 인접한 데이터들이 변조되는 문제다.<br />
이것도, 미세화 때문에 한 비트를 저장하는 데에 쓰이는 Cell 크기가 줄어들어서 생긴거다.<br />
이 문제를 해결하는 데에도 설계 변화와 추가 트랜지스터가 필요했다.<br />
<br />
로직 관점에서는 ECC(Error Correnction Code)로 문제를 막았다.<br />
당연히 ECC를 구현하려면 추가 트랜지스터, 웨이퍼를 써야 했다.<br />
설계 관점에서는 미세공정 진행하면서도 비트당 저장되는 전하의 양을 유지해야 했다. -&gt; Cell을 위로 쌓게 되었다.<br />
<br />
높은 탑을 쌓는건 어려웠지만, 그래도 DRAM은 로직보다 설계가 간단한 편이라 그럭저럭 용량이 늘어나고 있었다.<br />
<br />
그리고, clock이 빨라짐에 따라, 하나의 회로 안에서 신호가 제대로 전달되지 않는 문제들이 점점 커지게 되었다.<br />
회로 스위칭이 전달되려면 전류가 그만큼 빠르게 변할 수 있어야 하는데, Si의 한계에 부딪혔다고 한다. 의미는 찾아봐야겠다.<br />
<br />
하여간 그래서 회로 크기를 줄여야 했다. 회로가 너무 크면 같은 회로 내에서 다른 상태를 갖게 되니까.<br />
근데, 하드웨어 크기를 줄이면 또 성능이 떨어졌다.<br />
이때, 회로를 쪼갠 작은 단위를 ‘파이프라인’이라 부른다.<br />
<br />
CPU 제조사들은 이로 인한 성능 저하를 막기 위해 캐시메모리를 늘리고 분기예측기를 추가했다.<br />
하지만, 이런 식으로 하드웨어를 추가하자 회로 배선이 더 어려워졌고, 비순차수행 구현도 더 어려워졌다.<br />
<br />
연결 관계가 복잡해져 상호작용하는 block 수가 늘어나자 칩 전체의 문제를 파악하는 것도 힘들어졌고, 공정이 미세화되면서 같은 회로도 특성이 나빠져 원래 설계를 못쓰게 되는 경우도 생겼다.<br />
Meltdown, Spectre 등 하드웨어 수준의 보안 결함은 언젠가 터질 수밖에 없는 일이었다.<br />
<br />
시장에도 변화가 생겼다. 배치해야 할 트랜지스터가 늘어났기에 회로 설계 인력, 검증 인력, 개발 기간 모두 늘어나게 됐다.<br />
그리고 파운드리의 제조 역시 복잡해졌기 때문에, 실물 칩을 받게 되는데에 걸리는 시간도 길어졌다.<br />
<br />
소규모 회사들은 한두번 실수하면 회사가 망할 수준이 된거다. 개발 비용이 그만큼 올라갔으니까.<br />
대기업이라고 해도, 옛날보다 칩을 많이 팔아야 수익을 낼 수 있었다.<br />
<br />
그래서 파운드리 뿐 아니라 팹리스 기업들도 대기업과 중소기업 차이가 엄청나게 벌어진다.<br />
이미 큰 고객을 갖고 있으면 매년 조금씩 개선해서 팔면 되는데, 새로 진입하려는 회사들은 첨단 공정, 커다란 칩 설계에서 발생할 수 있는 온갖 문제를 다 해결해야 한다.<br />
<br />
이렇게 하드웨어 발전이 어려워지고 느려지게 되자, 소프트웨어 회사들은 어떻게 했을까?<br />
일단 단일코어 성능이 더 이상 빠르게 발전할 수 없음을 받아들이고, 다중코어(멀티코어)에 맞는 프로그래밍을 새로 해야 했다.<br />
그러다보니 수많은 다중코어 버그, 최적화 문제들을 해결해야 했다. 작은 회사들에게는 큰 부담이 되었다.<br />
<br />
프로그래밍 회사들은 CPU 말고 다른걸 찾아나서기 시작했다.<br />
필요하다면 CPU 말고, 특별한 목적만을 위해 설계된 가속기를 도입하려고 했고, 시장에는 물리 연산 전용 카드가 등장했다.<br />
대규모 단순 수치 계산을 위해 VGA를 사용하려는 회사가 늘어났고, 엔비디아 CUDA라는 VGA 기반 프로그래밍 라이브러리를 제공하기 시작했다.<br />
<br />
극단적으로 규모가 큰 소프트웨어 회사들은 자신들이 쓸 하드웨어를 스스로 설계하려는 욕심을 갖게 됐다.<br />
예를 들어, 구글은 알파고에 필요한 연산을 가속할 카드인 TPU를 직접 설계해 사용했다.<br />
구글에 따르면, TPU는 알파고를 위한 연산에서는 전성비가 동시대 CPU, GPU에 30~80배였다.<br />
당연히 다른 작업들에서는 깡통이었지만 문제가 되지는 않았다.<br />
<br />
이렇게 되자, 인텔의 입지가 좀 위험해졌다. 소프트웨어 회사들이 CPU 말고 다른 하드웨어를 알아보고 있잖아!<br />
VGA가 중요해졌고, 회사들이 자기에게 맞는 칩을 만들게 되면서 설계의 중요성이 커지게 됐다. 팹리스의 영향력이 커지게 된 것이다.<br />
<br />
반면, 메모리 회사들은 상대적으로 적은 위협을 받았다. 뭐 메모리는 써야 할 것 아닌가.<br />
<br />
전세계 핸드폰 시장을 노키아가 주름잡던 2007년, 아이폰이라는 것이 등장했다.<br />
그 후로는 사람들이 핸드폰을 오래 켜놓고 사용하기 시작했고, 핸드폰에는 전력 절감에 집중한 부품들과 커다란 배터리가 들어가게 되었다.<br />
Seagate, Western Digital(WD)같은 회사들은 아이팟에게 겪었던 수모를 또 겪게 된다. (HDD 회사들)<br />
HDD가 핸드폰에 들어갈 수는 없으니까.<br />
<br />
대신, 메모리 회사들은 플래시메모리를 팔면 되니까 상관 없었다.<br />
<br />
메모리 회사들은 전력소모를 줄인 모바일 DRAM을 설계해 이걸 엄청 팔아먹으며 개꿀을 빨았다. 물량도 많이 팔리고, 부가가치도 많이 붙여서 팔았으니까.<br />
<br />
모바일 시장 최초의 AP는 ARM 기반이었다. 스마트폰 시장이 확장되자, 핸드폰 제조사들은 자신에게 맞는 AP를 찾아나섰다.<br />
퀄컴, 브로드컴, 삼성전자 LSI사업부 등이 AP 설계 분야에 들어왔다.<br />
<br />
삼성전자의 엑시노스9와 애플의 A11은 모바일 회사들의 CPU 설계가 상당히 발전해 있다는걸 보여준다.<br />
두 CPU는 각각 ARM 기반, x86-64 기반이다.<br />
<br />
스마트폰이 PC보다 안좋은 점 중 하나는 입력장치다. 그래서 비밀번호 입력 등이 PC보다 힘들다.<br />
그래서 스마트폰은 개인 인증을 위해 홍채, 지문 등 생체 인식과 영상, 음성 인식 등 AI 인식을 도입했다.<br />
이런 인증에서 중요한건 반응 속도와 정확도인데, 스마트폰 회사들은 AI 연산 가속용 칩은 NPU를 탑재해 전력을 아끼면서도 인식이 잘 되도록 했다.<br />
<br />
스마트폰 수요가 넘쳐나게 되자, 수요를 바탕으로 막대한 자본 투자가 가능해졌다.<br />
원래 파운드리 기업들의 미세공정 수준은 인텔에게 2~3배정도 밀렸는데, 스마트폰 수요가 폭증하고 저전력 고성능 AP의 수요가 늘어나기 시작하자 파운드리들이 자본투자를 바탕으로 인텔을 따라잡기 시작했다.<br />
<br />
사람들이 스마트폰으로 정보를 주고받으면서, 데이터 저장 수요 또한 폭증했다.<br />
유튜브도 많이들 보니까 영상도 저장해둬야 하고, 사람들이 찍는 사진 화질도 계속 올라갔다.<br />
<br />
근데 이러면, 서버에 정보를 기록하는 성능보다 정보를 빠르게 읽는 성능이 더 중요하다.<br />
<br />
영상이든 사진이든, 업로드 초기에 가장 시간당 조회수가 많고, 그 뒤로는 계속 떨어지게 된다.<br />
그래서 업로드 초기에는 고성능 저장소에 뒀다가, 나중에 저성능 보관소로 옮기면 비용을 아낄 수 있다.<br />
<br />
메모리 회사들과 팹리스 회사들은 그에 맞는 제품들을 내놨다.<br />
가장 빠른 저장소는 DRAM이니까 서버용 DRAM을 내놓고,<br />
DRAM보다는 느리지만 그래도 빠른 TLC 기반 SSD를 내놨다.<br />
그리고 이런 서버들의 수요를 맞추기 위해 RI(Read Intensive) 제품을 내놨다.<br />
<br />
메모리 제조사들은 1개 방에 4개 데이터를 저장할 수 있는 QLC 기술도 개발해 제품을 만들어내기 시작했다.<br />
<br />
2007년 발매된 아이폰에 의해, 반도체 시장을 다 먹고 있던 인텔이 크게 흔들렸다.<br />
저전력, 휴대성에 대한 사용자 요구가 엄청나게 늘어났다.<br />
그래도, 인텔은 서버 시장 성장을 타고 돈을 꽤 벌긴 했다.<br />
<br />
AP가 중요해져서 팹리스 회사들이 크게 성장했고, AP에는 미세공정도 중요하니 파운드리도 크게 발전했다.<br />
메모리 회사들은 DRAM과 낸드플래시 메모리를 팔아먹을 스마트폰 시장이 생겨서 좋았고, 서버에도 메모리를 엄청나게 팔아먹게 된다.<br />
<br />
2016년에 알파고가 이세돌을 이겼다. 그 뒤로 인공지능이 계속 언급되며 발전했는데, 인공지능 학습을 위해 새로운 하드웨어들이 필요해졌다.<br />
학습능력은 초당 얼마나 많은 자료를 학습시킬 수 있는가라서 고용량, 고대역폭 메모리가 필요한데, 메모리 회사들이 연구하던 HBM이 이 목적에 적합했다.<br />
<br />
수많은 회사들이 HBM을 사고싶어했으며, HBM이 없을 때에는 그래픽용 메모리인 GDDR도 고려했다.<br />
하이엔드 VGA도 AI 가속을 염두에 뒀기 때문에 HBM이 필요했다.<br />
<br />
일단 HBM을 쓰면 대규모 데이터를 빠르게 들여올 수 있는데, 이 대규모 데이터를 빠르게 처리하는게 또 중요해졌다.<br />
그래서 엔비디아같은 GPU 제조사들에게 기회가 생겼다.<br />
<br />
CPU는 수많은 조건들을 따져야 하는 복잡한 작업을 빠르게 하고, 그걸 GPU한테 던져주면 GPU가 대규모 데이터 작업을 한다.<br />
<br />
현대 슈퍼스칼라 CPU는 상당한 웨이퍼 면적을 분기예측기 등 조건문 실행시 성능을 향상시켜줄 수 있는 하드웨어에 할당하고 있고,<br />
DRAM으로부터 받아와 실행했던 수많은 분기문들을 명령어 캐시(I-Cache)메모리에 저장하고 있다.<br />
<br />
CPU에도 정수 연산장치가 있긴 하지만, GPU 정수연산 성능이 압도적이다.<br />
<br />
모니터의 각 픽셀들은 서로에게 영향을 주지 않기 때문에, 1천만개 픽셀이 있다면 1천개씩 1만번 계산해도 된다.<br />
<br />
엔비디아는 개발자들이 엔비디아 제품을 쓰도록 생태계를 조성하기 위해 CUDA를 출시했다.<br />
CUDA는 개발자들이 GPU 기반 프로그래밍을 할 때, CPU 위 C언어 등에서 개발할 때같은 익숙한 느낌으로 개발할 수 있게 해놨다.<br />
CUDA 문법이 C 문법이랑 비슷하다.<br />
<br />
CUDA 덕분에, 프로그래머들은 자신이 어떤 칩을 사용하는지 신경쓰지 않아도 되게 되었다.<br />
프로그래머 입장에서는 엔비디아가 제조한 VGA만 사용하면 된다.<br />
일단 CUDA 기반으로 프로그램을 짜면, 이후에 GPU를 교체하더라도 추가 작업을 할 필요가 없게 된 것이다.<br />
<br />
옛날에 인텔 CPU 기반 프로그래머들이 인텔 CPU 기반으로 프로그램 만들면 나중에 다시 짜야 할까봐 걱정할 필요 없었듯이,<br />
CUDA 개발자들도 엔비디아를 믿고 걱정을 덜었다. 그래서 CUDA 덕분에 코드 참조나 이직도 쉬워졌다.<br />
<br />
근데 스마트폰에서도 안면인식, 지문인식을 위해 AI연산이 필요했다.<br />
물론 스마트폰은 PC와 환경이 많이 달랐다.<br />
<br />
엔비디아가 만들던건 대형 기판에서 100W 이상 먹는 거대한 칩들이라,<br />
핸드폰에 들어가는 NPU들은 엔비디아가 아니라 삼성전자, 애플 등 대형 스마트폰 제조사와 퀄컴같은 팹리스 업체들이 만들게 됐다.<br />
<br />
FPGA의 원래 목적은, 웨이퍼로 실물 칩을 만들기 전에 설계를 검증해보는거다.<br />
만약 칩 설계 단계에서 실수가 일어나게 되면 제조 공정을 통째로 엎어버려야 하기 때문에 실수의 대가가 크다.<br />
심할 경우에는, 제조사가 문제 원인을 알면서도 해결을 포기한다.<br />
하드웨어 한두개 크기가 바뀌어 배선도 바꿔야 하고, 결국 칩 형태가 크게 변해 칩 특성과 수율을 처음부터 다시 맞춰야 할 수도 있다.<br />
<br />
인텔도 멜트다운, 스펙터 결함 수정을 거부한 바 있다.<br />
<br />
어쨌든, 이런 설계 실수는 막아야 한다. 이를 위해 수많은 방법론들이 나왔다.<br />
일단 컴퓨터 시뮬레이션이 있다. 편하긴 한데, 문제는 시뮬레이션이 아주 느리다는 거다. 1초 보려고 며칠 돌리는 식이다.<br />
<br />
게다가, 칩의 동작은 그 칩에서 동작하는 소프트웨어와 큰 연관이 있다.<br />
그렇다고 이미 느린 시뮬레이션에 거대한 소프트웨어까지 올려서 시뮬레이션하면 더 느려져서 아무것도 확인 못한다.<br />
<br />
이건 팹리스 업체들에게 큰 부담이다. 팹리스 업체들이라고 해도 칩만 만들어 파는게 아니라, 해당 칩에서 사용 가능한 펌웨어까지는 같이 내놔야 하기 때문이다.<br />
근데 컴퓨터 시뮬레이션이 너무 느리면, 시뮬레이션으로 소프트웨어를 개발하거나 동작을 확인하는게 불가능하다.<br />
<br />
그렇다고 실제로 칩을 만들어봐? 이건 비싸고, 오래걸리고, 뭐 하나 수정하면 또 만들어봐야 하는데 그건 현실적으로 말이 안된다.<br />
<br />
그래서 FPGA를 쓴다. PC에서는 프로그램 형태로 시뮬레이션이 돌아가지만, FPGA에서는 실제 웨이퍼에서 로직이 동작하듯 돌아간다.<br />
물론 실제 칩보다는 느리긴 하지만, 그래도 이걸 써서 소프트웨어를 돌려보고, 구조를 바꿔서도 돌려보는 것으로 설계를 검증하고 펌웨어를 개발할 수 있다.<br />
하지만 가격이 CPU보다 비싸다.<br />
<br />
원래 FPGA는 이렇게 칩 시뮬레이션을 위해 쓰는 것이었는데, 이걸 AI 연산 등 특정 연산을 위해 쓰려는 사람들이 나타났다.<br />
<br />
2017년, 중국계 채굴 업체 BitMain은 채굴 전용 칩 Antminer를 설계해, 1600W 전력으로 14TH/s를 달성했다.<br />
엔비디아 GPU를 8개 연결한 채굴기들은 비슷한 전력으로 1GH/s밖에 달성하지 못했다. 1만배가 넘는 차이다.<br />
<br />
그럼 무조건 ASIC(Application Specific Intergrated Circuit)이 이득인가?<br />
위험할 수 있는게, 채굴에 필요한 알고리즘이 변하면 원래 쓰던 ASIC를 쓸 수 없게 된다.<br />
이더리움도 채굴용 하드웨어, 전용 칩 사용을 막기 위해 알고리즘을 바꾼 적이 있다.<br />
<br />
그래서, 하드웨어 수정이 필요할 수도 있는 곳에는 FPGA를 쓴다. 아예 ASIC로 만들면 수정이 안되니까.<br />
전무님이 Hardwire로 구워버리면 싸다고 한게 이 얘기인가?<br />
결국 FPGA가 쓰이게 된건 ASIC 대신이고, ASIC는 GPU보다 특정 연산을 위해 쓰는거였다.<br />
<br />
결국, 컴퓨팅 성능 발전이 느려지고 AI가 대두되어 GPU를 많이 쓰는 거였으니,<br />
컴퓨팅 성능 발전이 느려져 FPGA를 쓰게 됐다고도 볼 수 있다.<br />
<br />
클라우드 서비스가 나오면서(아마존 AWS, 마이크로소프트 Azure), 기업들이 모두 HPC(고성능컴퓨터)를 살 필요 없이 클라우드를 이용하면 되게 되었다.<br />
<br />
RDIMM(고성능 DRAM 모듈)의 수요가 증가했고, 서버용 SSD 수요가 늘어나 낸드플래시 매출이 올라갔다.<br />
메모리 회사들도 돈벌고, 가상화 호스팅 업체들도 돈 많이 벌었다.<br />
<br />
휴대용 디바이스 수요 증가-&gt; 그거에 맞는 칩 설계하는 팹리스 성장 -&gt; 파운드리 매출도 성장<br />
<br />
전통적인 CPU 성능 증가에 한계, AI 등 응용프로그램 등장 -&gt; GPU, FPGA 등 특정 연산에 강점을 보이는 칩들이 연산용 반도체의 주도권을 갖게 됐다.<br />
<br />
기존 회사들은 CPU에 최적화된 프로그램을 버리고, 전용 가속기에 맞는 프로그램을 설계하게 됐다.<br />
<br />
메모리 구조가 바뀔 필요는 없었다. 메모리 회사들은 그냥 팔면 됐고,<br />
대규모 병렬 처리가 많이 필요해지면서 HBM같은 고부가가치 고성능 메모리에 대한 수요가 생겼다.<br />
<br />
전통적인 로직 반도체 회사들은 모바일 플랫폼 발전+GPU 수요 상승에 편승 못하고 매출에 타격을 입었지만,<br />
AWS 등 서버 서비스가 확장되어 서버용 CPU를 팔며 이득을 보긴 했다.<br />
<br />
그럼에도, 인텔은 ARM 서버들의 도전을 모두 물리치고 여전히 최강자로 군림하고 있다.<br />
여전히 기업 서버 시장에서 인텔을 대체할 회사는 없다.<br />
<br />
인텔은 FPGA 회사 Altera를 인수했다. 이게 반격의 실마리가 될 수도 있다.<br />
어쩌면 FPGA도 가상화하여 전세계에 임대하는 사업 모델을 만들어볼 수도 있을 것이다.<br />
이렇게 되면 칩 설계 회사들도 앱 개발 회사들처럼 작은 규모로도 운영할 수 있게 될거다.<br />
<br />
20세기 말, IT 붐이 불었을 때 전세계에는 수많은 검색엔진 회사들이 나타났다.<br />
야후, 마이크로소프트, 구글, 라이코스 등이 나타났다. 모두 나름의 알고리즘, 검색 랭킹 시스템, 기능이 있었다.<br />
<br />
검색엔진은 사람이 더 많은 쪽으로 몰리게 되어 있다. 사람들이 많이 검색하면 데이터가 더 많이 쌓여서 알고리즘이 더 정확해지고, 사람들이 또 몰리기 때문이다.<br />
<br />
야후는 이것저것 띄워서 종합 포탈사이트로 갔고, 구글은 그냥 검색엔진만 띄워놨다. 결국에는 구글로 다들 모였다.<br />
<br />
구글은 회사규모가 커지자 다른걸 해보기 시작했다. 알파고도 처음에는 76개 GPU를 썼지만, 이세돌이랑 바둑둘때는 48개 TPU를 썼다.<br />
이게 되자 구글은 자신감을 얻고, 하드웨어를 다 스스로 만들기로 했다.<br />
<br />
아마존은 Graviton이라는 ARM 기반 CPU를 발표했다.<br />
Graviton은 지금은 AWS에서만 쓰이고 있지만, 나중에 어디 쓰일지는 모른다.<br />
<br />
마이크로소프트도 자체 제작 CPU를 만들 것이라고 발표했다.<br />
<br />
스마트폰 시대가 오자, 피처폰 + 특수목적 폰 + 업무기능이 되는 PDA를 만들려던 Motorola와 Blackberry는 존재감을 잃어버렸고,<br />
노키아는 시장의 최강자였으나 애플/삼성전자에게 시장을 빼앗기고 마이크로소프트에게 매각됐다.<br />
그러고는 윈도우폰을 만들더니 죽어버렸다.<br />
<br />
스마트폰으로 전환해 살아남은 회사들도 고민이 많았다. 일단 기능이 많아져 배터리가 중요해졌다.<br />
메모리도 중요해졌는데, 스마트폰 회사들의 메모리 발주량이 워낙 많다보니 메모리 회사들도 스마트폰 회사가 원하는대로 메모리를 만들어주고, 전담 지원팀도 만들어줬다.<br />
<br />
메모리 회사 입장에서는 고객사와 친해져서 중국 등 다른 회사들이 잘 못들어오게 되어 좋았고,<br />
스마트폰 회사 입장에서는 메모리 회사들이 내가 원하는 스펙의 메모리만 찍어내주니 다른 중소 경쟁사들이 못들어와서 좋았다.<br />
중소 기업들은 주문량이 적어 메모리 회사들이 지원을 안해준다.<br />
<br />
애플이 아이폰을 처음 만들때, 처음에는 인텔쪽 CPU들을 알아봤다고 한다. 하지만 너무 크고 전력도 많이 먹었다.<br />
성능이야 좋았는데, 당시 해봐야 인터넷 검색 정도 할건데 그정도 성능이 필요하지는 않았다.<br />
<br />
이때, 애플은 삼성전자의 S5L8900을 발견했다.<br />
성능은 인텔에 비하면 구데기였지만, 전력 소모량과 크기를 맞출 수 있는 AP였다.<br />
AP에 eD램도 부착되어있어 별도 DRAM도 필요 없었고, 그렇기에 공간을 아낄 수 있었다.<br />
설계, 파운드리, 패키징 다 하던 삼성전자라 가능한 일이었다.<br />
<br />
S5L8900은 x86 대신 ARM의 ISA를 쓰고 있었다.<br />
원래 ARM은 저전력에 많이 쓰였다.<br />
그래서 스마트폰은 이후로도 ARM 기반으로 성장했다.<br />
<br />
2010년, 애플의 아이폰을 따라 삼성갤럭시가 나온다. 갤럭시와 함께 구글이 모바일 시장에 들어온다.<br />
모두 ARM 기반이라 호환성 문제도 없었고, ARM 기반 소프트웨어 개발을 위한 수많은 프로그램들이 나와 수많은 벤처기업들이 사업에 뛰어들었다.<br />
<br />
인텔은 미세공정의 우위 + 노키아를 인수한 마이크로소프트와의 동맹 관계로 베이트레일 + 윈도우폰 조합으로 반격을 준비했으나 잘 안됐다.<br />
<br />
미세공정도 AMD가 인텔을 따라잡아버렸다.<br />
<br />
인텔이 메모리 시장 진출을 위해 내세웠던 SCM(Storage Class Memory) 사업도 큰 곤란을 겪고 있다.<br />
3D  XPoint라는 물건을 내놨다고 한다.<br />
<br />
3D XPoint 자체의 성능은 D램보다 나쁘지만, 용량이 커서 하드디스크나 SSD에 덜 다녀와도 된다.<br />
하지만 이걸 만드는게 생각보다 어려웠다고 한다. 그래서 잘 안됐다. Optane? Lehi?<br />
<br />
2015년 7월, 인텔은 167억 달러(거의 1년치 순이익)로 FPGA 회사 Altera를 샀다.<br />
2017년에는 AMD에서 그래픽을 총괄하던 Raja Koduri를 영입하고, Xe라는 그래픽 칩 개발을 시작했다.<br />
<br />
인텔이 왜 이런 짓을 했냐? 칩간 연결이라고 보고 있다.<br />
파이썬, C언어 하는 사람이 FPGA 쓰려고 Verilog 해야 한다고 하면 힘들거다.<br />
하지만 칩들이 통합된 개발 환경이 생긴다면, ‘FPGA에서 뭔가를 돌려라’ 정도 명령으로 FPGA를 쓸 수 있다.<br />
<br />
인텔은 CPU 회사라서, CPU랑 상호작용 필요한 부품들을 함께 패키징해 성능을 끌어올릴 수 있다.<br />
<br />
인텔이 포베로스같은 칩간 연결기술에 투자한것도 이런 통합을 위해서다.<br />
<br />
라자 코두리가 인텔에 와서 설계하기 시작한 Ponte Vecchio 칩은 신형공정/구형공정, 고성능/저성능 연결기술 모두가 사용됐다.<br />
포베로스 3D 패키징도 사용됐다고 한다.<br />
<br />
이런 하드웨어 수준의 통합을 소프트웨어로 만들고자 하는게 인텔의 OneAPI다.<br />
그냥 프로그램 하나 만들어두고, 새로운 하드웨어를 추가하면 그걸 자동으로 써서 최종 소프트웨어 성능이 자동으로 늘어나게 하는거다.<br />
<br />
인텔은 2021년에 파운드리 사업에 재진출하며, 인텔 파운드리 고객들에게  x86 칩 설계 IP를 제공하겠다고 했다.<br />
x86은 지금도 세계에서 가장 강력한 영향력을 행사하는 IP로, x86에 접근해서 커스텀한다면 서버 회사들은 엄청난 효율의 서버를 만들어낼 수 있을 것이다.<br />
<br />
2021년 3월, 인텔 행사에서 마이크로소프트 CEO는 인털 설계로 마이크로소프트만의 커스텀 칩을 만들겠다고 발표했다.<br />
<br />
인텔이 발표한 신규 제품 Alder Lake는 인텔 최초로 ARM의 big.LITTLE에 대응하는 이종 혼합 코어가 들어가 있다.<br />
여기 포함된 고효율 코어(Efficient core, E코어)는 인텔의 고효율코어(Performance core, P코어)와 비교했을 때,<br />
면적당 성능이 2배정도 되는 것으로 보인다. E코어 발전속도가 지금 좀 빠르다.<br />
*P코어 고효율코어 맞나?<br />
<br />
인텔은 어쨌든 스마트폰도 놓치고 공정도 따라잡혔지만, 생태계에 많은 투자를 해놨다.<br />
<br />
ARM은 다른 팹리스처럼 칩을 설계하고 위탁 제조해서 파는것도 아니고,<br />
타사의 칩을 받아 원하는 대로 만들어주는 것도 아니고,<br />
자사 칩, 칩의 일부, 자사 칩이 수행 가능한 ISA를 판다.<br />
<br />
ex)<br />
1코어 라이선스(ARM이 직접 설계한 코어 라이센스 + ARM ISA 사용 권리):<br />
삼성전자 엑시노스 LITTLE 코어, 퀄컴 스냅드래곤 LITTLE 코어, 화웨이 기린 980코어들<br />
<br />
아키텍처 라이선스(ARM ISA 사용 권리, 칩 설계는 직접):<br />
삼성전자 엑시노스 M 아키텍처, 퀄컴 고성능 Kryo 아키텍처, 애플 자체 아키텍처<br />
<br />
모바일 시대가 도래하자, 애플을 시작으로 ARM의 설계는 불티나게 팔려나가기 시작했다.<br />
<br />
수많은 회사들이 ARM의 코어 디자인을 구입하고, 자신만의 주변회로를 만들어 CPU를 만들었다.<br />
삼성전자, 퀄컴은 ISA만 구입하고, 코어는 스스로 설계했다.<br />
ARM과 거래하면 일단 AP를 만들어볼 수는 있었다.<br />
<br />
ARM에게는 피처폰 시절부터 사용해온 Mali라는 그래픽 솔루션도 있었다.<br />
<br />
시장이 계속 발전하면서, 발전 속도가 느려지기 시작했다. ARM은 고객들과 계속 대화해보며 문제를 들어보고, 수정 계획을 세웠다.<br />
<br />
ARM의 전성비 문제를 해결하기 위한 해결책이었던 빅리틀은, 웨이퍼 위에 최고 전성비를 갖는 구간이 다른 2개 CPU를 배치한다.<br />
가벼운 작업에서는 가벼운 작업에 효율이 좋은 가벼운 코어를 쓰고, 무거운 작업에서는 거대한 슈퍼스칼라 코어를 쓴다.<br />
간단해 보이지만, 또 쉬운 일은 아니었다.<br />
<br />
ARM은 이 계획을 미리 회사들과 공유했다. OS가 좀 고생해줘야 하는 방식이라 그렇다.<br />
OS가 작업의 경중을 잘 따져 코어들에게 분배해야 하기 때문이다.<br />
<br />
ARM은 이렇게, x86 진영이 해내지 못했던 이종 코어간의 컴퓨팅 모델을 만들었다.<br />
<br />
인텔은 2020년에야 최초로 Lakefield라는 이종 아키텍처 CPU를 소량 내놓았고,<br />
2021년 말에야 데스크탑용으로 알더레이크라는 CPU를 내놓았다.<br />
인텔은 이때서야 이종 아키텍처 CPU 시대에 합류하게 됐다.<br />
<br />
ARM이 차지하지 못한 시장도 있었다. 서버 시장은 전통적으로 최상단에 메인프레임이라 부르는 시스템이 존재하며,<br />
여기는 IBM의 POWER 아키텍처가 강력하게 자리잡고 있었다. 여기는 인텔도 못들어간 시장이다.<br />
괜히 메인프레임 바꿨다가 서버가 몇초라도 멈추면 엄청난 돈을 물어내야 한다.<br />
<br />
메인프레임 아래에는 서버 시장이 있고, 여기는 인텔이 먹어치운 시장이다.<br />
퀄컴, 캐비엄, Applied Micro 등 회사들이 ARM기반 칩을 내놓긴 했지만, 시장의 선택을 받지는 못했다.<br />
<br />
ARM 서버들은 Throughput(최대 처리 용량)은 뛰어났지만, 반응성(개발 코어의 최대 속도)이나 메모리 성능에서는 인텔을 이기지 못했다.<br />
<br />
게다가 페이스북, 마이크로소프트, 구글 등 거대 고객사가 ARM기반 CPU로 움직이려면,<br />
그들이 수십년간 x86위에서 개발해온 소프트웨어가 ARM기반에서 작동하도록 다시 새로 Compile해야 한다는 뜻이다.<br />
<br />
괜히 바꿨다가 온갖 버그가 발생해 서비스가 느려지거나 멈추기라도 하면 그건 엄청난 손실이다.<br />
<br />
2021년, ARM의 서버 진출은 AWS의 극히 일부 서비스(EC2 A1), CloudFlare의 edge server 정도로 한정되어 있다.<br />
<br />
데스크탑 시장은? ARM기반 CPU에서 쓸 프로그램이 없으니 필요가 없다.<br />
스마트폰에서 돌아가는 ARM 기반 프로그램들은 x86환경에서 크로스 컴파일러를 이용해 만들어진다.<br />
저전력 환경에서 돌아가는 수많은 프로그램들은 저전력 환경에서는 못만드는 것이다.<br />
<br />
ARM의 모바일 GPU는 성능, 시장 점유율에서 퀄컴, 파워VR을 이기지 못하고,<br />
AMD의 GPU 설계인 RDNA는 삼성전자와의 협업을 통해 모바일에 진출하려 한다.<br />
<br />
결국 ARM은 모바일 CPU 말고는 제대로 먹은 시장이 없는데, 스마트폰 출하량은 감소하기 시작했다.<br />
그리고 퀄컴, 화웨이, 삼성전자같이 AP 대부분을 스스로 설계하는 회사들 비중이 커지면 ARM도 위험해질 수 있다.<br />
ISA만 라이센싱하는건 설계 자체를 라이센싱하는것보다 훨씬 이익이 적다.<br />
<br />
ARM은 어쨌든 새로운 시장을 찾아내야 하는 상황이다.<br />
그래도 ARM은 자체 칩을 제조하지 않기 때문에 경쟁하기보다는 협력하려는 회사가 많다.<br />
엔비디아가 ARM을 합병하려 하기도 했었는데, 잘 안된 것으로 알고 있다.<br />
<br />
원래 GPU는 프로그램 수행의 결과대로 움직여 영상을 띄워주는 정도의 일을 했다.<br />
그러다가 CPU 성능 향상이 한계에 다다르고, 대안으로 떠오른 기계학습이 주목받게 됐다. 그런데 GPU가 기계학습에 적합했다.<br />
CPU도 4~20개 등 다중코어 시스템이 되긴 했지만, 여전히 거대한 디코더와 비순차 수행기 등 거대한 하드웨어가 필요했다.<br />
<br />
엔비디아가 행운을 공짜로 얻은건 아니고, 오래 전부터 GPU로 대규모 연산이 필요한 시장에 들어가려는 노력을 해 왔다.<br />
<br />
엔비디아는 2008년 Ageia라는 물리연산 가속기 전문 회사를 인수해 대규모 물리학 시뮬레이션이 필요한 분야에 진출하려고 했다.<br />
물리 연산도 특정 정지된 시간에 공간상에 표시된 물체들의 속도 등을 각자 따로 계산하면 되는 작업이기 때문이다.<br />
<br />
CUDA는 2006년에 처음 출시됐다. 엔비디아 그래픽카드기만 하면 쓸 수 있었고, 물리학 시뮬레이션과 인코딩/디코딩 등에 쓰였다.<br />
그러다가 AI가 주목받자 CUDA가 더 중요해졌고, 엔비디아는 GPU를 GPGPU라고 재명명하기도 했다.<br />
GPU는 단순히 모니터에 그림을 띄우는 칩이 아니라는 엔비디아의 선언이었다.<br />
<br />
GPU 경쟁사로는 CPU와 GPU 둘 다 하는 AMD 정도가 있었는데, AMD는 CPU를 말아먹고 죽어가던 상태였다.<br />
<br />
인텔, ARM도 GPU에서 엔비디아를 이길 수 없었다. ARM은 대형 하드웨어를 두고 고성능 컴퓨팅을 시도해본 적이 없었고,<br />
인텔은 Larrabee라는 x86 기반 병렬 프로세서를 개발하려 했지만, 제대로 된 성능을 내지 못했다.<br />
<br />
인텔의 그래픽카드는 2017년 라자 코두리가 AMD에서 옮겨오고 나서야 개선되기 시작했고,<br />
2020년 들어서야 쓸만한 물건이 나오게 된다.<br />
<br />
이렇게 엔비디아는 그래픽카드에 있어 독보적인 위치를 갖게 되었으며, 돈을 많이 벌어 TSMC의 최첨단 공정을 쓰게 됐다.<br />
<br />
GPU 가격은 100~200만원 정도인데, 머신러닝 전용 카드들의 가격은 천만원이 넘는다.<br />
그리고 이제는 단일카드만 파는게 아니라, 일종의 소형 슈퍼컴퓨터같은 형태로 팔기도 한다.<br />
여러개의 엔비디아 A100 카드를 엮어 만드는 엔비디아 DGX A100의 초기 출시가는 19만9천달러였다.<br />
근데 이런 비싼 제품들도 늘 공급부족이다.<br />
<br />
이런 가격대가 가능한건, 머신러닝이 엄청난 부가가치를 창출해 여기 참여한 회사들의 매출, 실적이 폭등해 부자가 되었기 때문이다.<br />
그러니 비싼돈 내고 이런것들을 사간다.<br />
<br />
엔비디아는 기존 PC 게이밍 시장은 그대로 유지한 채로, 슈퍼컴퓨팅, 서버, 자율주행차 등 수많은 분야들을 차지했다.<br />
Tegra라는 모바일 시장용 칩은 실패하긴 했다.<br />
<br />
근데 Tegra는 아예 뒤진건 아니고, 모바일 시장 말고 인공지능 에지(Edge AI) 시장을 개척하고 있다.<br />
엔비디아의 머신러닝 솔루션인 Jetson 시리즈의 칩으로 사용된 것이다.<br />
<br />
현재 Tegra는 초저전력부터 고성능까지 다양한 형태로 설계되어,<br />
전자는 머신러닝 입문자들이 좋아하는 Jetson 나노에,<br />
후자는 자율주행에도 사용 가능한 고급형 모델인 Xavier에 투입하고 있다.<br />
<br />
그리고, 지금 엔비디아가 갖춘 수많은 개발 인프라와 개발자 집단, 하드웨어 성능을 따라잡는 회사가 나오기는 힘들어 보인다.<br />
직업 시장에는 CUDA 프로그래밍을 전문으로 하는 프로그래머들이 이미 많이 자리잡았다.<br />
<br />
이 지위를 유지하기 위해 엔비디아는 지속적으로 CUDA에 새로운 기능들을 도입하고 있으며,<br />
Jetson Nano같이 CUDA를 사용하는 개발자 보드를 개발해 초보자들이 계속 들어오도록 만들고 있다.<br />
Jetson Nano는 카메라까지 사도 20만원이면 살 수 있다고 한다.<br />
<br />
엔비디아는 Jetson Nano에 약 10줄정도 코드만 짜면 이미지 인식 등을 시킬 수 있도록 ‘Jetson Inference’의 예시를 무료로 제시하고 있다.<br />
우리와 함께하면 인공지능 쉽다! 같은 느낌이다.<br />
<br />
하지만 엔비디아에게도 위협은 존재한다. 엔비디아의 새로운 상품들은 대부분 GPGPU를 중심으로 하고 있으며,<br />
필요한 경우 기존 GPGPU 근처에 ARM 프로세서를 결합하여 제어 능력을 부여한 것이 대부분이다.<br />
엔비디아는 이런 소형 프로세서의 자체 설계를 갖고 있지 않다.<br />
<br />
그래서, 만약 인텔같은 거대한 기업이 고성능 로직 프로세서와 기존 x86 생태계에 FPGA나 GPGPU를 결합하고,<br />
이를 통해 강력한 부가가치를 만들어내기 시작하면 엔비디아에게 큰 위협이 될 수 있다.<br />
<br />
지금은 GPU 분야에서 힘을 쓰지는 못하고 있지만, x86의 2인자 AMD도 엔비디아에게 위협이 될 수 있다.<br />
<br />
아니면 구글같은 강력한 소프트웨어 기업이 알파고에서 했던 것처럼 자체 가속기를 설계해 사용하고,<br />
나아가 해당 가속기에 맞는 소프트웨어 환경을 구축하기 시작할 수도 있다.<br />
이렇게 되면 엔비디아는 고객도 잃어버리고 생태계 주도권도 잃어버리게 된다.<br />
<br />
엔비디아 역시 텐서 연산기를 칩에 내장하는 등의 방식으로 성능 우위를 유지하려 한다.<br />
하지만, 결국 필요한 연산의 종류를 결정하는건 소프트웨어 기업이라 언제나 한발 늦게 될 수도 있다.<br />
<br />
그래서 엔비디아가 ARM 인수를 시도했을 것이다.<br />
<br />
TSMC는 B2B가 중심이라 이름이 알려진 기업도 아니었고, MCU, PMIC 등을 위탁생산하는 기업이었다.<br />
대만의 UMC와 경쟁하긴 했지만, 잘 알려지지는 않은 상태였다.<br />
<br />
이러다가 모바일 혁명이 시작됐다. 사람들은 스마트폰에 원하는게 더 많아졌고,<br />
결국 스마트폰 AP에는 저전력 + 고성능이 필요해졌다.<br /></p>

<p>저전력+고성능을 달성하려면 공정 미세화가 필요했다. -&gt; TSMC는 스마트폰 시대가 오자 큰 돈을 벌었다.
머신러닝 유행도 TSMC에게 큰 도움이 되었다.
GPU는 팹리스 업체인 엔비디아가 설계하는건데, 얘네도 제품 만들려면 TSMC에 와야 했다.</p>

<p>TSMC의 자본투자 규모는 2015년에 인텔을 넘어섰다. TSMC는 엄청나게 투자를 하는 기업이다.</p>

<p>TSMC는 구세대 공정도 많이 열어놓고 있다. 작은 회사들은 칩 설계를 매년 바꾸기 힘들다.
ex) 7nm에서는 하이엔드용 스마트폰 AP, CPU, 엔비디아 GPGPU등을 생산하고,
14~28nm에서는 중간급 스마트폰 AP, SSD용 마이크로컨트롤러 등 생산</p>

<p>중소기업들은 안전하게 옛날 공정들 많이 쓴다.
괜히 새로 설계했다가 문제생기면 감당 안되고, 새로 설계하는 것도 힘들다.</p>

<p>시스템 전체에서 사용하는 에너지가 너무 커서, 전력을 조금 아끼더라도 티가 안나는 분야에서는 10년 전 공정도 쓴다. 자동차 ECU 등.</p>

<p>2012년만 해도, TSMC 뿐 아니라 글로벌파운드리, 삼성전자, UMC, SMIC 등이 있었지만,
TSMC, 삼성전자만 남게 되었다. 그래서 얘네한테 주문이 다 몰리게 되었다.</p>

<p>글로벌파운드리가 생산하던 IBM의 POWER CPU들은 삼성전자에게 넘어갔고,
AMD의 물량은 TSMC에게 넘어갔다.</p>

<p>그렇다고 TSMC가 가격경쟁으로 삼성전자 파운드리를 말려죽이기에는 삼성전자 파운드리가 너무 큰 기업이다.
아마 이대로 계속 갈 것이다.</p>

<p>TSMC의 단점?
먼저 뭔가 만들어내지는 못하는 회사다. TSMC가 뭐 설계하려고 하면 고객들이 항의할거다.</p>

<p>구글은 검색엔진 회사로 시작해 이것저것 하는 회사다. 모바일에서는 안드로이드 OS로 마이크로소프트 OS 독점을 깼고,
폐쇄적이던 애플을 대신해 플레이스토어 중심의 새로운 ARM 기반 소프트웨어 생태계를 만들었다.</p>

<p>구글은 원래 반도체 엄청나게 사들이는 회사였다. 메모리, CPU, SSD 등 사가서 데이터센터를 지었다.
이제는 스스로 설계도 한다. 구글의 TPU는 구글이 스스로 설계하고 TSMC 28nm 공정으로 만든 칩이다.</p>

<p>이제는 소프트웨어 대기업들이 하드웨어 설계를 소프트웨어처럼 자신들의 지적 자산으로 인식하기 시작했다.
CPU 성능 향상이 느려지자, 소프트웨어로 할 수 있는 일이 느려져서 소프트웨어 회사들이 불편을 느꼈다.
소프트웨어 회사 규모도 커졌으니, 그냥 직접 만들자는게 이들의 생각이었다.</p>

<p>원래 칩을 설계하면 칩 제조사랑 소프트웨어 회사가 대화를 많이 해야 한다.
근데 구글은 그냥 같은 회사 내에서 대화하면 됐기 때문에, 이 과정이 훨씬 빨랐다.
그리고 원래 제조사들은 칩의 모든것을 보여주지 않고 한정된 스펙만 보여주지만,
구글 내에서는 모든 성능을 다 보면서 스펙에 맞는 소프트웨어를 만들 수 있었다.</p>

<p>구글은 TPU를 만들고 나서, 이 칩의 성능, 전력소모량, 아키텍처까지 공개했다.
이 칩에 적용할 사용자 라이브러리인 Tensorflow를 open source로 전환했고,
구글이 썼던 학습 데이터 세트도 공개해놨다.</p>

<p>전세계 모두가 이걸 쓰게 되고, 쓰면서 버그 리포트까지 해준다면 구글도 손해보는건 아니다.</p>

<p>구글은 TPU에서 멈추지 않고, 2021년 VCU라는 칩을 공개했다.
이 칩은 유튜브 영상 압축 및 변환만 전문으로 하는 칩이다.
이걸 쓰면 인텔 CPU 기반 서버 쓸때보다 훨씬 비용이 절감된다고 한다.</p>

<p>구글은 자신들이 선호하는 압축 방식(H264 VP9)을 선택해서, CPU가 주는 유연함이 필요 없는 상황이다.</p>

<p>삼성전자는 지난 40년간 엄청나게 발전했다.
메모리에서는 NEC, 히타치, 도시바 등 일본 메모리 업체들을 이겼고,
디스플레이에서는 Sharp를 넘어섰고,
가전에서는 소니를 제쳤다.
노키아는 스마트폰 시대가 시작되자 삼성전자에게 밀려 사라졌다.</p>

<p>기존 가전 및 IT 업체들과 삼성전자의 차이는 압도적인 수직 계열화를 통한 최적화 및 부가가치 흡수 능력이다.
삼성전자는 강력한 자체 공급망을 갖고 있다.</p>

<p>스마트폰 시장: 저장소, DRAM, 디스플레이, AP설계, AP제조, 전자부품, OS인데 삼성전자는 이중 OS 빼고는 다 스스로 만든다.</p>

<p>수직 계열화의 힘은 스마트폰 저장소(eMMC, UFS 등)에서도 나타난다.
eMMC, UFS같은 제품이 나오려면 반도체 공장에서 일단 낸드 기술이 완성되어야 한다.
이 raw NAND를 신형 컨트롤러에 부착하고, 그에 맞는 펌웨어(제어프로그램)을 탑재하고 수일~수개월간의 테스트를 거쳐야 한다.</p>

<p>그래서 UFS 만들 때에는 정말 다양한 회사들이 서로 소통해야 하는데,
삼성전자는 모든게 회사 안에서 이루어질 수 있다. 이게 삼성전자의 장점이다.</p>

<p>삼성전자는 메모리의 표준을 정하는 JEDEC, NVMe 표준 제정 및 수정에 깊게 관여하고 있다. 뭔 kv-SSD라는 것도 만들었다.</p>

<p>하지만 삼성전자도 소프트웨어는 제대로 못만들고 있다.
스마트폰용 바다OS는 시장에 제대로 나가보지도 못하고 사라졌다.
타이젠은 일부 스마트워치 제품들에서만 라이센스 비용 절약 목적으로 사용된다.
원래 목적이었던 사물인터넷 진출이나 디바이스간 연결은 제대로 안되고 있다.</p>

<p>삼성 그룹으로 보면 자체 브랜드 아파트까지 있기 때문에, 집-가전제품-스마트폰 연결성을 실현하기에 좋은 환경을 갖고 있다.
하지만 제대로 못해내고 있다.</p>

<p>빅스비도 제대로 안되고 있다.
OS, AI 모두 시장 초기 진입이 중요한 소프트웨어들이다보니, 삼성전자의 fast follower 전략의 한계일 수도 있다.</p>

<p>소프트웨어 개발자는 OS를 통한 유통망을 쓰게 된다. 소프트웨어 개발자 혼자 유통망을 만들기는 너무 어렵다.
개발자는 안드로이드, iOS에서만 잘 작동하게 만들면 유통은 플레이스토어, 앱스토어에서 해준다.</p>

<p>새로운 OS를 만들어서 여기 끼어들려고 해도, 소프트웨어 개발자들은 얼마나 버틸지도 모르는 새로운 OS를 위해 소프트웨어를 만들 이유가 없다.</p>

<p>AI는 쌓인 데이터가 그 성능을 결정하게 된다.
신경망 구조에 대한 연구는 거의 끝났고, 이제는 학습용 데이터의 양과 질이 중요하다.
특히 구글은 검색 엔진을 다 먹어버렸기에, 데이터에서는 경쟁자들이 구글을 따라가기 어렵다.</p>

<p>구글은 CAPTCHA(로봇이 아닙니다)같은 무료 보안 프로그램, QuickDraw같은 놀이용 프로그램으로 AI 능력을 향상시키고 있다.
이렇게 전세계 사람들이 구글의 AI성능을 올려주고 있는데, 삼성전자가 AI 인력 몇명을 채용해봐야 이걸 뒤집기는 어려울거다.</p>

<p>대리인 문제: 일을 시키는 사람과 일을 하는 사람 사이 정보 비대칭으로 생기는 업무 효율성 감소, 감시비용 증가
구글은 대리인 문제도 없이 AI를 키우고 있다.</p>

<p>그래도, 삼성전자는 OS, AI를 제외한 애플리케이션들에서는 성과가 있다.
삼성전자는 일반적인 어플 개발 회사들과는 달리, 강력한 하드웨어적 지원이 가능하다.</p>

<p>간편 페이:
MST: 대리점 수가 많지만, 스마트폰에 하드웨어가 필요하다. 플라스틱 카드들이 MST 방식이다.
NFC: 이미 폰에 탑재되어 있지만, 가맹점 단말기가 부족하다.
QR코드: 하드웨어가 필요 없지만, 개별 제휴가 필요하다.</p>

<p>삼성전자는 하드웨어 업체 LoopPay를 인수해 MST 방식 관련 기술을 얻었고, 2015년 삼성페이를 출시했다.</p>

<p>삼성전자는 삼성페이로 수수료 장사를 하지 않을 것이라 말해 은행권과 카드사의 지지를 이끌어냈다.</p>

<p>그 외에도 삼성전자는 다른 소프트웨어 분야를 선점하기 위한 노력을 하고 있다.
예를 들어 기어VR을 출시하면서 동시에 개발자용 VR기기를 발매했다.
아직 표준화가 안된 VR 시장에 투자한건데, 사실 VR 시장이 활성화되지는 못했다.</p>

<p>삼성전자는 2019년 세계 최초로 7나노 EUV를 도입했지만, EUV 출력 부족과 공정특성 부족으로 고생했다.
애플, 퀄컴은 삼성전자오 제품 포트폴리오가 겹쳐 갈등을 빚었고, 삼성전자는 일단 파운드리를 별도 사업부로 분리했다.</p>

<p>TSMC는 3nm에서도 FinFET을 유지하기로 했는데, 삼성전자는 3nm에서 GAAFET을 선제적으로 도입해 도전장을 내놓을 계획이다.</p>

<p>생태계를 미리 만들어놓기 위해, 삼성전자는 2019년 SAFE(Samsung Advanced Foundry Ecosystem)라는 파운드리 생태계를 런칭하고, 2022년에 양산될 3nm공정의 PDK를 2019년에 설계 회사들에게 배포하는 등 노력을 했다.</p>

<p>삼성전자는 지금도 거대한 규모를 가진 기업이다. 메모리, 파운드리, 스마트폰을 다 갖고 있다.
소프트웨어까지 갖추게 된다면 진짜 차원이 다른 기업이 될 것이다.</p>

<p>삼성페이는 초기에 MST를 통해 사용자를 끌어모았지만, 해외 사용자 확보는 미흡했다.
어쨌든 MST나 갤럭시 S6의 엣지 전용 앱 등 삼성만이 할 수 있는 일들을 하다보면 언젠가 흥할 것이다.</p>

<p>2015년, 중국 전국인민대표회의에서 ‘중국제조2025’라는 산업 계획을 발표했다.
향후 핵심동력이 될 10대 산업을 선정하고, 2025년까지 중국이 제조업 수준을 독일, 일본 수준으로 끌어올리겠다는거다.</p>

<p>그 분야 중 하나가 ‘차세대 정보기술’인데, 이 과정에서 중국 정부는 반도체의 핵심 설계기술을 확보하고 핵심 칩을 생산할 것임을 분명히 했다.</p>

<p>한국은 대중 수출 비중이 크고, 첨단 제조업이 국민 경제에서 차지하는 비중이 높아
중국이 첨단 제조업 국가가 될 경우 피해가 클 수 있다.</p>

<p>중국은 그 뒤 반도체에 돈을 쏟아붓기 시작했다.
중국은 서방 국가에서 반도체 회사가 매물로 나올때마다 언제나 적극적으로 M&amp;A를 시도한다.</p>

<p>2015년, 미국의 낸드 및 낸드 기반 솔루션 업체인 샌디스크가 매물로 나왔을 때도 인수합병을 시도했고, 마이크론을 인수하려고 시도한 적도 있다.</p>

<p>이게 잘 안되자, 샌디스크를 인수하려 했던 미국의 HDD 전문업체인 웨스턴디지털에 지분참여를 시도했다.</p>

<p>그 다음해에 도시바가 회계부정 사건으로 도시바 메모리를 분리하려 매각하려 했을 때도 컨소시엄 형태로 입찰에 참여했다.</p>

<p>이런 인수 시도들에서 미국이나 서방세계의 견제로 큰 효과를 보지는 못했지만, Spreadtrum같은 회사들의 지분은 꾸준히 매입하고 있다.</p>

<p>중국은 거대 IT기업들도 많고 스마트폰 제조사들도 많지만, 반도체 부품, 개발 툴은 서방 국가들에게 의존하고 있다.
그래서 이런 반도체 굴기를 하는 것일 것이다.</p>

<p>2019년 1월, 수원지법은 한국에서 D램설계를 주도했던 김모씨의 CXMT(창신 메모리 테크놀로지스) 이직을 막아달라는 삼성전자의 요구를 받아들여 11월까지 거기서 일하지 말라고 했다.</p>

<p>이때, 법원은 중국 반도체 회사들의 D램 설계 기술이 3~10년정도 뒤처진 것으로 보인다고 했다.
메모리에서 3~10년이면 아예 상대가 안되는 수준이다.</p>

<p>칩이 커질수록 수율이 떨어진다. 그래서 작은 칩은 수율 96%, 큰 칩은 수율 85% 이런 식일 수 있다. 그러면 그만큼 원가 경쟁력이 떨어진다.</p>

<p>그래서 거대한 칩은 결함 없이 개발하는게 사실상 불가능하다.
엔비디아에서는 일단 대형 칩을 기준으로 제조한 뒤, 불량이 난 영역을 죽이거나 잘라서(Cut-chip) 하위 라인업의 그래픽카드에 쓰는 식으로 대응하고 있다.</p>

<p>하지만 D램은 8기가비트 칩의 일부를 죽여 7기가바이트나 6기가바이트로 판매할 수 없다. 무조건 8기가비트를 맞춰야 한다.
그래서 제조과정에 추가 cell들을 집어넣고, 웨이퍼 제조가 끝나면 레이저 퓨즈를 이용해 특성이 나쁜 cell들을 다시 mapping해주는 과정(레이저 수리)을 진행한다.</p>

<p>하지만 제조 노하우가 부족한 중국이 이걸 능숙하게 하는 것도 어렵고, 모든 결함이 레이저 수리로 고쳐질 수 있는 것도 아니다.</p>

<p>중국은 기술이 딸려서 칩 크기를 줄이지도 못하고 있다. 그래서 수율 밀도 모두 상대가 안된다.</p>

<p>낸드는 기술이 D램보다 간단하고, 중국과의 기술 격차가 3~4년인 것으로 파악되고 있다.
D램처럼 칩당 용량대결을 위해 대형화할 필요 없이, 작은 사이즈로 생산해 여러개 겹치는 것이 가능하다.</p>

<p>SSD는 D램과 달리 각 칩이 CPU와 직결될 필요가 없으며,
CPU가 직접 나노초 단위 고속 접근을 요청하지도 않기 때문이다.</p>

<p>게다가, 컨트롤러의 도움을 받아 낸드의 불량한 특성을 어느정도 상쇄할 수도 있다.
낸드 양산 특성이 불량하더라도 소프트웨어 알고리즘으로 어느정도 상쇄할 수 있고, 대만 등에서 컨트롤러를 구하는 것은 어렵지 않다.
그래서 중국은 낸드 분야에서 더 빠른 발전을 보이고 있다.</p>

<p>하지만 시장을 차지하기에는 여전히 어려움이 있다.
수율이 여전히 낮고, 중국 기술력이 발표에 미치지 못한다는 간접적인 증거가 상당히 많다.</p>

<p>2018년 6월, 중국의 YTC는 초고석 낸드용 인터페이스인 Xtacking 기술을 발표했다.
YMTC는 이 기술로 전체 낸드 면적의 20~30%를 차지하는 peripheral(주변회로)들을 cell 아래로 감출 수 있고,
기존 낸드들의 400~800MBps를 4배로 넘는 3GBps를 달성할 수 있다고 말했다.
대단해 보이지만, 사실 알고보면 좀 이상한 기술이다.</p>

<p>일단, 2개의 웨이퍼가 필요한 기술이다. 한쪽 웨이퍼에는 기존 낸드 칩, 한쪽 웨이퍼에는 CMOS를 만들어 둘을 결합한 방식이다.
아니 근데, 기존 메모리 시장은 같은 양의 웨이퍼에 더 많은 cell을 올리려고 노력하고 있었다.
당연히, 웨이퍼 하나 더 가져와서 별도 칩을 만들면 성능 개선이 가능한건 다른 낸드 제조사들도 다 알고 있었다.</p>

<p>도시바, 하이닉스는 1개 웨이퍼만으로 이 문제를 해결했다.
웨이퍼 맨 아래쪽에 CMOS를 형성한 후, 그 위에 3D 낸드를 깎아내는 방식을 썼다.
이 기술을 COP(Cell Over Peripheral)이라 부르며, 하이닉스는 이 기술에 4D낸드라는 이름을 붙였다.
신기하게도, 두 기술 모두 Flash Memory Summit에서 동일한 날짜에 공개되었다.
이 회사들은 YMTC와 달리, 웨이퍼 추가 없이도 성능을 올린 것이다.</p>

<p>이상한 점 또 하나는, IO 성능은 낸드 전체 성능에 생각보다 영향이 적다는 점이다.
낸드의 동작은 두 단계로 이루어져 있다.</p>

<p>쓰기: IO를 통해 data를 받아들인 후, Cell에 강한 전압을 가해 기록한다.
읽기: Cell에 적당한 전압을 가해 읽고, IO를 통해 데이터를 내보낸다.</p>

<p>근데, 여기서 IO를 통해 data가 왔다갔다 하는 시간보다는 Cell에서 data를 읽고 쓰는 시간이 훨씬 오래 걸린다.
Cell &lt;-&gt; Page buffer &lt;-&gt; 외부 이런 구조다.</p>

<p>그래서 IO속도만 빨라봐야 의미가 없다.
게다가, 중국산 낸드 성능은 다른 글로벌 회사들보다 떨어질테니, IO 속도는 더욱 의미가 없어진다.</p>

<p>그리고 요즘은 낸드가 단품으로 팔리는 경우가 줄어들고 컨트롤러와 합쳐진 형태로 많이 팔리는데, 이건 중국 업체들에게는 장점이 아닌 리스크다.</p>

<p>현재 SSD, eMMC 등 낸드 기반 스토리지 시장은 서서히 낸드 팹을 가진 회사들을 중심으로 재편되고 있다.
2015 Q4 ~ 2017 Q4동안 전체 SSD 용량 출하량은 81% 늘어났는데,
낸드를 제조하지 않는 업체의 SSD 출하량은 25% 가까이 감소했다.</p>

<p>이건 낸드가 복잡해지자 특성이 제조사별로 크게 차이나기 시작해서 그렇다.
아무리 코드, 컨트롤러를 뛰어나게 설계하고 준비해놓았다고 해도,
사오는 낸드의 특성이 크게 변해버리면 또 새로 만들어야 한다.</p>

<p>근데 IDM 업체들은 자사 낸드의 특성을 미리 알고 준비해둘 수 있어서, 제품 출시도 빠르고 성능도 좋다.
결국 SSD가 아닌 일반 낸드를 사서 자기들 컨트롤러 붙여 파는, 작은 컨트롤러 전문 업체들이 줄어들게 된다.
이러면 낸드 찍어내는 회사 입장에서는, 자기들이 개발한 낸드 특성이 안좋을때 낸드 떨이로 팔아버릴 수 있는 잠재적 고객들이 줄어든게 된다.</p>

<p>IDM 업체들은 타사의 raw NAND를 사서 SSD를 만들지는 않는다.
예외적으로, 인텔이 하이닉스의 TLC 낸드를 사서 소비자형 SSD를 만든 사례가 있는데,
이건 인텔 공장 라인업이 대부분 데이터센터에 치중되어있어, 단가 따져보니 소비자형 SSD 만들거면 낸드 사와서 파는게 낫다고 판단했던 예외적인 경우다.</p>

<p>일반적으로, 낸드 제조사가 아닌 SSD 제조사들은 컨트롤러를 팹리스에서 구매한 뒤,
낸드 완제품을 사와서 PCB에 조립해 판매한다.</p>

<p>근데 팹리스가 파는 컨트롤러들은 일반적으로 최대 연결 가능한 낸드 갯수에 제한이 있다.
컨트롤러에 낸드와의 연결 통로를 만드는 일이 어려운 일이라, 컨트롤러 크기가 커지는 원인이 되기 때문이다.</p>

<p>근데 SSD는 일반적으로 1개 컨트롤러에 여러개 낸드가 붙어 만들어지기 때문에,
SSD 파는 입장에서는 컨트롤러 하나당 더 많은 낸드가 연결될수록 원가 경쟁에서 유리하다.</p>

<p>어쨌든 낸드 회사의 영향력이 강해짐에 따라, 컨트롤러 회사들은 특정 회사 낸드 기준으로 컨트롤러를 설계하고 성능을 측정하게 되었다.
새로운 낸드를 썼다가는 성능 보장도 힘들고, 생각지 못한 펌웨어/컨트롤러 문제가 생길 수 있다.</p>

<p>반도체 기술 뿐 아니라, 웨이퍼 양도 문제가 된다.
중국은 기술이 딸려서, 한국 업체들보다 훨씬 많은 웨이퍼를 투입해야 같은 비트를 생산할 수 있다. cell 밀도를 못따라가니까.
여기에, 중국 업체들 칩 크기가 더 크니까 수율까지 낮을거다. 수율을 커버치려면 그만큼 더 많은 웨이퍼를 써야 한다.</p>

<p>그래서 기술이 딸리면 웨이퍼를 엄청 쓰게 되는데, 이때 웨이퍼에 비례해 소비가 늘어난다.
자재값, 전기, 소모품(식각용 물질, 포토레지스트 등)이 소모되기에, 웨이퍼 값만 나가는게 아니라 온갖 비용이 다 나간다.</p>

<p>그래서 기술이 정말 중요한 업계고, 기술이 있다고 해도 돈이 엄청나게 드는 사업이다.</p>

<p>근데 중국이 여기 들어올 수 있겠나? 아까 말한대로, 동일 물량을 생산해도 웨이퍼가 몇배로 필요하고, 거기에 비례해 온갖 소모비용이 따라오고, 심지어 시장에 새로 들어오는거니까 판관비도 기존 기업들보다 더 써야 한다.
DRAM이든 낸드든 해당되는 얘기다.</p>

<p>중국이 그나마 기대해볼만한 부분은 대용량화+공정미세화가 멈추는거다.
미세화가 멈추면, 현재 압도적 진입장벽인 비트당 원가 차이가 좁혀질 수 있다. 미세화가 멈춰서 원가 더 못줄이니까.
대용량화가 멈추면 칩을 작게 만들어도 될거다. 그럼 웨이퍼당 수율 압박에서 벗어날 수 있다.
실제로, DRAM의 연간 원가 감소율은 옛날에 비해 상당히 감소했다.</p>

<p>근데 그렇다 해도 기존 선두 사업자들은 만들기 어려운 HBM등을 개발하며 동일 공정하에서도 기술 격차를 유지할거다.</p>

<p>일반적으로, 모바일 시장은 물리적 공간의 제약이 커서 DRAM을 4개까지만 겹쳐서 사용할 수 있다.
그 이상 겹치려고 하면 전력 및 발열 제어, 칩 높이 조정, 와이어 연결 등의 문제가 발생하게 된다.
지금도 하이엔드 스마트폰은 공간이 부족해, 발열 덩어리들은 AP, DRAM, UFS를 한개 패키지 안에 쌓아놓은 방식(ePOP 기술)을 쓴다.</p>

<p>AP에 DRAM을 추가로 연결하려면 추가 Pin이 필요한데, 이러면 면적이 더 필요하다. 그래서 한번 달때 용량 큰 DRAM을 달아줘야 한다.
저가형이라고 용량 작은 DRAM을 달아봐도, 메모리가 먹는 전력량은 하이엔드랑 다를게 없다. 그니까 진짜 DRAM은 용량 큰거 달아줘야 한다.
*핸드폰들 스펙 확인해보기</p>

<p>그래서 중국 회사들이 용량 작은 DRAM을 생산해도 저가형 핸드폰에조차 못들어간다.
그렇기에, 중국이 DRAM을 내수화하면 중국은 구데기 핸드폰밖에 못만든다.</p>

<p>PC는? PC도 메인보드 보면 D램 장착 가능 갯수에 제한이 있다.</p>

<p>뭐 문서작업, 웹서핑용 컴퓨터라면 DRAM 용량이 그렇게 중요하지는 않다. 문제생겨서 껐다 켜도 그렇게 큰 문제는 아니니까.
근데 서버같은 경우에는 DRAM 용량이 극단적으로 중요하다. 그래서 서버컴퓨터에는 메모리 꽂을 자리가 항상 부족하다.
그래서 여기서도, 중국의 용량 작은 DRAM은 쓸모가 없다.</p>

<p>그럼 애초에 큰 용량이 필요하지 않은 가전 등에 들어가는 DRAM은?
일단 가전은 시장 규모가 훨씬 작고, 메모리 성능이 중요하지도 않기 때문에 비트당 가격이 낮다.
그리고, 이런 시장에 머무르면 발전을 못한다.</p>

<p>삼성전자, 하이닉스 등 메모리 업체들은 매해 단순히 미세공정 개선으로 cell 크기만 줄이고 있는게 아니다.
반도체 개발의 시작은 시장조사다. 작년에 특정 회사와 비즈니스가 잘 안됐거나 높은 가격을 받지 못했다면 그 이유를 생각해보는거다.
전력 소모가 컸기 때문이라면 차기 메모리를 설계할대는 전력을 아끼는 형태의 cell을 만들어야 하고,
그 설계에 맞는 노공기와 에칭 장비들을 정해야 한다.</p>

<p>성능 부족이 원인이었다면, 성능에 관계되는 센스앰프 등의 하드웨어를 강화하는 방향으로 설계를 수정하고 이에 맞는 새로운 장비들을 도입해야 한다.
그리고 이런 장비들이 100%에 가까운 가동률로 굴러가도록 프로세스를 구성해야 한다.
이런 식으로 개선하는 일들은 노하우가 많은 업체들도 하기 힘들어하는 일인데, 신생업체가 하기는 더 어렵다.</p>

<p>중국기업은 JHICC, YMTC 정도가 있다.</p>

<p>여기까지는 메모리반도체 관련이었는데, 설계 파운드리 분야는 중국이 들어올 수 있을까?</p>

<p>설계 분야에서, 모바일 분야에서는 중국이 이미 상당한 수준에 올라와 있다.
대표적인 회사는 화웨이의 자회사인 HiSilicon으로, 이미 Kirin이라는 자체 AP도 갖고 있다.
다만, 얘네는 삼성전자 퀄컴과 다르게 자체 ARM 코어 설계를 만들지는 않고, ARM이 제공하는 코어 설계를 쓴다.
그래서 칩 판매당 순이익이 더 적을 것으로 추정된다.</p>

<p>HiSilicon은 하이엔드부터 로우엔드, 최근에는 서버까지 커버할 정도의 라인업을 갖고 있다.
Kirin이라는 AP, Kunpeng이라는 서버 CPU, Ascend라는 AI 가속기까지 있다.
이렇게 정말 다양한 칩들을 설계하고 있다.</p>

<p>설계가 어려운 일이긴 하지만, 그렇다고 공장처럼 대규모 투자가 필요한 일은 아니니까.</p>

<p>유니그룹도 중국의 설계 회사다. 유니그룹도 ARM에게서 코어 디자인을 라이센싱해와서 AP를 설계해 판매한다.
얘네는 HiSilicon처럼 모회사에 스마트폰 사업부가 있는 것은 아니고, 그냥 외부에 판매한다.</p>

<p>유니그룹은 중저가형 폰에 들어가는 AP를 만드는 회사다. 검증된 구 공정을 써서 만든다. 그래서 엄청 싸다.
삼성전자도 초저가형 라인업에 이 회사의 칩을 사용한 사례가 있다.</p>

<p>하이실리콘과 유니그룹 정도가 세계 top10에 들어가는 중국의 팹리스 회사들이다.
중국 팹리스 업체들은 성장하고 있긴 하지만, 대부분 물량이 중국 스마트폰 업체들에게 팔리고 있다.
다른 곳에는 잘 안팔리는거다.</p>

<p>그리고, 두 회사 모두 AP 설계시 ARM에서 설계한 코어를 쓴다. 아직 중국 팹리스에서는 고성능 로직을 자체설계하지 못하고 있다는거다.
최근 화웨이가 발표한 ARM기반 64코어 프로세서인 Kunpeng 920도 ARM에서 Ares라는 디자인을 사와서 설계한거다.</p>

<p>한국에서는 삼성전자 LSI사업부가 나머지 국내 팹리스 업체들 합친것보다 큰 매출을 내고 있다.</p>

<p>세계 팹리스 매출액 1등은 브로드컴이다.</p>

<p>파운드리는 중국이 따라잡기가 더 어렵다.
2018년 8월, 세계 2위 파운드리였던 글로벌파운드리는 차기 공정이었던 7nm의 양산을 포기한다고 발표했다.
대만 UMC도 7nm에 안끼어들고 14나노(성숙한 공정)에 집중하겠다고 했다. 얘네는 왜 7nm 경쟁에 안들어왔을까?</p>

<p>파운드리 투자액도, 메모리 시장처럼 기하급수적으로 늘어났고, 고객과의 관계도 중요하다.
EUV장비 대당 1500억정도 하는데, 이거 수십개 들여놓고 고객들 주문 받아서 기기 가동률 확보해 계속 돌리지 않으면 바로 망한다.</p>

<p>고객사 입장에서는 괜히 파운드리 바꿨다가 문제가 생길 수 있으니 안바꾼다.
파운드리를 바꾸면 없던 문제가 생길 수도 있고, 발열/전력소모량이 늘어날 수도 있다.
실제로, 아이폰이 TSMC와 삼성전자 양쪽에 맡겼을때 칩이 조금 다르게 나왔다고 한다.</p>

<p>효율적이었던 특정 트랜지스터가 파운드리 바꾸면 효율이 나빠지거나, 셀 설계 차이로 인해 회로의 지연 시간 등이 달라질 수 있다.
웨이퍼 실물을 받아 확인하는 데에만 3달정도 걸리니까, 경쟁을 해야 하는 기업들에게는 큰 부담이다.</p>

<p>그리고 고객들 역시 파운드리 굴리는게 어려운 일이라는걸 알고 있기에,
웬만하면 성공 가능성이 높은 파운드리와 일하려고 한다. 그래서 대형 파운드리로 몰린다.</p>

<p>그래서 SMIC는 1등이 될 수 있을 것인가?
TSMC 삼성전자로부터 고객을 뺏어와야 할텐데, 신뢰성 / 기존설계 재사용성 때문에 기존 고객들은 거의 안움직일거다.</p>

<p>글로벌파운드리 사업 재조정 선언 후, AMD CPU는 TSMC에게 갔고, IBM 메인프레임 프로세서는 삼성전자에게 갔다.</p>

<p>반도체산업 만드는게 돈만으로 되는게 아니다. 돈 많은 중동에서 해낸건 글로벌파운드리 인수뿐이다.</p>

<p>일본 회사들, HDD 회사들은 시장의 혁신가들이 원하는 것을 제공하지 못했기 때문에 시장을 잃어버렸고,
인텔은 혁신가들이 원하는걸 적절한 시간에 내놓았다. 하위호환성, 소프트웨어 개발 표준 등.</p>

<p>인텔 x86은 폐쇄적인 설계였다. 인텔 말고는 아무도 x86설계 만드는 법을 몰라서,
아무도 저전력, 소형 x86을 못만들었다. 인텔도 그런거 필요 없다고 생각해서 안만들다가 ARM이 시장을 먹어버렸다.</p>

<p>마이크로소프트의 사티아 나델라는 Azure를 통해 마이크로소프트를 살려냈다.</p>

<p>인텔이 2015년 발표한 3D Xpoint: 비트당 가격이 DRAM보다 훨씬 낮아질것이며, DIMM당 용량은 DRAM보다 압도적이다.
대신 반응속도가 좀 나쁘긴 하지만, 그래도 DRAM 용량이 중요한 곳이 있을 것이다.</p>

<p>넓은 영역 메모리에 가끔 접근해야 하는 대형 DB라면, DRAM + SSD보다 TCO가 뛰어날 수도 있다.</p>

<p>SADP, SAQP: 최근 미세공정에 사용하는 기술. Spacer Assisted Double(Quadruple) Patterning</p>

<p>공정의 전반적인 프로세스: 보호막을 전체에 씌운다, 모양 바꾸고 싶은 곳의 보호막을 뚫는다, 다른 물질을 뿌린다.
이게 끝나면 BEOL 과정으로 넘어가, 수많은 금속 배선들을 소자와 연결한다.</p>

<p>BEOL이 끝나면 웨이퍼를 테스트 장비로 보내서, 잘 만들어졌나 확인한다.
테스트용 Probe, 고온/저온 chamber등이 필요하다.</p>

<p>패턴이 아주 미세한 FEOL 단계에서 사용되는 노광기는, 패턴이 매우 큰 BEOL 마지막단계에서 사용되는 노광기와는 다를 수 있다.</p>

<p>식각도 맨 아래 소자층에 쓰이는 장비와 금속 상단부에 쓰이는 장비는 다르다.
하부는 정확도 &gt; 처리량이지만, 상부는 처리량 &gt; 정확도로 비용을 낮춘다.</p>

<p>장비중에서는 노광기가 제일 비싸다. 실제로 웨이퍼 위에 뭔가 그리는건 노광기 뿐이고,
나머지 기기들은 웨이퍼를 어딘가에 통째로 넣고 담그거나 산화시키거나 한다.</p>

<p>EDA tool로 기능 중심의 설계를 할 수 있다.
팹리스에서는 이걸 Front-end design이라 부르고,
Synopsis, Cadence같은 회사들이 tool을 제공한다.
여기서 쓰이는 언어로는 Verilog가 있다.</p>

<p>당연히, Verilog code에는 이 칩이 삼성전자 10nm인지 TSMC 5nm인지 안적혀있다.</p>

<p>팹리스는 파운드리에서 PDK를 받아와서, 자기 코드와 PDK로 GDS(Graphic Design System) 파일을 만들어낸다.
GDS 파일은 포토마스크를 만들때 사용되고, 파운드리를 통해 칩으로 만들어진다. 이걸 팹리스에서 back-end design이라 부른다.</p>

<p>결국 칩의 기능 만드는 영역과 칩을 실제로 만드는 영역이 있는거다.</p>

<p>팹리스는 외부 IP를 라이센싱 받아오기도 한다.
이때 코드 형태로 받아오면 soft macro, 물리적 실체로 받아오면 hard macro다.</p>

<p>soft macro는 특성을 예상하기 힘든 대신, 자사 칩과 완전히 한 덩어리로 만들어 넣을 수 있다.</p>

<p>hard macro는 칩 내에서 자유로운 모습으로 변경은 불가능하지만, hard macro 부분의 성능, 전력 사용량, 면적 등은 쉽게 추측할 수 있다.</p>

<p>이렇게 칩을 만들었으면, 패키징 과정에서 다리를 붙이든가 BGA로 만들든가 해야 기판에 납땜되거나 달라붙어 동작을 할 수 있다.</p>

<p>물론, 팹리스는 이 작업을 해주는 업체를 찾아 맡겨야 한다.</p>

<p>파운드리 입장에서는 EDA tool에서 잘 돌아가도록 PDK를 잘 만들어야 한다. 이건 쉬운 일은 아니다.</p>

<p>Silvaco라는 회사는 파운드리한테 공정정보를 받아 PDK를 만들어주는 일을 한다.</p>

<p>Renesas(르네사스) 여기 뭐하는 곳이더라?</p>

<p>7nm LPP(Low Power Plus): 삼성전자 공정, TSMC N7: TSMC 공정</p>

<p>파운드리는 남들 다 쓰는 회사로 모이는 경향이 있다. 협력사의 IP, EDA tool 등 인프라가 많으니까.</p>

<p>미세공정 난이도가 올라가면서, 수많은 팹리스들이 다른 종류의 칩을 제조한 후 후공정에서 결합하는 방식을 쓰고 있다.
그래서 파운드리도 칩 제조 뿐 아니라 칩간 결합과 패키징까지 하는 회사가 되어가고 있다.</p>

<p>인텔도 Foveros, EMIB같이 새로운 결합기술을 개발하기 시작했고,
TSMC도 플립칩같은 공간 절약 기술, CoWoS(Chip on Wafer on Substrate, 반도체 칩을 기판 대신 Si 웨이퍼 위에서 연결)등의 고성능 연결 기술을 개발했다.</p>

<p>이런 연결기술은 중요하다. 2020년 발매된 NVIDIA A100은 GPU에 HBM을 6개나 결합해야 했다.
하지만 당시 삼성전자는 최대 4개 HBM만 결합할 수 있었다. 엔비디아는 TSMC에게 갈 수밖에 없었다.</p>

<p>결합 기술은 생각보다 어렵고 복잡하며, 칩의 전력 사용량 등에 큰 영향을 준다.</p>

<p>파운드리는 생태계가 중요하다. 가져다 쓸 수 있는 IP가 많은게 좋다.
TSMC: IP Alliance Program, 삼성전자: SAFE, 인텔: x86 공개
등으로 생태계 구축 노력을 하고 있다.</p>

<p>기존 파운드리들의 자체 IP는 SRAM같은 단순한 구조뿐이었는데, 인텔 파운드리는 x86 IP를 제공한다. 이걸로 승부보려고 한다.</p>

	</div>
</article>
		</div>
	</div>
  </body>
</html>