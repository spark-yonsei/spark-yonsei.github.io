<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-02-16T00:58:09+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seil Park</title><subtitle></subtitle><entry><title type="html">Chip war 내용</title><link href="http://localhost:4000/history/2023/10/04/History-Chip-war-%EB%82%B4%EC%9A%A9.html" rel="alternate" type="text/html" title="Chip war 내용" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/history/2023/10/04/History%20-%20Chip%20war%20%EB%82%B4%EC%9A%A9</id><content type="html" xml:base="http://localhost:4000/history/2023/10/04/History-Chip-war-%EB%82%B4%EC%9A%A9.html"><![CDATA[<p>Chip war 내용 정리:<br />
2차대전까지는 강철과 알루미늄으로, 냉전때는 핵 개수로 승부했다면 현대 사회에서는 국가들이 computing power, 즉 연산력으로 승부한다.<br />
아시아 국가들은 지난 50년간 반도체 흐름을 타고, 실리콘을 발판으로 부상했다.<br />
아이폰에 들어가는 배터리, 블루투스, 와이파이, 통신, 오디오, 카메라 관리는 모두 다른 칩들이 한다. 메모리 칩은 일본의 키오시아, 무선 주파수 인식(Radiofrequency identification)칩은 캘리포니아의 skyworks, 오디오칩은 오스틴의 Cirrus Logic이 만든다.<br />
아이폰의 운영체제가 돌아가는 프로세서는 애플이 설계하긴 하지만, TSMC에서 만든다. 아이폰 12S에는 개당 118억개정도 되는 트랜지스터들이 들어간다.<br />
<br />
ARM은 일본 소프트뱅크에 인수된 영국회사인데, 연구소가 캘리포니아랑 이스라엘에 있다.<br />
<br />
칩 테스트는 동남아시아에서 많이 한다.<br />
<br />
OPEC도 세계 석유 40%를 갖고 있을 뿐이다. 경제 분야 중 이렇게 소수 기업들이 이것저것 독점하고 있는 분야는 반도체뿐이다.<br />
<br />
에니악에는 진공관 18,000개가 들어갔다. 즉 스위치 18,000개가 들어갔다. 진공관은 작동할 때 전구처럼 빛나기 때문에, 당시 벌레가 많이 꼬였다. 그래서 주기적으로 ‘debugging’을 해줘야 했다.<br />
<br />
벨 연구소는 뉴저지에 있었다. AT&amp;T는 전화 회사였기 때문에, 트랜지스터를 스위치보다는 증폭기로 생각했다.<br />
<br />
TI는 2차대전때 잠수함 탐지용 초음파 장비를 만들어 팔았고, 냉전때는 미국 정부에 IC 많이 팔아 부자가 되었다.<br />
<br />
트랜지스터 관련해서 노벨상 받은 사람들:<br />
John Bardeen은 2번, William Shockley도 1번<br />
<br />
Minuteman: 미국-우주-모스크바 상정한 궤도 진입 미사일<br />
<br />
TI에서 처음 리소그래피로 회로 그릴때는 코닥의 감광액을 썼는데, 그냥 쓰기에는 감광액의 순도가 부족해서 원심분리기로 감광액을 재처리해 썼다.<br />
<br />
Morris Chang(모리스 창)은 TSMC 창업자인데, 중국 본토 출신이고 2차대전까지 중국 살다가 국공내전때 탈출했다. 홍콩을 거쳐 미국으로 갔고, 처음에는 하버드 영문학과 다니다가 MIT 기계공학과로 옮겼다.<br />
<br />
졸업 후에는 Sylvania라는 회사에 갔다가 TI로 갔고, TI의 수율을 엄청나게 끌어올렸다.<br />
<br />
쇼클리는 노벨상까지 받은 사람이었지만, 사업은 결국 잘 안됐다. 진짜 산업을 이끈건 시간 갈아넣으면서 수율 개선한 사람들이었다.<br />
<br />
2022년 기준으로,<br />
한국은 모든 메모리 칩의 44%, 모든 프로세서 칩의 8%를 생산한다.<br />
일본은 모든 종류의 칩의 17%를 생산한다.<br />
대만은 모든 프로세서 칩의 41%, 최첨단 칩의 90%를 생산한다.<br />
싱가포르는 모든 종류 칩의 5%를, 중국은 15%를 생산한다.<br />
<br />
소련은 미국 반도체를 베끼려고 했는데, 이게 핵무기 베끼는것까지는 됐는데 반도체 베끼는 데에는 실패했다. 소련의 장점은 품질이 아니라 물량이었는데, 반도체는 품질이 너무나 중요하다.<br />
<br />
소련에 훌륭한 과학자들을 많았지만, 공정에는 문서로 정리되지 않은 부분들도 많이 필요했고, 그 모든걸 스파이가 훔쳐다줄수는 없었다. 그래서 공정이 불가능했고, 어찌저찌 하더라도 수율이 안좋았다.<br />
<br />
1962년 11월, 일본의 이케다 하야토 총리가 프랑스 가서 샤를드골을 만나고, 소니 트랜지스터 라디오를 선물했다.<br />
<br />
1960년대 말, 대만 시급은 19센트, 말레이시아 시급은 15센트, 싱가포르는 11센트, 한국은 10센트였다. 미국은 인건비도 비싼데다가 노조까지 있었다. 그래서 페어차일드 반도체는 반도체 제품 조립을 홍콩에서 했다. 그 뒤 싱가포르도 추가됐다.<br />
<br />
베트남전쟁 이후, 동아시아 반공정권들은 베트남같이 되지 않기 위해, 미국이 자기 나라에 더욱 관심가지게 하려고 노력한다. 그 노력중에는 반도체 공장을 짓는 것도 있었다.<br />
<br />
Fairchild Semiconductor(페어차일드 반도체)는 Gordon Moore가 처음 세웠던 회사다. 이 사람이 두번째로 세운 회사가 인텔이다.<br />
<br />
Intel은 Integrated Electronics의 줄임말이다.<br />
인텔의 첫 제품은 DRAM이었다. 메모리칩은 기기에 맞게 특화될 필요가 없기 때문에 대량생산이 용이하다. 다른 칩들은 기억이 아니라 계산을 해야 해서, 모두 다른 구조로 특별히 설계되어야 했다.<br />
<br />
이때 인텔 생각: 표준화된 로직 칩과 메모리칩이 있으면, 필요한 기능에 해당하는 소프트웨어만 설치하면 될거다. 이 생각이 나중에 cpu로 이어진다.<br />
<br />
HP는 Hewlett-Packard의 약자고, 창업자 2명의 이름이다. 둘 다 스탠포드 출신이다.<br />
<br />
1980년대는 미국 반도체 업계에게 힘든 시기였다. 일본이랑 경쟁해야 했다.<br />
<br />
1980년대에는 인텔, TI뿐 아니라 도시마, NEC같은 일본 회사들도 DRAM을 만들게 됐다. 옛날 일본은 ‘우리가 미국보다 혁신은 느려도 실현은 더 빠르다’라고 하며 미국을 쫓아갔는데, 가전제품 시장을 일본이 먹어치우기 시작하고 1979년에는 워크맨까지 나오며 혁신으로도 미국을 앞질러버렸다.<br />
<br />
미군정이 일본을 키운거긴 하다. 망하게 했다가 공산화되는 것보다는 부하로 잘 키워놓는게 낫다고 판단해서 미군정 이후로도 미국은 일본을 계속 도와줬다.<br />
<br />
일본도 미국에 산업스파이 엄청 보냈다. 일본 정부는 1974년까지 미국 기업이 일본에 팔 수 있는 반도체 개수에 쿼터제를 도입하고 있었다. 그래서 일본 기업들이 미국 시장 먹는동안 미국 기업들은 일본 시장을 먹을 수가 없었다. 일본 정부가 자국 회사들에게 돈을 주기도 했다.<br />
<br />
금리 차이도 컸다. 1980년대 미국은 베트남전쟁때 달러 찍어낸 여파로 생긴 인플레이션에 고통받고 있었다. 기준금리가 높을때는 21.5%까지도 올라갔다고 한다.<br />
<br />
하지만 일본은 사람들이 하도 저축을 많이 해서 은행에 현금이 많았고, 그래서 은행들이 기업들에게 저금리 대출을 많이 해줄 수 있었다. 당시 일본 금리는 6~7%정도였다.<br />
<br />
인텔이 DRAM을 개발한 지 10년 뒤, 인텔의 DRAM 점유율은 1.7%밖에 안되게 되었다. 일본이 DRAM 시장을 다 먹어버렸다! 80년대 내내, 미국 시장에서 일본의 반도체 점유율은 계속 상승했다.<br />
<br />
반도체 공정 쪽도 미국이 그리 앞서나가지 못했다. Photolithography는 현미경 뒤집어서 작은 그림 그리는 데에서 시작했던 만큼 렌즈가 필요한데, 당시 렌즈는 독일의 Carl Zeiss와 일본의 Nikon이 주도했다.<br />
<br />
Stepper라는 장비가 있는데, 웨이퍼 전체에 빛을 쏜느게 아니라 die마다 빛을 쏘게 해주는 장비다. 원래 1978년에 미국 GCA가 만들었지만, 80년대 중반부터는 Nikon에게 시장을 뺏겼다.<br />
<br />
이때쯤만 해도 반도체 공정이 정말 정밀해져서, 천둥이 치면 기압이 변해서 공기 굴절률이 변해 빛이 꺾여 그림이 이상하게 그려질 수 있는 수준이었다.<br />
<br />
1973년, 1979년에 각각 오일쇼크가 있었다. 원인은 OPEC이 감행한 석유수출 통제였다. 당시 아랍 국가들이 이스라엘과 친한 미국을 조지려고 석유 수출을 차단한 것이다. 그 결과 스태그플레이션 등 많은 문제들이 생겼다.<br />
<br />
실리콘밸리 사람들은 미국 정부에 가서, 반도체 일본이 다 먹어치우게 내버려두면 반도체도 석유 꼴 난다고 말하며 좀 도와달라고 했다.<br />
<br />
1986년, 미국 정부가 일본 정부에 항의해 일본은 DRAM 수출량을 제한했다. 하지만 미국 반도체 회사들은 이미 거의 다 뒤진 후였다.<br />
<br />
인텔 창립자 중 하나인 밥 노이스가 미국 정부랑 같이 미국 반도체 회사들좀 살려보려고 노력했다. 특히 당시 니콘, 캐논, ASML을 경쟁사로 갖고 있던 GCA를 어떻게든 살려보려고 많이 노력했지만, 밥 노이스는 1990년에 죽고 GCA도 1993년에 망했다. 미국 내 리소그래피 업계는 이렇게 다 죽어버렸다.<br />
<br />
마이크론은 1978년에 설립됐는데, 이 시기는 미국 DRAM 시장이 일본한테 탈탈 털리던 시기였다. 이때, 마이크론은 3가지 노력을 했다.<br />
<br />
1. DRAM 성능이 좀 별로더라도 가격이 싸도록 설계해서 비용 절감,<br />
2. DRAM 성능이 좀 별로더라도 공정 단순화해서 비용 절감,<br />
3. 일본 반도체 칩에 관세 물려달라고 정부한테 찡찡거리기<br />
<br />
인텔은 DRAM 사업이 박살나긴 했지만, 마이크로프로세서 사업은 아직 살아있었다. 이때쯤 IBM에서 개인용 컴퓨터를 출시했는데, 인텔이 거기 들어갈 칩을 만들었다. 그리고, 인텔은 결국 DRAM을 포기하고 마이크로프로세서에 집중하기로 했다.<br />
<br />
인텔은 직원들 일본 보내서 공정 다 배워오게 하고, 회사 분위기를 기계적으로 바꿨다. 이제 연구소보다는 공장이 됐고, 수율도 오르고 비용도 절감됐다.<br />
<br />
그리고 1985년 플라자 합의로 엔화 가치가 2배가 되어 미국도 수출 경쟁력이 생겼고, 이때쯤 미국 금리도 내려오기 시작했다.<br />
<br />
이렇게 인텔이 PC용 칩, 마이크로소프트가 운영체제 독점한 상태로 컴퓨터 시대가 열렸다.<br />
<br />
삼성이 반도체 사업을 시작하기 전부터, 한국은 미국, 일본 칩의 조립과 패키징을 하는 주요 장소였다. 삼성전자가 성공한건 한국 정부와 미국 기업들의 지원 덕분이었다. 미국 기업들 입장에서는 DRAM 만들어봐야 일본한테 털리니까 삼성전자에 관련기술 던져주고 돈이라도 받는게 나았다. 일본 말고 다른 DRAM 공급원 있으면 좋기도 하고. 이게 1983년이다.<br />
<br />
UCSD의 Andrew Viterbi가 만든 알고리즘 CDMA는 이론상 훌륭하긴 했는데, 연산량이 아주 많았다. 그러다 반도체가 계속 발전중이니 이제 가능해질거다! 라고 생각한 Andrew Viterbi와 Irwin Jacobs가 퀄컴을 창업했다. 이렇게 CDMA를 통해, 주어진 BW 안에 더 많은 신호를 넣을 수 있게 됐다.<br />
<br />
소련은 미국 반도체, 장비들을 계속 훔쳐갔지만 결국 따라잡지는 못했다.<br />
소련의 문제는:<br />
1. 정치적 간섭이 심했고,<br />
2. 시장이 군수시장 뿐이었고,<br />
3. 국제 공급망이 없어서 미국-일본-동아시아 같은 협력이 불가능했다.<br />
<br />
반도체로 강화된 무기는 걸프전에서 볼 수 있었다. 당시 뉴욕타임즈 헤드라인: 강철을 이긴 실리콘<br />
<br />
페이브웨이 폭탄: TI가 만든 유도탄<br />
<br />
일본 기업들은 정부 지원을 계속 받아, 그냥 만들던거 계속 찍어냈다.<br />
<br />
삼성전자, 마이크론 등이 DRAM 시장에 들어와도 그들은 그냥 무지성으로 생산량 더 늘려서 더 많이 찍어냈다.<br />
<br />
소니는 다른 일본 회사들이랑 다르게, DRAM 말고 이미지센서에 투자했다. 그리고 아직까지 그걸로 먹고 산다.<br />
<br />
도시바에서는 마스오카 후지오가 플래시메모리를 만들었는데, 도시바는 발견을 무시했고 결국 그걸 제품으로 개발해 시장에 내놓은 것은 인텔이었다.<br />
<br />
일본 기업들은 DRAM으로 돈을 많이 벌어서 마이크로프로세서 시장을 무시했지만, 컴퓨터 시대가 오자 DRAM보다 마이크로프로세서가 훨씬 중요해졌다.<br />
<br />
1990년에는 고르바초프가 실리콘밸리를 방문하고, 스탠포드에서 강연도 했다. 이렇게 냉전 패배 인정했다.<br />
<br />
1993년부터 미국은 반도체를 다시 수출하기 시작헀고, 1998년에는 삼성전자가 DRAM 최대생산자 자리를 먹었다. 1980년대 말에 90%였던 일본의 시장 점유율은 1998년에는 20%가 됐다.<br />
<br />
대만 입장에서는, 반도체 조립은 동아시아 국가들 모두 하니까 뭔가 더 돈 될만한 다른 산업을 찾아야 했다. 한국처럼 조립에서 생산으로 나아가지 못하면 중국한테 일 다 뺏길 상황이었다.<br />
<br />
대만은 모리스 창을 불러서, 니 하고싶은거 다 하게 해줄 테니까 대만에 반도체 산업 한번 만들어보라고 했다. 모리스 창이 하고 싶었던건 반도체 제조 특화 기업이었다.<br />
<br />
TSMC 설립에 투자한건 대만 정부, 대만 자본가들, 필립스 정도였다. 그마저도 대만 자본가들은 대만 정부에 의해 강제참여 당한거였다.<br />
<br />
실리콘밸리의 작은 팹리스 업체들은 항상 다른 회사 fab을 빌려야 했는데, 그러면서도 항상 기술 도둑맞지 않을까 불안했다. 그래서 그들에게 TSMC가 아주 좋아보였다.<br />
<br />
TSMC는 1987년에 세워졌는데, 화웨이도 런정페이가 1987년에 선전에 세웠다.<br />
<br />
80년대 중국 지도자는 장쩌민이었는데, 상하이교통대 전기공학과 출신이라 그런지 전자기술을 중요시했다.<br />
<br />
DRAM시장을 계속 점유하려면 공정 선폭(technology node)의 연속성을 확보해야 하는데, 여기에 많은 돈이 든다. 삼성전자는 정부의 지원으로 이걸 확보했다.<br />
<br />
Technology node: 해당 공정에서 구현 가능한 최소 선폭<br />
<br />
중국은 반도체 생산이 중요하다고 생각하긴 했는데, 반도체를 생산할 인프라는 없었다. 그러던 중 리처드 창이 2000년에 중국에 SMIC를 설립했고, TSMC를 따라하기 시작했다.<br />
<br />
이런 식으로, TSMC의 경쟁자들이 많아지기 시작했다. SMIC, 싱가포르의 차터드반도체, 대만의 UMC, 대만의 뱅가드반도체, 한국의 삼성전자 등이 경쟁자였다.<br />
<br />
*중국 주석: 마오쩌둥-화궈펑-덩샤오핑-장쩌민-후진타오-시진핑<br />
<br />
1990년대에는 리소그래피의 미래가 불투명한 시대였다. 당시에는 파장을 더 줄여서, DUV를 넘어 EUV를 써야 하는 시대가 왔다. 그런데, EUV 장비 개발은 돈이 정말 엄청나게 드는 일이었다.<br />
<br />
당시 리소그래피 회사는 캐논, 니콘, ASML이 있었다. ASML은 필립스의 리소그래피 부서가 떨어져 나와 생긴 곳이다.<br />
<br />
ASML은 네덜란드 회사기 때문에, 미국 회사들은 일본의 캐논, 니콘보다는 ASML과 거래하는걸 더 선호했다. 그리고 필립스가 TSMC 초기 투자자였기 때문에, ASML은 TSMC라는 확실한 고객을 갖고 성장할 수 있엇다.<br />
<br />
결국 캐논, 니콘은 EUV 장비 개발을 포기했고, ASML만 성공했다.<br />
<br />
CPU가 계산하는 방식을 설정하는 규칙 모음 = CPU architecture<br />
인텔이 만든 아키텍처가 x86이었고, x86이 개인 컴퓨터 CPU의 표준이 되었다.<br />
X86이 최고의 아키텍처라서 시장을 먹었다기보다는, IBM이 만든 최초의 개인용 컴퓨터에서 x86 아키텍처를 썼기 때문이다.<br />
<br />
X86 기반 CPU를 안쓰는 컴퓨터 업체로는 애플이 있었지만, 2006년부터는 애플도 x86 기반 CPU를 쓰기로 했다.<br />
<br />
UC 버클리에서는 RISC라는 아키텍처를 만들었는데, 이 아키텍처가 x86보다 더 효율적이고 가볍다. 그래서 인텔도 x86 버리고 RISC로 옮길까 하다가, 전환 비용이 너무 비싸다고 판단해 그만뒀다. 그래서 지금도 컴퓨터들은 대부분 x86 기반이다.<br />
<br />
인텔은 데이터센터 시장도 그들의 프로세서를 내세워 IBM, HP를 밀어내고 먹어버렸다. 현재 인텔 또는 AMD 프로세서 없이는 데이터센터가 있을 수 없다.<br />
<br />
인텔이 버린 RISC는 1990년 애플이 세운 벤처회사 ARM이 써보기로 했다. ARM은 직접 칩을 설계하지는 않고, 아키텍처 사용권 라이선스만 팹리스 업체들에게 판다.<br />
<br />
하지만 ARM은 인텔의 점유율을 뺏지는 못했다. 인텔 프로세서랑 마이크로소프트 운영체제랑 너무 협력이 강했기 때문이다. 이게 1990~2000년대 이야기다.<br />
<br />
그러다가 휴대용 기기가 많이 나오기 시작하면서, 에너지 효율이 좋은 ARM 프로세서가 인기를 얻게 됐다. 닌텐도 게임기에도 ARM 프로세서가 들어간다.<br />
<br />
인텔은 컴퓨터 시장에서 독점으로 돈 벌며 휴대기기 시장 무시하다가 결국 핸드폰 프로세서 시장을 놓쳐버렸다. 인텔은 쭉 PC용 칩, 서버용 칩만 만들었다.<br />
<br />
애플이 아이폰 만들기 전에 인텔을 찾아가서 핸드폰이 컴퓨터처럼 동작할 수 있게 프로세서 만들어줄 수 있겠냐고 계약을 제시했었는데, 인텔이 계약을 받아들이지 않았다.<br />
<br />
인텔도 나중에는 모바일 칩 제작하려 했지만, 별 성과는 없었다.<br />
<br />
Applied materials: 웨이퍼 위에 얇은 막 씌우는 장비 제조사<br />
Lam Research: Etching 기술력이 최고<br />
KLA: 웨이퍼, 마스크 위 나노미터 단위 오차 찾아내는 장비 제조사<br />
Tokyo Electron도 장비 제조사다.<br />
<br />
2010년대 초, 당시 최신 마이크로프로세서는 개당 10억개의 트랜지스터를 갖고 있었다. 이런 칩을 설계하려면 설계용 소프트웨어가 필요하고, 그런 소프트웨어를 만들 수 있는 곳은 Cadence, Synopsys, Mentor 3곳이었다. 모두 미국회사다.<br />
<br />
2000년대 들어, 많이들 반도체 산업을 3개 영역으로 나누게 되었다.<br />
Logic: 스마트폰, 컴퓨터, 서버를 운영하는 프로세서<br />
Memory: DRAM, 플래시메모리 등<br />
Analog: 시각,음성신호를 디지털 데이터로 치환, 네트워크에 접속하고 통신할 수 있게 해주는 통신 칩, 장비 제어 등<br />
<br />
여기서, 3번째 영역인 아날로그 영역은 무어의 법칙과 무관하다. 즉, 매년 성능이 exponential하게 증가하지 않는다.<br />
<br />
요즘 Analog 칩들은 3/4가 180nm 이상 공정에서 생산된다. 180nm 공정은 1990년대 후반에 나온 공정인데도 말이다.<br />
<br />
그래서 analog칩 분야의 경제 논리는 logic칩, 메모리칩 분야와 다르다. 무조건 트랜지스터 크기를 줄여야 하는 분야가 아닌 것이다.<br />
<br />
그래서 analog칩 만들거면 트랜지스터 크기 줄이려고 미친듯이 경쟁할 필요가 없고, 따라서 비싼 팹을 쓸 필요도 없다. 그렇기에 이익률이 높은 분야다. 대신, 설계 잘하는게 그만큼 중요한 분야다.<br />
<br />
지금 아날로그 칩 업체중 가장 큰 곳은 TI고, 그 밖에도 Onsemi, Skyworks, Analog Device같은 미국 회사들이 있다.<br />
<br />
1990년대 말, 일본 DRAM업체들은 Elpida라는 이름으로 하나의 회사가 되었다. 일본 경제도 안좋았고, 마이크론, 삼성전자, 하이닉스에 맞서기 위해서였다. 하지만 Elpida는 2013년에 마이크론에게 인수되고 말았다.<br />
<br />
이렇게 되어, 마이크론은 미국 뿐 아니라 일본, 대만, 싱가포르에도 팹을 보유하게 됐다. 결국, 이 세상 모든 DRAM은 동아시아에서 만들어진다.<br />
<br />
삼성잔자는 낸드플래시 메모리의 35%를 만든다. 나머지를 일본의 Kioxia, 미국의 마이크론과 웨스턴디지털이 나눠갖는다. 마이크론과 웨스턴디지털도 생산설비가 일본, 싱가포르에 있어서, 결국 낸드플래시 메모리도 다 동아시아에서 만들어진다.<br />
<br />
오늘날, 최신 팹을 갖추려면 200억달러 이상이 든다. 그래서 요즘은 다들 팹리스 회사를 만든다! 팹리스 창업은 수백만 달러면 된다.<br />
<br />
엔비디아는 1993년에 만들어졌다. 엔비디아는 그래픽 연산을 위한 프로세서, GPU들을 만들었다. 그러다가 2006년 CUDA를 내놨다. CUDA를 쓰면 GPU를 그래픽이랑 상관 없는 분야에서도 쓸 수 있다. 즉, 시장이 넓어진다!<br />
<br />
그래서 엔비디아가 CUDA를 열심히 관리한다. 병렬처리가 필요한 곳이면 어디서든 GPU를 쓸 수 있다.<br />
<br />
퀄컴은 1985년에 세워졌다. Quality + Communication = Qualcomm<br />
퀄컴은 CDMA가 성공한 후, 통신 칩 뿐 아니라 아예 AP(Application Processor)를 통째로 만들기 시작했다.<br />
CDMA는 연산이 많이 필요한데, 그걸 가능하게 하는 칩을 만든 것이다.<br />
<br />
FPGA(Field-Programmable Gate Array)하는 회사는 Altera, Xilinx가 있다.<br />
<br />
AMD도 원래 팹이 있었지만, 아부다비 정부에 제조 사업부를 매각했다. 그게 지금의 Global Foundries다.<br />
<br />
2000년대 주안부터는 SiO2 두께가 원자 2개정도가 되어 tunneling이 심해졌다. 그리고 채널 폭도 너무 좁아져서, 전류가 새어나가는 일이 많아졌다.<br />
<br />
그래서 FinFET이 나왔다. FinFET은 전기장을 위에서만 거는게 아니라 옆으로도 걸어서, 전자를 더 잘 제어할 수 있고 전류 누설도 막을 수 있다. 물론, 이런 나노미터 단위 3차원 구조는 엄청난 공정 기술이 필요하다.<br />
<br />
2008년 금융 위기때, 제품이 안팔리니 반도체 업계도 힘들었다. TSMC는 이때, 모리스 창의 주도 하에 모바일 시장 내다보고 투자 엄청나게 헀다.<br />
<br />
1세대 아이폰:<br />
운영체제는 애플이  만든 iOS였지만, 칩 설계와 생산은 삼성전자에서 많이 했다. 그 외에도 인텔의 메모리 칩, Wolfson의 오디오 프로세서, 독일의 Infinion의 무선 네트워크 접속용 모뎀 칩, CSR의 블루투스 칩, Skyworks의 신호 증폭기 등이 들어갔다.<br />
<br />
아이폰 출시 1년 후, 애플은 PA semi라는 에너지 효율 좋은 프로세서를 설계하는 스타트업을 인수하고, 반도체 설계자들을 고용했다.<br />
2년 후, 애플은 A4라는 자체 AP를 만들었다고 발표했다. 아이폰4에 이 A4가 들어갔는데, 아마 이게 짐 켈러 업적일거다.<br />
<br />
애플은 메인 프로세서 뿐 아니라 에어팟같은 주변기기의 보조 칩도 스스로 만든다.<br />
<br />
스마트폰 AP는 설계하는 데에 비용이 많이 든다 그래서 중저가 스마트폰 만드는 회사들은 그냥 퀄컴에서 칩 사온다.<br />
<br />
아이폰에 의해 노키아, 블랙베리같은 회사들은 다 죽어버렸다.<br />
<br />
Foxconn, Wistron같은 대만 기업들은 중국에서 애플 제품 조립 설비를 운영한다.<br />
<br />
EUV 애기가 갑자기 다시 나오는데, 옛날에는 리소그래피에 그냥 가시광선을 이용했다.<br />
근데 가시광선이면 파장이 수백나노미터 정도라서, 회로가 작아지자 한계에 봉착했다.<br />
<br />
그래서, 가시광선보다 파장이 짧은 248nm, 193nm 자외선들을 리소그래피에 쓰기 시작했다.<br />
이렇게 하면 가시광선보다 더 정교한 그림을 그릴 수 있었지만, 여기에도 한계가 있었다.<br />
<br />
그래서, 회사들은 파장 13.5nm EUV(극자외선)을 연구하기 시작했다.<br />
<br />
DUV(심자외선)을 쓸 때는 자외선을 물에 투과시키거나 여러 겹의 마스크에 통과시킴으로써 193nm DUV로 193nm보다 좁은 폭의 패턴을 그렸다.<br />
이 짓거리가 2010년대 중반 3nm FinFET 공정까지 이어졌다! 하지만 그 이상은 이젠 너무 어려웠다.<br />
<br />
TSMC, 삼성전자, 인텔, 글로벌파운드리즈 모두 EUV 도입을 원했다. 글로벌파운드리즈는 2010년 싱가포르의 파운드리업체인 차터드반도체를 인수했고, 2014년에는 IBM 반도체사업부를 인수했다.<br />
IBM이 소프트웨어에만 집중하기로 하면서 팔아버렸거든.<br />
<br />
하지만 EUV개발에는 돈이 너무 많이 들어서, 글로벌파운드리즈는 결국 EUV 경쟁을 포기했다. 이제 삼성전자, TSMC, 인텔밖에 남지 않았다.<br />
<br />
2010년대 실리콘밸리에서, 반도체설계와 제조를 모두 하는 회사는 인텔 뿐이었다. 하지만 둘다 어중간했다.<br />
<br />
2010년대 들어, 데이터센터용 프로세서와 서버용 프로세서가 많이 필요해지면서 인텔 칩이 많이 팔렸다.<br />
아마존 웹서비스, 마이크로소프트 azure, 구글 클라우드 등<br />
<br />
근데, 그 2010년대쯤부터 연산력에 대한 수요가 변하기 시작했다. AI 때문이다.<br />
CPU는 서로 다른 유형의 계산을 수행할 수 있기 때문에 범용성을 가지지만, 그 모든 계산을 순차적으로, 한번에 하나씩 해야 한다.<br />
<br />
하지만 AI는 매번 다른 데이터를 받아서 매번 같은 연산을 하는 것으로 학습된다. 그래서 범용성같은거 필요 없고, 그냥 노가다 잘하는 칩이 있으면 된다.<br />
<br />
GPU는 병렬로 연산을 진행하기 때문에 많은 계산을 동시에 처리할 수 있다. 그래서 AI에 GPU가 많이 쓰이게 되었고, 이게 2010년대 초다.<br />
<br />
엔비디아는 인공지능에 회사의 미래를 걸고, CUDA를 개선하기 위해 엄청나게 노력했다. 그리고 인공지능 뿐 아니라 데이터센터에서도 GPU를 많이 필요로 하게 되어, 엔비디아가 떡상했다.<br />
<br />
그렇다고 엔비디아 미래가 무조건 밝은건 아니다. 클라우드 회사들(아마존, 마이크로소프트, 구글, 텐센트, 알리바바)이 그들 각자의 수요에 맞는 칩을 설계하기 시작했기 때문이다.<br />
<br />
예를 들어, 구글은 구글의 Tensorflow 라이브러리에 최적화된 TPU(Tensor Processing Unit)를 구글 데이터센터에 쓰고 있다.<br />
<br />
하여간, GPU가 훨씬 중요해졌기 때문에 인텔의 데이터센터용 프로세서도 이제 많이 안팔리게 됐다. 결국, 인텔은 반도체 설계한것도 잘 안팔리고, 파운드리는 TSMC, 삼성전자랑 기술 격차가 말도 안되는 수준이다. 그냥 회사가 좀 망했다.<br />
<br />
2020년 기준, 지구상의 EUV장비 중 절반은 TSMC에 있다.<br />
<br />
2020년대, 최첨단 프로세서를 제조할 수 있는 회사는 TSMC와 삼성전자 뿐이다. 근데 둘다 동아시아에 있고, 중국이랑 가까워서 미국이 걱정이 많다.<br />
<br />
중국은 구글, 페이스북을 차단하고 바이두, 텐센트로 그 자리를 대체하고 있다. 애플, 마이크로소프트는 중국 검열에 협조하겠다고 한 뒤 중국 시장에 들어갈 수 있었다.<br />
<br />
2000년대, 2010년대에 중국이 가장 많은 돈을 쓴 수입제품은 석유가 아니라 반도체였다.<br />
<br />
시진핑의 ‘중국제조2025’라는 계획이 있다. 2015년 85%였던 반도체 수입 비중을 2025년에는 30%까지 줄이자는거다.<br />
근데 첨단 팹의 가격은 2014년에 이미 100억달러정도 했다. 그래서 달성하려면 돈이 엄청 들거다.<br />
<br />
2017년, 중국은 2600억달러어치 반도체를 수입했다.<br />
이건 사우디아라비아의 석유 수출이나 독일의 자동차 수출보다 훨씬 많은 금액이다.<br />
<br />
2017년 기준으로, 집적회로는 한국 수출 총액의 15%, 싱가포르 수출 총액의 17%, 말레이시아 수출 총액의 19%, 필리핀 수출 총액의 21%, 대만 수출 총액의 36%를 차지한다. 중국은 이렇게나 큰 시장을 내수화 하려고 하는거다.<br />
이 전자제푸 공금망은 지난 50년간 아시아 경제 성장을 떠받쳐왔다.<br />
<br />
IBM은 미국 정부에 소프트웨어를 많이 팔아왔는데, 에드워드 스노든이란 사람이 모스크바로 망명하며 미국의 첩보작전 관련 문서들을 다 공개해버렸다.
미국 정부의 도청 정황이 드러난 문서들이었는데 그래서 중국 회사들이 IBM 제품들을 구매하지 않게 됐다.<br />
<br />
그래서 IBM은 중국 정부를 찾아가서 반도체 기술좀 줄테니까 우리 제품좀 사달라고 했다. 이게 IBM만 그런게 아니고, 퀄컴, AMD도 기술 협약, 기술 이전을 체결하면서라도 중국 시장에 들어가려고 했다.<br />
이런 식으로, 중국은 반도체 제조 기술을 쌓아가고 있다.<br />
<br />
칭화유니그룹은 칭화대가 설립한 회사로, 표방하는건 대학의 연구를 사업으로 전환하는 것이지만, 실제로는 중국 정부가 시키는 대로 부동산, 반도체에 돈 뿌리는 회사다. 얘네는 해외 반도체 회사들 인수하려고 많은 노력을 했다.<br />
<br />
이병철이 건어물 회사였던 삼성을 키운 방법은:<br />
1. 정부와 좋은 관게를 유지하려고 노력했다.<br />
2. 서양, 일본의 제품 몇개를 골라 그것을 같은 품질에 낮은 가격으로 만드는걸 목표로 했다.<br />
3. 세계화에 적극적이었다.<br />
<br />
중국에서는 화웨이가 텐센트, 알리바바와 달리 초기부터 외국과의 경쟁을 받아들였다. 창업자가 런정페이인데, 런정페이도 이병철처럼 가성비 좋은 버전 만들어서 파는데에 주력했다. 런정페이는 충칭대 출신이고, 1987년에 화웨이 창업했다.<br />
<br />
선전은 외국 투자가 장려되는 곳이라, 런정페이는 선전으로 가서 전화교환기 사업을 시작했다. 그리고 이 기술은 이후 데이터 송수신에 게속 응용됐으며, 현재 무선기지국 분야 3대 사업자는 핀란드의 노키아, 스웨덴의 에릭슨, 중국의 화웨이다.<br />
<br />
화웨이는 매년 150억달러 정도를 연구개발에 쓴다. 이 정도를 연구개발에 쓰는 회사는 구글, 아마존, 폭스바겐 정도다.<br />
이게 가능한 이유는 중국 정부 지원을 받아서도 있다.<br />
<br />
화웨이는 인프라 제공을 넘어 스마트폰 시장까지 들어갔다. 그리고 자사 스마트폰용 반도체 개발도 한다!<br />
<br />
2010년대 말, 화웨이의 HiSilicon 사업부는 TSMC의 두번째로 큰 고객이 됐다. 가장 큰 고객은 애플이다.<br />
어쨌든 화웨이는 무려 스마트폰 프로세서까지 설계하는 회사가 된거다. 이건 엄청난 기술력이다.<br />
<br />
2G폰은 사진을 주고받을 수 있었고, 3G폰은 웹사이트를 열 수 있었으며, 4G폰은 비디오 스트리밍이 가능했다.<br />
<br />
주파수, 그니까 spectrum은 Silicon보다 훨씬 비싸다. 그래서 퀄컴은 송수신되는 데이터를 최적화할 방법을 찾아내고,<br />
아날로그디바이스같은 회사들은 더 정교하게, 더 효율적으로 무선 신호를 주고받을 수 있ㄴ느 RF transceiver들을 만들었다.<br />
<br />
Beamforming: 기지국이 전화기의 위치를 파악한 후, 그쪽으로 전파를 보낸다.<br />
<br />
테슬라도 반도체 설계 분야의 주요 회사중 하나다. 자체 설계한 반도체를 많이 쓴다.<br />
<br />
AI에 중요한건 데이터, 알고리즘, 연산력이다. 이 중 중국이 가지지 못한건 연산력이다.<br />
연산력에 쓰이는 하드웨어 다 미국이 설계하고 대만이 만들기 때문이다.<br />
<br />
아날로그 칩, RF칩은 비싼 fab을 쓸 필요는 없다.<br />
<br />
ZTE: 중국 국영 통신 인프라 업체. 화웨이는 민간 업체다.<br />
<br />
트럼프 취임때쯤, 중국은 미국의 최대 고객이자 최대 경쟁자였다.<br />
<br />
DRAM 시장은 규모의 경제를 필요로 한다. 그래서 삼성전자, 하이닉스, 마이크론같은 대기업들밖에 안남았다.<br />
<br />
현재 반도체 장비 제작사들은 미국(Applied Materials, Lam Research, KLA), 일본, 네덜란드(ASML)이 전부다.<br />
그래서 얘네한테 무역제재당하면 반도체 못만든다.<br />
<br />
푸젠진화반도체(JHICC)가 마이크론 DRAM 기밀 훔쳐가자, 미국이 일본까지 끌어들여 함께 무역제재를 걸어버렸다.<br />
결국 JHICC는 DRAM 생산을 못하게 됐다.<br />
<br />
미국은 동맹국들에게 5G네트워크에서 화웨이를 손절하라고 말햇다. 호주, 폴란드, 프랑스 등은 미국 말대로 완전 손절했는데,<br />
독일은 자동차 시장때문에 눈치를 봤고 영국은 아예 손절을 안했다.<br />
<br />
2020년, 미국은 화웨이를 패기로 했다. 일단 미국산 칩을 화웨이에 수출 못하게 헀다.<br />
하지만 화웨이는 칩 설계 역량이 있엇기 때문에, TSMC만 안막히면 큰 문제 없었다.<br />
<br />
그래서 2020년 5월에는 모든 미국산 반도체 관련 제품 수출을 막았다.<br />
미국 장비가 없으면 TSMC라고 해도 칩을 만들어줄 수가 없다!<br />
이렇게 되자 영국도 화웨이를 손절했다.<br />
<br />
그렇다고 중국 회사들이 다 망한것은 아니다. 텐센트, 알리바바는 살아있고, SMIC도 첨당 장비는 못사도 공장 계속 굴리고 있고,<br />
화웨이도 첨단 반도체만 못사는거라 4G 네트워크 사업은 게속 하고 있다.<br />
<br />
우한에 YMTC라는 낸드플래시 메모리업체가 있는데, 여기는 코로나 락다운때도 안닫혔다.<br />
정부에서 아예 특별 열차편을 만들어서 회사 오갈 수 있게 해줬다고 한다.<br />
<br />
요즘 ASML에서 만들려는 것은 high-NA EUV다.<br />
NA = Numerical Aperture. 2020년 중반 출시 예정이고, 대당 3억달러쯤 할 예정이다.<br />
<br />
RISC-V는 x86이나 ARM과 달리 open source architecture다.<br />
RISC-V 재단은 지정학적 중립성을 위해 미국에서 스위스로 위치를 옮겼다.<br />
알리바바 등 회사들은 RISC-V에 기바한 프로세서들을 설계하고 있다.<br />
<br />
SMIC 전략: 스마트폰, 데이터센터는 첨단 칩이 필요하지만, 자동차는 그렇지 않다.<br />
그래서, 구세대칩을 찍어내서 첨단기술이 필요하지 않은 전기차에 많이 들어가게 한다.<br />
<br />
중국은 SiC, GaN(질화갈륨) 등 반도체 물질에도 투자한다.<br />
전기차 출력 관리용 반도체에 많이 쓰일 것으로 예상돼서.<br />
중국은 전기차 많이 하는 나라다.<br />
<br />
2020년, 코로나 때문에 자동차 회사들이 반도체 주문을 많이 줄였다.<br />
그러는 동안 사람들은 컴퓨터를 사서 PC, 데이터센터용 칩이 많이 필렸다.<br />
<br />
근데 자동차 수요가 생각보다 빨리 회복돼서, 자동차 회사들이 뒤늦게 반도체를 주문하긴 했지만 주문이 밀려서 고생했다.<br /></p>]]></content><author><name></name></author><category term="History" /><summary type="html"><![CDATA[Chip war 내용 정리: 2차대전까지는 강철과 알루미늄으로, 냉전때는 핵 개수로 승부했다면 현대 사회에서는 국가들이 computing power, 즉 연산력으로 승부한다. 아시아 국가들은 지난 50년간 반도체 흐름을 타고, 실리콘을 발판으로 부상했다. 아이폰에 들어가는 배터리, 블루투스, 와이파이, 통신, 오디오, 카메라 관리는 모두 다른 칩들이 한다. 메모리 칩은 일본의 키오시아, 무선 주파수 인식(Radiofrequency identification)칩은 캘리포니아의 skyworks, 오디오칩은 오스틴의 Cirrus Logic이 만든다. 아이폰의 운영체제가 돌아가는 프로세서는 애플이 설계하긴 하지만, TSMC에서 만든다. 아이폰 12S에는 개당 118억개정도 되는 트랜지스터들이 들어간다. ARM은 일본 소프트뱅크에 인수된 영국회사인데, 연구소가 캘리포니아랑 이스라엘에 있다. 칩 테스트는 동남아시아에서 많이 한다. OPEC도 세계 석유 40%를 갖고 있을 뿐이다. 경제 분야 중 이렇게 소수 기업들이 이것저것 독점하고 있는 분야는 반도체뿐이다. 에니악에는 진공관 18,000개가 들어갔다. 즉 스위치 18,000개가 들어갔다. 진공관은 작동할 때 전구처럼 빛나기 때문에, 당시 벌레가 많이 꼬였다. 그래서 주기적으로 ‘debugging’을 해줘야 했다. 벨 연구소는 뉴저지에 있었다. AT&amp;T는 전화 회사였기 때문에, 트랜지스터를 스위치보다는 증폭기로 생각했다. TI는 2차대전때 잠수함 탐지용 초음파 장비를 만들어 팔았고, 냉전때는 미국 정부에 IC 많이 팔아 부자가 되었다. 트랜지스터 관련해서 노벨상 받은 사람들: John Bardeen은 2번, William Shockley도 1번 Minuteman: 미국-우주-모스크바 상정한 궤도 진입 미사일 TI에서 처음 리소그래피로 회로 그릴때는 코닥의 감광액을 썼는데, 그냥 쓰기에는 감광액의 순도가 부족해서 원심분리기로 감광액을 재처리해 썼다. Morris Chang(모리스 창)은 TSMC 창업자인데, 중국 본토 출신이고 2차대전까지 중국 살다가 국공내전때 탈출했다. 홍콩을 거쳐 미국으로 갔고, 처음에는 하버드 영문학과 다니다가 MIT 기계공학과로 옮겼다. 졸업 후에는 Sylvania라는 회사에 갔다가 TI로 갔고, TI의 수율을 엄청나게 끌어올렸다. 쇼클리는 노벨상까지 받은 사람이었지만, 사업은 결국 잘 안됐다. 진짜 산업을 이끈건 시간 갈아넣으면서 수율 개선한 사람들이었다. 2022년 기준으로, 한국은 모든 메모리 칩의 44%, 모든 프로세서 칩의 8%를 생산한다. 일본은 모든 종류의 칩의 17%를 생산한다. 대만은 모든 프로세서 칩의 41%, 최첨단 칩의 90%를 생산한다. 싱가포르는 모든 종류 칩의 5%를, 중국은 15%를 생산한다. 소련은 미국 반도체를 베끼려고 했는데, 이게 핵무기 베끼는것까지는 됐는데 반도체 베끼는 데에는 실패했다. 소련의 장점은 품질이 아니라 물량이었는데, 반도체는 품질이 너무나 중요하다. 소련에 훌륭한 과학자들을 많았지만, 공정에는 문서로 정리되지 않은 부분들도 많이 필요했고, 그 모든걸 스파이가 훔쳐다줄수는 없었다. 그래서 공정이 불가능했고, 어찌저찌 하더라도 수율이 안좋았다. 1962년 11월, 일본의 이케다 하야토 총리가 프랑스 가서 샤를드골을 만나고, 소니 트랜지스터 라디오를 선물했다. 1960년대 말, 대만 시급은 19센트, 말레이시아 시급은 15센트, 싱가포르는 11센트, 한국은 10센트였다. 미국은 인건비도 비싼데다가 노조까지 있었다. 그래서 페어차일드 반도체는 반도체 제품 조립을 홍콩에서 했다. 그 뒤 싱가포르도 추가됐다. 베트남전쟁 이후, 동아시아 반공정권들은 베트남같이 되지 않기 위해, 미국이 자기 나라에 더욱 관심가지게 하려고 노력한다. 그 노력중에는 반도체 공장을 짓는 것도 있었다. Fairchild Semiconductor(페어차일드 반도체)는 Gordon Moore가 처음 세웠던 회사다. 이 사람이 두번째로 세운 회사가 인텔이다. Intel은 Integrated Electronics의 줄임말이다. 인텔의 첫 제품은 DRAM이었다. 메모리칩은 기기에 맞게 특화될 필요가 없기 때문에 대량생산이 용이하다. 다른 칩들은 기억이 아니라 계산을 해야 해서, 모두 다른 구조로 특별히 설계되어야 했다. 이때 인텔 생각: 표준화된 로직 칩과 메모리칩이 있으면, 필요한 기능에 해당하는 소프트웨어만 설치하면 될거다. 이 생각이 나중에 cpu로 이어진다. HP는 Hewlett-Packard의 약자고, 창업자 2명의 이름이다. 둘 다 스탠포드 출신이다. 1980년대는 미국 반도체 업계에게 힘든 시기였다. 일본이랑 경쟁해야 했다. 1980년대에는 인텔, TI뿐 아니라 도시마, NEC같은 일본 회사들도 DRAM을 만들게 됐다. 옛날 일본은 ‘우리가 미국보다 혁신은 느려도 실현은 더 빠르다’라고 하며 미국을 쫓아갔는데, 가전제품 시장을 일본이 먹어치우기 시작하고 1979년에는 워크맨까지 나오며 혁신으로도 미국을 앞질러버렸다. 미군정이 일본을 키운거긴 하다. 망하게 했다가 공산화되는 것보다는 부하로 잘 키워놓는게 낫다고 판단해서 미군정 이후로도 미국은 일본을 계속 도와줬다. 일본도 미국에 산업스파이 엄청 보냈다. 일본 정부는 1974년까지 미국 기업이 일본에 팔 수 있는 반도체 개수에 쿼터제를 도입하고 있었다. 그래서 일본 기업들이 미국 시장 먹는동안 미국 기업들은 일본 시장을 먹을 수가 없었다. 일본 정부가 자국 회사들에게 돈을 주기도 했다. 금리 차이도 컸다. 1980년대 미국은 베트남전쟁때 달러 찍어낸 여파로 생긴 인플레이션에 고통받고 있었다. 기준금리가 높을때는 21.5%까지도 올라갔다고 한다. 하지만 일본은 사람들이 하도 저축을 많이 해서 은행에 현금이 많았고, 그래서 은행들이 기업들에게 저금리 대출을 많이 해줄 수 있었다. 당시 일본 금리는 6~7%정도였다. 인텔이 DRAM을 개발한 지 10년 뒤, 인텔의 DRAM 점유율은 1.7%밖에 안되게 되었다. 일본이 DRAM 시장을 다 먹어버렸다! 80년대 내내, 미국 시장에서 일본의 반도체 점유율은 계속 상승했다. 반도체 공정 쪽도 미국이 그리 앞서나가지 못했다. Photolithography는 현미경 뒤집어서 작은 그림 그리는 데에서 시작했던 만큼 렌즈가 필요한데, 당시 렌즈는 독일의 Carl Zeiss와 일본의 Nikon이 주도했다. Stepper라는 장비가 있는데, 웨이퍼 전체에 빛을 쏜느게 아니라 die마다 빛을 쏘게 해주는 장비다. 원래 1978년에 미국 GCA가 만들었지만, 80년대 중반부터는 Nikon에게 시장을 뺏겼다. 이때쯤만 해도 반도체 공정이 정말 정밀해져서, 천둥이 치면 기압이 변해서 공기 굴절률이 변해 빛이 꺾여 그림이 이상하게 그려질 수 있는 수준이었다. 1973년, 1979년에 각각 오일쇼크가 있었다. 원인은 OPEC이 감행한 석유수출 통제였다. 당시 아랍 국가들이 이스라엘과 친한 미국을 조지려고 석유 수출을 차단한 것이다. 그 결과 스태그플레이션 등 많은 문제들이 생겼다. 실리콘밸리 사람들은 미국 정부에 가서, 반도체 일본이 다 먹어치우게 내버려두면 반도체도 석유 꼴 난다고 말하며 좀 도와달라고 했다. 1986년, 미국 정부가 일본 정부에 항의해 일본은 DRAM 수출량을 제한했다. 하지만 미국 반도체 회사들은 이미 거의 다 뒤진 후였다. 인텔 창립자 중 하나인 밥 노이스가 미국 정부랑 같이 미국 반도체 회사들좀 살려보려고 노력했다. 특히 당시 니콘, 캐논, ASML을 경쟁사로 갖고 있던 GCA를 어떻게든 살려보려고 많이 노력했지만, 밥 노이스는 1990년에 죽고 GCA도 1993년에 망했다. 미국 내 리소그래피 업계는 이렇게 다 죽어버렸다. 마이크론은 1978년에 설립됐는데, 이 시기는 미국 DRAM 시장이 일본한테 탈탈 털리던 시기였다. 이때, 마이크론은 3가지 노력을 했다. 1. DRAM 성능이 좀 별로더라도 가격이 싸도록 설계해서 비용 절감, 2. DRAM 성능이 좀 별로더라도 공정 단순화해서 비용 절감, 3. 일본 반도체 칩에 관세 물려달라고 정부한테 찡찡거리기 인텔은 DRAM 사업이 박살나긴 했지만, 마이크로프로세서 사업은 아직 살아있었다. 이때쯤 IBM에서 개인용 컴퓨터를 출시했는데, 인텔이 거기 들어갈 칩을 만들었다. 그리고, 인텔은 결국 DRAM을 포기하고 마이크로프로세서에 집중하기로 했다. 인텔은 직원들 일본 보내서 공정 다 배워오게 하고, 회사 분위기를 기계적으로 바꿨다. 이제 연구소보다는 공장이 됐고, 수율도 오르고 비용도 절감됐다. 그리고 1985년 플라자 합의로 엔화 가치가 2배가 되어 미국도 수출 경쟁력이 생겼고, 이때쯤 미국 금리도 내려오기 시작했다. 이렇게 인텔이 PC용 칩, 마이크로소프트가 운영체제 독점한 상태로 컴퓨터 시대가 열렸다. 삼성이 반도체 사업을 시작하기 전부터, 한국은 미국, 일본 칩의 조립과 패키징을 하는 주요 장소였다. 삼성전자가 성공한건 한국 정부와 미국 기업들의 지원 덕분이었다. 미국 기업들 입장에서는 DRAM 만들어봐야 일본한테 털리니까 삼성전자에 관련기술 던져주고 돈이라도 받는게 나았다. 일본 말고 다른 DRAM 공급원 있으면 좋기도 하고. 이게 1983년이다. UCSD의 Andrew Viterbi가 만든 알고리즘 CDMA는 이론상 훌륭하긴 했는데, 연산량이 아주 많았다. 그러다 반도체가 계속 발전중이니 이제 가능해질거다! 라고 생각한 Andrew Viterbi와 Irwin Jacobs가 퀄컴을 창업했다. 이렇게 CDMA를 통해, 주어진 BW 안에 더 많은 신호를 넣을 수 있게 됐다. 소련은 미국 반도체, 장비들을 계속 훔쳐갔지만 결국 따라잡지는 못했다. 소련의 문제는: 1. 정치적 간섭이 심했고, 2. 시장이 군수시장 뿐이었고, 3. 국제 공급망이 없어서 미국-일본-동아시아 같은 협력이 불가능했다. 반도체로 강화된 무기는 걸프전에서 볼 수 있었다. 당시 뉴욕타임즈 헤드라인: 강철을 이긴 실리콘 페이브웨이 폭탄: TI가 만든 유도탄 일본 기업들은 정부 지원을 계속 받아, 그냥 만들던거 계속 찍어냈다. 삼성전자, 마이크론 등이 DRAM 시장에 들어와도 그들은 그냥 무지성으로 생산량 더 늘려서 더 많이 찍어냈다. 소니는 다른 일본 회사들이랑 다르게, DRAM 말고 이미지센서에 투자했다. 그리고 아직까지 그걸로 먹고 산다. 도시바에서는 마스오카 후지오가 플래시메모리를 만들었는데, 도시바는 발견을 무시했고 결국 그걸 제품으로 개발해 시장에 내놓은 것은 인텔이었다. 일본 기업들은 DRAM으로 돈을 많이 벌어서 마이크로프로세서 시장을 무시했지만, 컴퓨터 시대가 오자 DRAM보다 마이크로프로세서가 훨씬 중요해졌다. 1990년에는 고르바초프가 실리콘밸리를 방문하고, 스탠포드에서 강연도 했다. 이렇게 냉전 패배 인정했다. 1993년부터 미국은 반도체를 다시 수출하기 시작헀고, 1998년에는 삼성전자가 DRAM 최대생산자 자리를 먹었다. 1980년대 말에 90%였던 일본의 시장 점유율은 1998년에는 20%가 됐다. 대만 입장에서는, 반도체 조립은 동아시아 국가들 모두 하니까 뭔가 더 돈 될만한 다른 산업을 찾아야 했다. 한국처럼 조립에서 생산으로 나아가지 못하면 중국한테 일 다 뺏길 상황이었다. 대만은 모리스 창을 불러서, 니 하고싶은거 다 하게 해줄 테니까 대만에 반도체 산업 한번 만들어보라고 했다. 모리스 창이 하고 싶었던건 반도체 제조 특화 기업이었다. TSMC 설립에 투자한건 대만 정부, 대만 자본가들, 필립스 정도였다. 그마저도 대만 자본가들은 대만 정부에 의해 강제참여 당한거였다. 실리콘밸리의 작은 팹리스 업체들은 항상 다른 회사 fab을 빌려야 했는데, 그러면서도 항상 기술 도둑맞지 않을까 불안했다. 그래서 그들에게 TSMC가 아주 좋아보였다. TSMC는 1987년에 세워졌는데, 화웨이도 런정페이가 1987년에 선전에 세웠다. 80년대 중국 지도자는 장쩌민이었는데, 상하이교통대 전기공학과 출신이라 그런지 전자기술을 중요시했다. DRAM시장을 계속 점유하려면 공정 선폭(technology node)의 연속성을 확보해야 하는데, 여기에 많은 돈이 든다. 삼성전자는 정부의 지원으로 이걸 확보했다. Technology node: 해당 공정에서 구현 가능한 최소 선폭 중국은 반도체 생산이 중요하다고 생각하긴 했는데, 반도체를 생산할 인프라는 없었다. 그러던 중 리처드 창이 2000년에 중국에 SMIC를 설립했고, TSMC를 따라하기 시작했다. 이런 식으로, TSMC의 경쟁자들이 많아지기 시작했다. SMIC, 싱가포르의 차터드반도체, 대만의 UMC, 대만의 뱅가드반도체, 한국의 삼성전자 등이 경쟁자였다. *중국 주석: 마오쩌둥-화궈펑-덩샤오핑-장쩌민-후진타오-시진핑 1990년대에는 리소그래피의 미래가 불투명한 시대였다. 당시에는 파장을 더 줄여서, DUV를 넘어 EUV를 써야 하는 시대가 왔다. 그런데, EUV 장비 개발은 돈이 정말 엄청나게 드는 일이었다. 당시 리소그래피 회사는 캐논, 니콘, ASML이 있었다. ASML은 필립스의 리소그래피 부서가 떨어져 나와 생긴 곳이다. ASML은 네덜란드 회사기 때문에, 미국 회사들은 일본의 캐논, 니콘보다는 ASML과 거래하는걸 더 선호했다. 그리고 필립스가 TSMC 초기 투자자였기 때문에, ASML은 TSMC라는 확실한 고객을 갖고 성장할 수 있엇다. 결국 캐논, 니콘은 EUV 장비 개발을 포기했고, ASML만 성공했다. CPU가 계산하는 방식을 설정하는 규칙 모음 = CPU architecture 인텔이 만든 아키텍처가 x86이었고, x86이 개인 컴퓨터 CPU의 표준이 되었다. X86이 최고의 아키텍처라서 시장을 먹었다기보다는, IBM이 만든 최초의 개인용 컴퓨터에서 x86 아키텍처를 썼기 때문이다. X86 기반 CPU를 안쓰는 컴퓨터 업체로는 애플이 있었지만, 2006년부터는 애플도 x86 기반 CPU를 쓰기로 했다. UC 버클리에서는 RISC라는 아키텍처를 만들었는데, 이 아키텍처가 x86보다 더 효율적이고 가볍다. 그래서 인텔도 x86 버리고 RISC로 옮길까 하다가, 전환 비용이 너무 비싸다고 판단해 그만뒀다. 그래서 지금도 컴퓨터들은 대부분 x86 기반이다. 인텔은 데이터센터 시장도 그들의 프로세서를 내세워 IBM, HP를 밀어내고 먹어버렸다. 현재 인텔 또는 AMD 프로세서 없이는 데이터센터가 있을 수 없다. 인텔이 버린 RISC는 1990년 애플이 세운 벤처회사 ARM이 써보기로 했다. ARM은 직접 칩을 설계하지는 않고, 아키텍처 사용권 라이선스만 팹리스 업체들에게 판다. 하지만 ARM은 인텔의 점유율을 뺏지는 못했다. 인텔 프로세서랑 마이크로소프트 운영체제랑 너무 협력이 강했기 때문이다. 이게 1990~2000년대 이야기다. 그러다가 휴대용 기기가 많이 나오기 시작하면서, 에너지 효율이 좋은 ARM 프로세서가 인기를 얻게 됐다. 닌텐도 게임기에도 ARM 프로세서가 들어간다. 인텔은 컴퓨터 시장에서 독점으로 돈 벌며 휴대기기 시장 무시하다가 결국 핸드폰 프로세서 시장을 놓쳐버렸다. 인텔은 쭉 PC용 칩, 서버용 칩만 만들었다. 애플이 아이폰 만들기 전에 인텔을 찾아가서 핸드폰이 컴퓨터처럼 동작할 수 있게 프로세서 만들어줄 수 있겠냐고 계약을 제시했었는데, 인텔이 계약을 받아들이지 않았다. 인텔도 나중에는 모바일 칩 제작하려 했지만, 별 성과는 없었다. Applied materials: 웨이퍼 위에 얇은 막 씌우는 장비 제조사 Lam Research: Etching 기술력이 최고 KLA: 웨이퍼, 마스크 위 나노미터 단위 오차 찾아내는 장비 제조사 Tokyo Electron도 장비 제조사다. 2010년대 초, 당시 최신 마이크로프로세서는 개당 10억개의 트랜지스터를 갖고 있었다. 이런 칩을 설계하려면 설계용 소프트웨어가 필요하고, 그런 소프트웨어를 만들 수 있는 곳은 Cadence, Synopsys, Mentor 3곳이었다. 모두 미국회사다. 2000년대 들어, 많이들 반도체 산업을 3개 영역으로 나누게 되었다. Logic: 스마트폰, 컴퓨터, 서버를 운영하는 프로세서 Memory: DRAM, 플래시메모리 등 Analog: 시각,음성신호를 디지털 데이터로 치환, 네트워크에 접속하고 통신할 수 있게 해주는 통신 칩, 장비 제어 등 여기서, 3번째 영역인 아날로그 영역은 무어의 법칙과 무관하다. 즉, 매년 성능이 exponential하게 증가하지 않는다. 요즘 Analog 칩들은 3/4가 180nm 이상 공정에서 생산된다. 180nm 공정은 1990년대 후반에 나온 공정인데도 말이다. 그래서 analog칩 분야의 경제 논리는 logic칩, 메모리칩 분야와 다르다. 무조건 트랜지스터 크기를 줄여야 하는 분야가 아닌 것이다. 그래서 analog칩 만들거면 트랜지스터 크기 줄이려고 미친듯이 경쟁할 필요가 없고, 따라서 비싼 팹을 쓸 필요도 없다. 그렇기에 이익률이 높은 분야다. 대신, 설계 잘하는게 그만큼 중요한 분야다. 지금 아날로그 칩 업체중 가장 큰 곳은 TI고, 그 밖에도 Onsemi, Skyworks, Analog Device같은 미국 회사들이 있다. 1990년대 말, 일본 DRAM업체들은 Elpida라는 이름으로 하나의 회사가 되었다. 일본 경제도 안좋았고, 마이크론, 삼성전자, 하이닉스에 맞서기 위해서였다. 하지만 Elpida는 2013년에 마이크론에게 인수되고 말았다. 이렇게 되어, 마이크론은 미국 뿐 아니라 일본, 대만, 싱가포르에도 팹을 보유하게 됐다. 결국, 이 세상 모든 DRAM은 동아시아에서 만들어진다. 삼성잔자는 낸드플래시 메모리의 35%를 만든다. 나머지를 일본의 Kioxia, 미국의 마이크론과 웨스턴디지털이 나눠갖는다. 마이크론과 웨스턴디지털도 생산설비가 일본, 싱가포르에 있어서, 결국 낸드플래시 메모리도 다 동아시아에서 만들어진다. 오늘날, 최신 팹을 갖추려면 200억달러 이상이 든다. 그래서 요즘은 다들 팹리스 회사를 만든다! 팹리스 창업은 수백만 달러면 된다. 엔비디아는 1993년에 만들어졌다. 엔비디아는 그래픽 연산을 위한 프로세서, GPU들을 만들었다. 그러다가 2006년 CUDA를 내놨다. CUDA를 쓰면 GPU를 그래픽이랑 상관 없는 분야에서도 쓸 수 있다. 즉, 시장이 넓어진다! 그래서 엔비디아가 CUDA를 열심히 관리한다. 병렬처리가 필요한 곳이면 어디서든 GPU를 쓸 수 있다. 퀄컴은 1985년에 세워졌다. Quality + Communication = Qualcomm 퀄컴은 CDMA가 성공한 후, 통신 칩 뿐 아니라 아예 AP(Application Processor)를 통째로 만들기 시작했다. CDMA는 연산이 많이 필요한데, 그걸 가능하게 하는 칩을 만든 것이다. FPGA(Field-Programmable Gate Array)하는 회사는 Altera, Xilinx가 있다. AMD도 원래 팹이 있었지만, 아부다비 정부에 제조 사업부를 매각했다. 그게 지금의 Global Foundries다. 2000년대 주안부터는 SiO2 두께가 원자 2개정도가 되어 tunneling이 심해졌다. 그리고 채널 폭도 너무 좁아져서, 전류가 새어나가는 일이 많아졌다. 그래서 FinFET이 나왔다. FinFET은 전기장을 위에서만 거는게 아니라 옆으로도 걸어서, 전자를 더 잘 제어할 수 있고 전류 누설도 막을 수 있다. 물론, 이런 나노미터 단위 3차원 구조는 엄청난 공정 기술이 필요하다. 2008년 금융 위기때, 제품이 안팔리니 반도체 업계도 힘들었다. TSMC는 이때, 모리스 창의 주도 하에 모바일 시장 내다보고 투자 엄청나게 헀다. 1세대 아이폰: 운영체제는 애플이 만든 iOS였지만, 칩 설계와 생산은 삼성전자에서 많이 했다. 그 외에도 인텔의 메모리 칩, Wolfson의 오디오 프로세서, 독일의 Infinion의 무선 네트워크 접속용 모뎀 칩, CSR의 블루투스 칩, Skyworks의 신호 증폭기 등이 들어갔다. 아이폰 출시 1년 후, 애플은 PA semi라는 에너지 효율 좋은 프로세서를 설계하는 스타트업을 인수하고, 반도체 설계자들을 고용했다. 2년 후, 애플은 A4라는 자체 AP를 만들었다고 발표했다. 아이폰4에 이 A4가 들어갔는데, 아마 이게 짐 켈러 업적일거다. 애플은 메인 프로세서 뿐 아니라 에어팟같은 주변기기의 보조 칩도 스스로 만든다. 스마트폰 AP는 설계하는 데에 비용이 많이 든다 그래서 중저가 스마트폰 만드는 회사들은 그냥 퀄컴에서 칩 사온다. 아이폰에 의해 노키아, 블랙베리같은 회사들은 다 죽어버렸다. Foxconn, Wistron같은 대만 기업들은 중국에서 애플 제품 조립 설비를 운영한다. EUV 애기가 갑자기 다시 나오는데, 옛날에는 리소그래피에 그냥 가시광선을 이용했다. 근데 가시광선이면 파장이 수백나노미터 정도라서, 회로가 작아지자 한계에 봉착했다. 그래서, 가시광선보다 파장이 짧은 248nm, 193nm 자외선들을 리소그래피에 쓰기 시작했다. 이렇게 하면 가시광선보다 더 정교한 그림을 그릴 수 있었지만, 여기에도 한계가 있었다. 그래서, 회사들은 파장 13.5nm EUV(극자외선)을 연구하기 시작했다. DUV(심자외선)을 쓸 때는 자외선을 물에 투과시키거나 여러 겹의 마스크에 통과시킴으로써 193nm DUV로 193nm보다 좁은 폭의 패턴을 그렸다. 이 짓거리가 2010년대 중반 3nm FinFET 공정까지 이어졌다! 하지만 그 이상은 이젠 너무 어려웠다. TSMC, 삼성전자, 인텔, 글로벌파운드리즈 모두 EUV 도입을 원했다. 글로벌파운드리즈는 2010년 싱가포르의 파운드리업체인 차터드반도체를 인수했고, 2014년에는 IBM 반도체사업부를 인수했다. IBM이 소프트웨어에만 집중하기로 하면서 팔아버렸거든. 하지만 EUV개발에는 돈이 너무 많이 들어서, 글로벌파운드리즈는 결국 EUV 경쟁을 포기했다. 이제 삼성전자, TSMC, 인텔밖에 남지 않았다. 2010년대 실리콘밸리에서, 반도체설계와 제조를 모두 하는 회사는 인텔 뿐이었다. 하지만 둘다 어중간했다. 2010년대 들어, 데이터센터용 프로세서와 서버용 프로세서가 많이 필요해지면서 인텔 칩이 많이 팔렸다. 아마존 웹서비스, 마이크로소프트 azure, 구글 클라우드 등 근데, 그 2010년대쯤부터 연산력에 대한 수요가 변하기 시작했다. AI 때문이다. CPU는 서로 다른 유형의 계산을 수행할 수 있기 때문에 범용성을 가지지만, 그 모든 계산을 순차적으로, 한번에 하나씩 해야 한다. 하지만 AI는 매번 다른 데이터를 받아서 매번 같은 연산을 하는 것으로 학습된다. 그래서 범용성같은거 필요 없고, 그냥 노가다 잘하는 칩이 있으면 된다. GPU는 병렬로 연산을 진행하기 때문에 많은 계산을 동시에 처리할 수 있다. 그래서 AI에 GPU가 많이 쓰이게 되었고, 이게 2010년대 초다. 엔비디아는 인공지능에 회사의 미래를 걸고, CUDA를 개선하기 위해 엄청나게 노력했다. 그리고 인공지능 뿐 아니라 데이터센터에서도 GPU를 많이 필요로 하게 되어, 엔비디아가 떡상했다. 그렇다고 엔비디아 미래가 무조건 밝은건 아니다. 클라우드 회사들(아마존, 마이크로소프트, 구글, 텐센트, 알리바바)이 그들 각자의 수요에 맞는 칩을 설계하기 시작했기 때문이다. 예를 들어, 구글은 구글의 Tensorflow 라이브러리에 최적화된 TPU(Tensor Processing Unit)를 구글 데이터센터에 쓰고 있다. 하여간, GPU가 훨씬 중요해졌기 때문에 인텔의 데이터센터용 프로세서도 이제 많이 안팔리게 됐다. 결국, 인텔은 반도체 설계한것도 잘 안팔리고, 파운드리는 TSMC, 삼성전자랑 기술 격차가 말도 안되는 수준이다. 그냥 회사가 좀 망했다. 2020년 기준, 지구상의 EUV장비 중 절반은 TSMC에 있다. 2020년대, 최첨단 프로세서를 제조할 수 있는 회사는 TSMC와 삼성전자 뿐이다. 근데 둘다 동아시아에 있고, 중국이랑 가까워서 미국이 걱정이 많다. 중국은 구글, 페이스북을 차단하고 바이두, 텐센트로 그 자리를 대체하고 있다. 애플, 마이크로소프트는 중국 검열에 협조하겠다고 한 뒤 중국 시장에 들어갈 수 있었다. 2000년대, 2010년대에 중국이 가장 많은 돈을 쓴 수입제품은 석유가 아니라 반도체였다. 시진핑의 ‘중국제조2025’라는 계획이 있다. 2015년 85%였던 반도체 수입 비중을 2025년에는 30%까지 줄이자는거다. 근데 첨단 팹의 가격은 2014년에 이미 100억달러정도 했다. 그래서 달성하려면 돈이 엄청 들거다. 2017년, 중국은 2600억달러어치 반도체를 수입했다. 이건 사우디아라비아의 석유 수출이나 독일의 자동차 수출보다 훨씬 많은 금액이다. 2017년 기준으로, 집적회로는 한국 수출 총액의 15%, 싱가포르 수출 총액의 17%, 말레이시아 수출 총액의 19%, 필리핀 수출 총액의 21%, 대만 수출 총액의 36%를 차지한다. 중국은 이렇게나 큰 시장을 내수화 하려고 하는거다. 이 전자제푸 공금망은 지난 50년간 아시아 경제 성장을 떠받쳐왔다. IBM은 미국 정부에 소프트웨어를 많이 팔아왔는데, 에드워드 스노든이란 사람이 모스크바로 망명하며 미국의 첩보작전 관련 문서들을 다 공개해버렸다. 미국 정부의 도청 정황이 드러난 문서들이었는데 그래서 중국 회사들이 IBM 제품들을 구매하지 않게 됐다. 그래서 IBM은 중국 정부를 찾아가서 반도체 기술좀 줄테니까 우리 제품좀 사달라고 했다. 이게 IBM만 그런게 아니고, 퀄컴, AMD도 기술 협약, 기술 이전을 체결하면서라도 중국 시장에 들어가려고 했다. 이런 식으로, 중국은 반도체 제조 기술을 쌓아가고 있다. 칭화유니그룹은 칭화대가 설립한 회사로, 표방하는건 대학의 연구를 사업으로 전환하는 것이지만, 실제로는 중국 정부가 시키는 대로 부동산, 반도체에 돈 뿌리는 회사다. 얘네는 해외 반도체 회사들 인수하려고 많은 노력을 했다. 이병철이 건어물 회사였던 삼성을 키운 방법은: 1. 정부와 좋은 관게를 유지하려고 노력했다. 2. 서양, 일본의 제품 몇개를 골라 그것을 같은 품질에 낮은 가격으로 만드는걸 목표로 했다. 3. 세계화에 적극적이었다. 중국에서는 화웨이가 텐센트, 알리바바와 달리 초기부터 외국과의 경쟁을 받아들였다. 창업자가 런정페이인데, 런정페이도 이병철처럼 가성비 좋은 버전 만들어서 파는데에 주력했다. 런정페이는 충칭대 출신이고, 1987년에 화웨이 창업했다. 선전은 외국 투자가 장려되는 곳이라, 런정페이는 선전으로 가서 전화교환기 사업을 시작했다. 그리고 이 기술은 이후 데이터 송수신에 게속 응용됐으며, 현재 무선기지국 분야 3대 사업자는 핀란드의 노키아, 스웨덴의 에릭슨, 중국의 화웨이다. 화웨이는 매년 150억달러 정도를 연구개발에 쓴다. 이 정도를 연구개발에 쓰는 회사는 구글, 아마존, 폭스바겐 정도다. 이게 가능한 이유는 중국 정부 지원을 받아서도 있다. 화웨이는 인프라 제공을 넘어 스마트폰 시장까지 들어갔다. 그리고 자사 스마트폰용 반도체 개발도 한다! 2010년대 말, 화웨이의 HiSilicon 사업부는 TSMC의 두번째로 큰 고객이 됐다. 가장 큰 고객은 애플이다. 어쨌든 화웨이는 무려 스마트폰 프로세서까지 설계하는 회사가 된거다. 이건 엄청난 기술력이다. 2G폰은 사진을 주고받을 수 있었고, 3G폰은 웹사이트를 열 수 있었으며, 4G폰은 비디오 스트리밍이 가능했다. 주파수, 그니까 spectrum은 Silicon보다 훨씬 비싸다. 그래서 퀄컴은 송수신되는 데이터를 최적화할 방법을 찾아내고, 아날로그디바이스같은 회사들은 더 정교하게, 더 효율적으로 무선 신호를 주고받을 수 있ㄴ느 RF transceiver들을 만들었다. Beamforming: 기지국이 전화기의 위치를 파악한 후, 그쪽으로 전파를 보낸다. 테슬라도 반도체 설계 분야의 주요 회사중 하나다. 자체 설계한 반도체를 많이 쓴다. AI에 중요한건 데이터, 알고리즘, 연산력이다. 이 중 중국이 가지지 못한건 연산력이다. 연산력에 쓰이는 하드웨어 다 미국이 설계하고 대만이 만들기 때문이다. 아날로그 칩, RF칩은 비싼 fab을 쓸 필요는 없다. ZTE: 중국 국영 통신 인프라 업체. 화웨이는 민간 업체다. 트럼프 취임때쯤, 중국은 미국의 최대 고객이자 최대 경쟁자였다. DRAM 시장은 규모의 경제를 필요로 한다. 그래서 삼성전자, 하이닉스, 마이크론같은 대기업들밖에 안남았다. 현재 반도체 장비 제작사들은 미국(Applied Materials, Lam Research, KLA), 일본, 네덜란드(ASML)이 전부다. 그래서 얘네한테 무역제재당하면 반도체 못만든다. 푸젠진화반도체(JHICC)가 마이크론 DRAM 기밀 훔쳐가자, 미국이 일본까지 끌어들여 함께 무역제재를 걸어버렸다. 결국 JHICC는 DRAM 생산을 못하게 됐다. 미국은 동맹국들에게 5G네트워크에서 화웨이를 손절하라고 말햇다. 호주, 폴란드, 프랑스 등은 미국 말대로 완전 손절했는데, 독일은 자동차 시장때문에 눈치를 봤고 영국은 아예 손절을 안했다. 2020년, 미국은 화웨이를 패기로 했다. 일단 미국산 칩을 화웨이에 수출 못하게 헀다. 하지만 화웨이는 칩 설계 역량이 있엇기 때문에, TSMC만 안막히면 큰 문제 없었다. 그래서 2020년 5월에는 모든 미국산 반도체 관련 제품 수출을 막았다. 미국 장비가 없으면 TSMC라고 해도 칩을 만들어줄 수가 없다! 이렇게 되자 영국도 화웨이를 손절했다. 그렇다고 중국 회사들이 다 망한것은 아니다. 텐센트, 알리바바는 살아있고, SMIC도 첨당 장비는 못사도 공장 계속 굴리고 있고, 화웨이도 첨단 반도체만 못사는거라 4G 네트워크 사업은 게속 하고 있다. 우한에 YMTC라는 낸드플래시 메모리업체가 있는데, 여기는 코로나 락다운때도 안닫혔다. 정부에서 아예 특별 열차편을 만들어서 회사 오갈 수 있게 해줬다고 한다. 요즘 ASML에서 만들려는 것은 high-NA EUV다. NA = Numerical Aperture. 2020년 중반 출시 예정이고, 대당 3억달러쯤 할 예정이다. RISC-V는 x86이나 ARM과 달리 open source architecture다. RISC-V 재단은 지정학적 중립성을 위해 미국에서 스위스로 위치를 옮겼다. 알리바바 등 회사들은 RISC-V에 기바한 프로세서들을 설계하고 있다. SMIC 전략: 스마트폰, 데이터센터는 첨단 칩이 필요하지만, 자동차는 그렇지 않다. 그래서, 구세대칩을 찍어내서 첨단기술이 필요하지 않은 전기차에 많이 들어가게 한다. 중국은 SiC, GaN(질화갈륨) 등 반도체 물질에도 투자한다. 전기차 출력 관리용 반도체에 많이 쓰일 것으로 예상돼서. 중국은 전기차 많이 하는 나라다. 2020년, 코로나 때문에 자동차 회사들이 반도체 주문을 많이 줄였다. 그러는 동안 사람들은 컴퓨터를 사서 PC, 데이터센터용 칩이 많이 필렸다. 근데 자동차 수요가 생각보다 빨리 회복돼서, 자동차 회사들이 뒤늦게 반도체를 주문하긴 했지만 주문이 밀려서 고생했다.]]></summary></entry><entry><title type="html">반도체제국 내용</title><link href="http://localhost:4000/history/2023/10/04/History-%EB%B0%98%EB%8F%84%EC%B2%B4%EC%A0%9C%EA%B5%AD-%EB%82%B4%EC%9A%A9.html" rel="alternate" type="text/html" title="반도체제국 내용" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/history/2023/10/04/History%20-%20%EB%B0%98%EB%8F%84%EC%B2%B4%EC%A0%9C%EA%B5%AD%20%EB%82%B4%EC%9A%A9</id><content type="html" xml:base="http://localhost:4000/history/2023/10/04/History-%EB%B0%98%EB%8F%84%EC%B2%B4%EC%A0%9C%EA%B5%AD-%EB%82%B4%EC%9A%A9.html"><![CDATA[<p>HiSilicon: 화웨이의 칩 설계 회사<br />
<br />
메모리 회사: 어느 정도 용량의 메모리가 가장 수요가 많을지, 어느 정도 전력 소비까지 고객이 감내할 수 있는지 확인<br />
<br />
인텔같은 로직 회사: 고객이 어느 정도의 CPU 성능을 요구할지, 노트북 고객들은 어느 정도의 배터리 수명을 요구하고 CPU로부터 어느 정도의 전력 및 발열까지 감내할 수 있는지를 조사한다.<br />
<br />
D램은 사용자로부터 명령과 주소를 받는 부분, 주소를 해독하는 부분, 데이터 저장소, 읽어온 데이터를 잠시 보관해두는 latch로 비교적 간단하게 구성된다.<br />
<br />
D램은 설계 기술이 상대적으로 적게 필요하고, 대신 면적의 대부분을 차지하는 데이터 저장소의 면적을 줄이는게 중요하다.<br />
<br />
현대 CPU들은 다층의 캐시메모리, 디코더, ROB, 대기소, 연산포트, ALU등 다양한 기능을 하는 많고 작은 하드웨어들로 연결되어 있다.<br />
<br />
chip의 설계를 도와주는 EDA, 설계를 제조기술에 맞춰주는 PDK<br />
<br />
하드웨어 버그 예시로는 인텔 CPU의 보안 결함?<br />
<br />
테스트 후에는 원래 패키징해서 팔지만, 공간이 중요해지며 아예 웨이퍼 단위로 사고팔기도 한다.<br />
<br />
요즘은 여러개 CPU나 CPU+GPU 등의 구조로 Heterogeneous 칩을 고속 인터커넥트로 연결해 한 패키지로 팔기도 한다.<br />
<br />
Dhrystone: 1984년에 제안된 컴퓨터 성능 측정의 지표<br />
<br />
인텔 11900K는 1초에 4,110억개 명령어를 처리한다(Dhrystone 기준)<br />
<br />
컴퓨터: CPU, 메모리, 보조기억장치(HDD, SSD, USB메모리 등)<br />
<br />
반도체 제조업이 다른 제조업드로가 구분되는 가장 큰 차이점은 기술력이 미치는 영향이 막대하다는 점이다.<br />
<br />
1980년 1MB 메모리 가격: 6480달러<br />
2015년 1MB 메모리 가격: 0.0042달러 (백만배정도 싸졌다)<br />
다른 제조업들은 이정도로 못줄인다<br />
<br />
Dennard Scaling: 같은 면적에 집적된 트랜지스터는 전력 소모량이 같다.<br />
그래서 기술이 앞서는 회사는 전력소모 특성도 앞서나간다. 같은 면적 안에 트랜지스터가 100개든 100만개든 전력 소모는 같으니까.<br />
<br />
그리고 반도체는 다른 제조업에 비해 부피가 작고 부가가치가 높은 편이라서, 한 회사가 만들어 전 세계로 수출할 수 있다.<br />
그래서 빵집마냥 다른 동네로 도망가서 장사하는게 안된다. 전세계랑 경쟁해야된다.<br />
<br />
제조원가에서 설비 투자 비용이 차지하는 비중이 압도적으로 크다는 점도 다른 제조업들과 다르다.<br />
<br />
그래서 매년 엄청난 돈을 장비에 써야 한다. 즉, 고정비용이 크다.<br />
고정비용이 크다는건, 그 해 생산을 안하더라도 아낄 수 있는 비용이 얼마 안된다는 뜻이다.<br />
<br />
게다가, 반도체 공장은 공장 재가동에 시간이 오래 걸린다.<br />
<br />
일반적으로 반도체는 포토마스크 한장을 처리하는 데에 하루 이상이 걸리며,<br />
구조가 간단하다는 DRAM도 2000년에 이미 20장 넘는 마스크를 사용했다.<br />
<br />
2013년에는 DRAM 웨이퍼 한장이 필요로 하는 마스크 수가 40장을 넘었다. 그만큼 오래 걸린다.<br />
<br />
결국, 제품 가격이 떡락했다고 해도, 생산을 멈추면 손해다. 변동비용이 매출액보다 커지면 그때는 생산을 멈춰야 하겠지만, 반도체 생산에서 변동비용은 상당히 작다. 고정비용이 그만큼 커서 그렇다.<br />
<br />
여기서 말하는 변동비용은 인건비, 웨이퍼 가격, 재료 가격<br />
<br />
그럼 제품 가격이 떡락하면 회사를 팔아버리는건? 회사마다 공장에서 쓰는 장비가 많이 달라서 어렵다.<br />
삼성전자는 지금까지 단 한번도 메모리 회사를 인수한 적이 없다.<br />
<br />
그래서, 물량으로 시장 점유율을 가져왔더라도, 가격이 떡락하면 떡락을 막을 수가 없다.<br />
어떻게어떻게 버텨서 살아남으면? 바닥회사들이 죽어서 싸움이 끝난건데, 이 경우 1등 기업과 겨우 살아남은 회사의 사정은 크게 차이난다. 기술력에 의한 원가 절감 차이가 엄청나니까. 그래서 1등을 절대 못따라잡는다.<br />
<br />
그래서 메모리 반도체 시장에서는, 설비투자를 해버렸으면 연구개발과 재무운영 외에는 해볼 수 있는게 없다.<br />
<br />
인텔 4004: 최초의 마이크로프로세서. ALU, 레지스터 등을 모두 웨이퍼조각 하나 위에 올려놨다.<br />
4bit연산까지밖에 안됐지만, 이거 이후로 컴퓨터 가격이 떨어지기 시작했다.<br />
<br />
1975: IBM 5100(1.9MHz CPU, 64kB 메모리)<br />
1987: IBM XT(4.77MHz CPU, 640kB 메모리)<br />
<br />
1983년 삼성전자의 도쿄 선언: 메모리 사업 시작하겠다<br />
<br />
일본 메모리는 품질 위주, 삼성전자는 원가 위주 -&gt; 딱 시장에 필요한 성능만 내고 나머지는 원가절감 하겠다.<br />
일본 메모리는 품질을 위해 제조공정이 길고, 장비 종류도 많았다.<br />
<br />
1989년, DRAM 용량이 4MBit를 넘어가자 DRAM의 형태가 문제되기 시작했다.<br />
<br />
지금까지는 평면에 트랜지스터와 DRAM을 함께 늘어놓는 식이었는데, 밀도를 높이려면 다른 방식을 써야 했다.<br />
<br />
Trench형과 Stack형을 두고 논쟁이 많았는데, 진대제, 권오현이 Stack이 낫다고 해서 이건희가 Stack으로 가자고 했다.<br />
<br />
이때 IBM, 도시바, NEC 등이 다 트렌치 하는동안 삼성전자만 Stack을 했고, 옳은 결정이었기에 4MBit Dram이 대박을 친다.<br />
*Stack방식의 장점 적어놓기. 불량분석 이득이었던듯?<br />
<br />
반도체 산업이 처음 시작됐을때는 100mm 웨이퍼를 썼다. 1980년대 들어서는 150mm, 90년대 들어서는 200mm가 주류가 됐다.<br />
웨이퍼가 커질수록 버려지는 부분도 줄어드니까, 면적비 이상의 이득이 난다.<br />
100mm 웨이퍼와 300mm웨이퍼는 웨이퍼당 칩 갯수가 10배 넘게 차이난다.<br />
<br />
유명한 빅 칩중 하나인 IBM의 POWER9는 25x27mm다.<br />
<br />
2001년, 웨이퍼 업체들이 300mm 양산기술을 확보했다. 메모리업체들 입장에서는 당연히 바꾸는게 장기적으로 이득이지만, 불황 시기에 엄청 비싼 장비들을 사와야 한다는게 부담으로 다가왔다. 공정 최적화도 또 해야 하는데.<br />
<br />
이 불황은 2000년 10월의 닷컴버블 붕괴 때문이다. 버블 고점에서는 64MBit DRAM이 20달러였는데, 2001년 2월에에는 3.8달러가 됐다.<br />
하여간, 회사들은 돈이 없어 300mm를 바로 도입 못했다. 안그래도 불황인데 여기서 또 문제생기면 그땐 진짜 망할 수도 있으니까.<br />
<br />
근데 삼성전자는 빠꾸 안치고 2001년 10월에 300mm 웨이퍼 공정을 도입했다. 그래서 2001년에는 메모리 업체들 중 삼성전자만 흑자였다는데, 10월에 도입했는데??<br />
<br />
DRAM Cell은 원래 8F^2 구조였다가 6F^2 구조로 개선됐다. 6F^2구조가 밀도가 더 높다.<br />
마이크론이 2006년 처음 했고, 바로 삼성전자도 시작했다.<br />
<br />
일본 기업들은 1등을 뺏겼지만, 삼성전자는 6F^2 cell, 300mm공장등을 해내며 계속 1등을 이어갔다.<br />
<br />
결국 원가를 깎아서 삼성전자가 이긴거다. 비싸고 오래가는 메모리는 필요 없는 시대가 됐다.<br />
삼성전자가 기술이 훌륭했다기보다는 시대 흐름을 잘 탄거다.<br />
<br />
컴퓨터의 구성 요소: CPU, 메모리, 보조기억장치(HDD, SSD)<br />
보조기억장치는 초기 컴퓨터 이론에 존재하지 않았다. 폰 노이만 구조에는 메모리, CPU만 존재한다.<br />
(그림)<br />
뭐 전원 꺼지면 메모리가 날아간다 라고 폰 노이만 구조에 적혀 있지는 않은데,<br />
실제로는 DRAM, SRAM으로 메모리를 만들게 되어 전원이 꺼지면 메모리가 날아가게 되었다. 이건 좀 문제가 됐다.<br />
<br />
전원이 꺼져도 데이터를 보존하는 기억장치가 필요해졌다.<br />
<br />
이때, CPU가 메모리 이외 공간에 데이터를 보낸다는건 그 데이터는 당분간 필요 없다는 뜻일거다.<br />
따라서, 데이터 보존 장치의 성능은 좀 떨어져도 될 것이다.<br />
<br />
DRAM은 CPU가 직접 접근해야 하기 때문에, 메모리의 모든 방이 CPU에서 바로 접근 가능해야 했다.<br />
이는 메모리 설계에서 큰 부담으로 작용했다.<br />
모든 데이터 방에 금속을 설치해서 연결해야 하기 때문이다.<br />
<br />
결국 큰 단위로 데이터를 저장하고, 데이터가 필요할 때는 그 단위를 통째로 불러와 필요한 것만 찾아 쓰는 방식이 되었다. 이게 더 원가가 낮았다.<br />
<br />
처음에는 메모리로 자기 테이프를 썼는데, 원하는 데이터가 테이프 시작과 끝에 흩어져 있으면 테이프를 여러번 감았다 풀었다 하며 읽어야 했다.<br />
이 경우 성능이 안좋았다.<br />
<br />
1956년, IBM이 하드디스크를 내놓는다.<br />
하드디스크는 구조상 최고성능-최악성능 차이가 적다. 테이프는 엄청 크다. 그림으로 설명 가능<br />
즉, 카세트테이프는 순차접근만 가능, 하드디스크는 무작위 접근이 가능하다.<br />
<br />
HDD는 자기 테이프보다 비쌌지만, 성능도 좋고 관리도 쉬웠다.<br />
세계 HDD 출하량: 1996년 1억대, 2013년 8억대<br />
<br />
근데, CPU 성능은 2001년(펜티엄 1.2GHz) ~ 2012년(샌디브리지 3.3GHz)동안 24배 증가(코어성능 6배, 코어개수 4배)했고,<br />
메모리 가격 대비 용량은 128배, 전송속도는 12배, 접근속도는 4배(2001년 PC-133, 2012년 DDR3-1600) 증가했다.<br />
<br />
CPU와 메모리의 발전은 이렇게 빨랐는데, HDD의 발전은 느렸다.<br />
웨스턴디지털의 HDD 성능, 용량 변화:<br />
2001(WB100EB): 전송속도 40, 반응속도 12.1, 용량 10GB<br />
2012(WD10EZEX): 전송속도 150, 반응속도 8.9, 용량 1TB *단위 확인 필요<br />
<br />
하드디스크를 보면 용량은 100배 늘었지만 최대 전송속도가 3.75배밖에 안올랐고,<br />
반응속도는 그냥 모터를 5400RPM에서 7200RPM으로 높인게 다다.<br />
<br />
그리고, 데이터 안정성 때문에 3.5인치 이상 디스크에서는 회전속도를 7200RPM 이상으로 올리는게 힘들었다.<br />
<br />
게다가 컴퓨팅 기술이 발전해서 각종 프로그램, OS가 발전해 파일들의 크기가 커졌고,<br />
그러면 사용자가 프로그램을 실행하면 하드디스크 여러곳을 탐색하며 다음 파일을 읽어야 하는데, 여기에서 큰 성능 하락이 발생했다.<br />
<br />
1995년에는 MS-DOS를 썼고, 2010년에는 윈도우7을 썼다.<br />
1995년에는 MS-DOS를 용량 1.44MB짜리 3.5인치 플로피디스크에 담아 썼는데(MS-DOS의 용량이 몇인지는 모르겠다)<br />
2010년 윈도우7은 용량이 2GB가 넘는다.<br />
<br />
DOS때는 부팅시 3~10개 파일만 로드하면 됐었는데, 윈도우 부팅에는 수천개 파일이 필요하다.<br />
DOS 시절 게임 용량은 MB단위였지만, 2000년대 중반 게임들은 로딩 시간이 1분은 필요하게 되었다.<br />
결국 HDD가 데이터 입출력 병목을 만드는 상황이 되었다.<br />
<br />
그리고, HDD는 기계장치 때문에 소형화하면 성능 저하가 크게 일어났고, 저전력 상태로 만드는 데에도 오래 걸려서 저전력 상태를 유지하기 어려웠다.<br />
결국 HDD는 저전력과 고성능 양쪽에 방해됐다.<br />
<br />
플래시메모리는 1980년 도시바에서 마스오카 후지오 박사가 만들었다.<br />
DRAM, SRAM은 전하를 매우 작은 도체로 구성된 공간에 집어넣어 1과 0을 구분했다.<br />
플래시메모리는 더 출입이 어려운 절연 구역에 고전압을 가해 전자를 터널링시켜 가두는 것으로 1과 0을 구분했다.<br />
<br />
당연히, 플래시메모리는 SRAM, DRAM보다 읽기쓰기 속도가 훨씬 느렸다. SRAM, DRAM은 도체랑 바로 붙어있으니까.<br />
<br />
DRAM은 수십나노초면 읽기쓰기가 가능했지만, 플래시메모리는 전압펌프를 가동해서 절연 공간에 전하를 집어넣을 때까지 수십~수천us가 필요했다.<br />
대신, 전자가 갇혀있게 되어 전원을 꺼도 메모리가 유지됐다.<br />
<br />
플래시메모리는 Block 단위로 데이터를 읽고 쓴다. Block 단위로 데이터가 뿅 하고 사라지기 때문에 ‘플래시’메모리라 부른다.<br />
<br />
근데 플래시메모리는 공정상의 한계때문에, 칩 내의 block 중 1% 이상이 결함을 갖고 나온다.<br />
그래서 에러 정정 장치를 붙여서 사용해야 했다.<br />
플래시메모리 제조사는 ‘0번 block은 결함이 없을 것’과 ‘칩 내 불량이 몇개 이하일 것’만 보장해 판매한다.<br />
<br />
플래시메모리는 컨트롤러와 결합하여 운영되며, 컨트롤러 안에 작은 소프트웨어를 탑재해 문제들을 해결한다.<br />
값싼 원가의 대가로 얻은 낮은 질을 소프트웨어로 해결하려는거다.<br />
<br />
NAND, NOR가 있는데 NOR는 Block이 작다. 수 바이트정도 된다.<br />
NAND는 Block이 수kB 수준이다.<br />
그래서 NOR는 각 방에 연결되는 도체를 더 많이 깔아야 하고, 그래서 더 비싸다.<br />
결국 반응속도는 NOR가 더 빠르고, 가격은 NAND가 더 싸다.<br />
쓰기는 NAND가 한번에 뭉텅이로 써서 더 빠르다. 읽기가 NOR가 더 빠른거다.<br />
<br />
후지오 마스오카 박사는 플래시메모리로 마그네틱 기반 저장소를 모두 대체하고 싶어했으나, 도시바는 그 아이디어에 관심이 없었다.<br />
<br />
인텔이 더 먼저 행동했다. 인텔은 CPU 사업을 하고 있었는데, CPU가 구동하기 위해서는 일련의 구동 코드들 및 BIOS를 마더보드에 저장해둘 외부 공간이 필요했다.<br />
그때까지의 컴퓨터들은 이 코드들을 PROM, EPROM, EEPROM에 저장해 왔다.<br />
<br />
하지만 PROM은 물리적으로 퓨즈를 끊어 0과 1을 구분하는 방식이었기 때문에, 실수하면 칩을 못쓰게 될 위험이 존재했다.<br />
EPROM은 데이터를 적기는 쉬운데, 지우려면 자외선을 조사하는 과정이 필요했다.<br />
EEPROM은 사용하기는 쉬웠지만 용량이 너무 작았다.<br />
<br />
플래시메모리는 이런 단점들이 없단느걸 인텔이 알아챘다.<br />
<br />
이 코드들은 컴퓨터 부팅시에만 사용되며, 프로그램 성능에는 영향을 주지 않ㄴ느다. 용량은 수십kB ~ 수십MB만 있으면 된다.<br />
데이터를 바꾸는 일은 BIOS 업데이트를 할 때 정도고, 사용자 환경과 분리되어 운영되기 때문에 데이터 접근 성능에 대한 걱정도 필요없다.<br />
물론 매년 필요한 용량이 증가하긴 했지만 얼마 안됐다. 결국, 그냥 간단하게 사용 가능하고 비휘발성이면 되는거였다.<br />
여기에 NOR 플래시메모리가 쓰이게 된다. 읽는게 빠르니까.<br />
<br />
NAND 플래시의 메모리 특성이 더 안좋았기 때문에, 컨트롤러는 NAND 플래시용 컨트롤러가 더 컸다. 그래도 가격은 NAND가 더 쌌다.<br />
<br />
인텔, 암드는 NOR 플래시메모리에 집중했다. 마더보드 위의 ROM을 대체하기 위함이었으니까.<br />
<br />
도시바는 후발주자가 되어버렸는데, 1991년 낸드플래시 개발을 완료하고 시장 진출을 선언했다.<br />
그리고 1992년, 삼성전자에게 낸드플래시 기술을 라이센싱해주게 된다.<br />
<br />
그 후, 삼성전자는 미래 디지털 제품 기술 확보를 위해 DRAM, NAND, CDMA에 역량을 쏟아붓겠다고 선언한다.<br />
지금 보면 꽤 성공했다.<br />
<br />
Sandisk는 메모리를 직접 제조하지는 않고, 플래시메모리에 들어갈 컨트롤러만 만들었다.<br />
당시 이름은 Sundisk였는데, 나중에 Sandisk로 바꾼거다.<br />
<br />
핸드폰도 발전하기 시작해서, 저장장치가 필요해졌다.<br />
플로피디스크는 성능 향상이 힘들었고, 신뢰하기 힘든 매체가 사용됐다.<br />
CD-ROM은 한번 적은 내용을 고치기 어려웠다.<br />
휴대용 HDD는 크기가 너무 컸고, 충격으로 데이터를 잃어버릴 수도 있었다.<br />
<br />
여기에도 플래시메모리가 쓰이게 된다.<br />
<br />
도시바는 SD(Secure Digital)라는 새로운 규격을 만들었고,<br />
이스라엘의 M-systems는 최초의 휴대용 USB를 만들었다. 자신들의 컨트롤러에 샌디스크의 낸드를 붙였다고 한다.<br />
근데 아까 샌디스크는 컨트롤러만 만들었다며? 뭐지<br />
<br />
당시 USB 용량은 8~16MB라 CD-ROM보다는 용량이 훨씬 작았지만, 휴대성과 안정성은 훨씬 좋았다.<br />
<br />
삼성전자는 DRAM 시장에서 승리한 후, 낸드플래시 비트당 가격이 DRAM보다 높은걸 보고 낸드플래시에 대규모 투자를 하게 된다.<br />
낸드플래시가 DRAM보다 싸지게 됐고, 가격경쟁이 심해지기 시작했다.<br />
<br />
이때쯤, 애플은 iPod보다 발전한 iPod Nano를 만들고 싶어했다.<br />
원래 아이팟에는 소형 HDD가 있었는데, 더 소형화하기 위해 플래시메모리를 쓰게 됐다.<br />
당시 NOR 대비 NAND 플래시의 칩당 밀도가 10배정도 높아서, 애플은 NAND 플래시를 쓰게 됐다. 이때, 삼성전자의 NAND플래시를 쓰게 된다.<br />
<br />
이 아이팟 나노가 엄청난 히트를 치게 되어, CD나 HDD 기반 음악감상 장치들이 도태되고 플래시메모리 기반 제품들이 많이 나오게 된다.<br />
결국 플래시메모리가 많이 필요해졌고, 삼성전자는 엄청난 이득을 보게 된다.<br />
<br />
다른 낸드 제조사들도 이득을 봤지만, 삼성전자에게 특히 유리했던 점은 삼성전자는 도시바와 달리 DRAM 사업부, 파운드리사업부도 있었다는 점이다.<br />
아이팟 나노는 모바일 SD램과 일종의 CPU인 미디어 프로세서도 필요했는데, 삼성전자는 이걸 다 만들어줄 수 있었다.<br />
<br />
그래서 애플에게 미디어 프로세서를 공급하던 팹리스 업체인 PortalPlayer는 2006년 애플과의 계약이 종료됐음을 발표하고, 이후 엔비디아에 인수된다.<br />
그 계약들이 다 삼성전자한테 넘어간거다.<br />
<br />
물론 모든 낸드 제조사들이 이득을 본건 아니었다. 일본의 르네사스 반도체는 2010년 12월, 더이상 낸드 개발을 하지 않겠다고 선언한다.<br />
<br />
2005년, 애플 아이팟 나노 덕분에 돈이 많아진 삼성전자는 갑자기 새로운 사업을 발표한다.<br />
1.8인치, 2.5인치 SSD시장에 진출하겠다는거다.<br />
<br />
SSD 1TB, HDD 10TB여도 SSD 1TB를 고를 수 있으니까. 10TB까지 필요 없잖아?<br />
<br />
그리고 SSD는 내충격성, 성능이 높으니 충격 방지 설계 비용, CPU에 소모될 예산을 아낄 수 있으니 전체 비용이 줄고 경량화가 가능하다.<br />
<br />
그리고 HDD컨트롤러보다 SSD컨트롤러가 더 만들기 쉬웠다. 기계적인거 신경 안써도 되니까.<br />
<br />
2006년: 삼성전자가 최초의 양산형 SSD를 699달러에 발표한다. 32GB짜리였다.<br />
<br />
이후 비슷한 SSD들이 컨트롤러 업체들에서 등장한다. 플렉스터, 퓨전IO, OCZ, Silicon Motion 등 업체들이었다.<br />
이 팹리스 업체들은 낸드를 사와서 자사 컨트롤러에 붙여 팔기 시작했다.<br />
<br />
UBER: 수정 불가능한 에러가 발생할 확률. Uncorrectable Bit Error Rate<br />
<br />
삼성전자는 DRAM때는 원가경쟁으로 경쟁업체들을 말려죽였고, 낸드 시장에서는 낸드와 다른 하드웨어들을 결합한 솔루션(노트북, 아이팟나노 등)들을 제공했다. NOR 대신 NAND쪽을 선택한 것도 옳은 선택이었다.<br />
(107페이지 삼성전자 연표)<br />
<br />
ISA: Instruction Set Architecture. 인텔, AMD CPU는 인텔의 ISA인 x86, x86-64를 쓴다.<br />
ARM CPU는 ARM V8을 쓴다.<br />
<br />
exe: 실행 파일. 사용자가 직접 클릭해서 실행할 수 있는 파일<br />
dll: 동적 라이브러리. 다른 실행파일이 불러와서 사용할 수 있는 형태의 파일.<br />
<br />
Compiler별로 프로그램 성능이 많이 달라질 수 있다. How are you?를 ‘요즘 어떻게 지내니?’, ‘요즘 어때?’ 등으로 번역 가능한 것처럼<br />
<br />
Compiler, 기계어는 인간이 다루기 어렵다.<br />
<br />
1977년 발사된 보이저1,2호의 메모리는 68kB였다. 보이저의 제어 프로그램은 기계어와 포트란으로 만들어졌다. 이 정도는 인간이 기계어로 만들 수 있다.<br />
<br />
근데, 예를 들어 윈도우 7은 4기가짜리다. 이런건 인간이 기계어로 못만든다.<br />
<br />
옛날 컴퓨터에는 확장 슬롯 자체가 없거나, 공인된 하드웨어만 붙일 수 있었다.<br />
<br />
1981년 IBM이 XT라는 컴퓨터를 만들었는데, 서드파티에게 컴퓨터를 오픈하고, 보조기억장치로 HDD와 플로피디스크를 갖추고, 확장 가능한 램 슬롯을 가진 구조였다.<br />
CPU는 인텔 8088, OS는 MS-DOS였다.<br />
<br />
당시에는 컴퓨터 관련 표준이 거의 없어서, 당시 컴퓨터들은 CPU가 동일한 ISA를 사용하더라도, 아니면 아예 같은 CPU를 사용하더라도 프로그램이 제대로 안돌아가는 경우가 많았다.<br />
요즘은 어느 회사 RAM을 쓰든, 어느 회사 GPU를 쓰든 잘 작동하잖아? 그때는 공식적으로 인정된 부품만 써야 했다.<br />
<br />
그래서 IBM PC가 혁신이었다. 맘대로 하드웨어 갈아끼울 수 있다니!<br />
<br />
당시 인텔8088은 가격이 저렴하고 성능이 뛰어났다. 그리고 인텔 x86-16은 CISC 명령어 체계를 사용하고 있었는데, x86-16에는 범용 레지스터(CPU 내부 연산 처리를 위한 초고속 저장공간)가 훨씬 많아서, 코드를 잘 짜면 경쟁자들을 압도할 수 있었다.<br />
<br />
그리고 8088은 내부로는 16비트를 사용해서 성능을 끌어올렸지만, 외부로는 8비트 버스를 갖고 있었다.<br />
당시 시장에 나온 하드웨어들은 다 8Bit였기 때문에 이게 나았다.<br />
<br />
그 후 모든 회사들이 달려들어 IBM PC를 리버스엔지니어링 했다. 모두가 IBM PC에 맞는 규격의 하드웨어, 소프트웨어를 만들게 되었다.<br />
<br />
그 전에는 다 규격이 달라서 거래처가 바뀌면 싹 다 다시 해야 했다.<br />
하지만 이제는 IBM PC 규격이 표준이 되어 이거대로 만들면 되는 상황이 됐다.<br />
<br />
그래서 IBM은 이득을 봤나? 그건 아니다. IBM도 경쟁자 중 하나가 되어버리고 말았다. 그래서 별로 이득 못봤다.<br />
<br />
그래도 PC 사업은 IBM의 사업들 중 하나였기 때문에 큰 타격은 없었다. IBM은 지금도 메인프레임 시장의 절대자다.<br />
<br />
이 시점에, 인텔은 두가지 결정을 내린다.<br />
1. 일본과의 DRAM 경쟁을 포기하고 CPU에 집중한다.<br />
2. 프로세서 생산을 타사에 맡기지 않는다.<br />
<br />
인텔을 왕좌에 올린 8088 프로세서를 위탁 생산하던 회사는 AMD, NEC, 후지츠 등 10개 가까이 되었다.<br />
이렇게 아웃소싱을 돌려서 웨이퍼 공장을 작게 유지하고 있었지만, Value chain의 상당 부분을 다른 회사들과 나눠 가져야 했고, 기술이 새어나갈 위험도 있었다.<br />
<br />
인텔은 자기 제품을 타사에 맡기지 않게 되며, 전세계 PC CPU의 설계부터 제조까지 모든 Value Chain을 장악하게 됐다.<br />
<br />
인텔이 자체 생산으로 물량을 돌릴 때, AMD는 인텔의 x86 ISA만 라이센싱하고 설계는 자체적으로 하기로 했다.<br />
인텔이 CPU 시장을 먹어버리긴 했지만, CPU는 수명이 길기 때문에 인텔은 과거 제품보다 더 좋은 제품들을 계속 만들어내야 했다.<br />
게다가 AMD도 쫓아오고 있었다.<br />
<br />
그래서, 독점시장이어도 인텔은 가만히 있을 수가 없었다.<br />
<br />
데너드 스케일링: 미세공정 발전으로 면적당 트랜지스터가 늘어나도 전력 소모는 늘어나지 않는다<br />
인텔은 전력 소비량을 유지하며 더 많은 부품을 CPU에 빽빽히 꽂아넣어 성능을 늘릴 수 있었다.<br />
또, 약간 밀도를 낮춰 동작 마진을 주는 것으로, 더 높은 clock으로 동작할 수 있게 했다.<br />
<br />
인텔은 과거 CPU에서 동작하는 프로그램들은 모두 상위 CPU에서도 동작하게 만들었다.<br />
뭐 ISA를 갈아엎는다거나 이런 행동을 하지 않은거다.<br />
<br />
컴퓨터가 발전하고 메모리 용량이 커지자, 인텔은 x86-16에서 x86-32로 넘어갔다. x86-16은 64kB 이상 메모리를 인식할 수 없기 때문이다.<br />
인텔 80386에서 이 변화가 발생했는데, 옛날 프로그램들 잘 돌아가도록 ‘Virtual 8086 mode’를 만들어놨다.<br />
지금도 가상환경에서는 8086 명령어 다 돌아간다.<br />
8086 프로세서는 트랜지스터 3만개인데, 요즘 프로세서들은 1억개 이상이다. 사실 아예 프로세서를 집어넣을 수도 있는거다.<br />
<br />
이런 호환성 유지는 쉬운 일이 아니다. 요즘 잘 안쓰는 명령어도 들고 가야 하니 웨이퍼 면적, 전력 낭비가 어느 정도씩 발생하게 된다.<br />
인텔은 이걸 감내하면서도 성능을 향상시킬 방법을 찾아나갔다.<br />
<br />
성능을 높이는 한가지 방법은 clock을 높이는 것이었다.<br />
최신 제조장비를 도입해서 미세공정 품질을 높이고, 이를 통해 cell의 크기를 줄이고 누설전류와 발열을 억제해 CPU의 최대 스위칭속도를 높이는거다.<br />
이걸로 clock이 2배 높아지면 CPU 성능도 2배가 된다.<br />
<br />
하지만 switching 속도가 너무 빨라지면 원래 같이 동작하는걸 전제로 만들었던 회로가 따로 동작할 수도 있다.<br />
(그림)<br />
<br />
1GHz면 30cm 이동하니까, 길이가 30cm 넘어가면 다른 clock이 걸리게 된다. 4GHz면 7.5cm<br />
그래서 한 덩어리였던 하드웨어 블록을 여러 덩어리로 쪼개야 한다. 이게 파이프라인이다.<br />
<br />
다른 방법은 아키텍처를 넓히는 것이다. 인텔 Sandy Bridge, Haswell, Skylake 등이 이런 아키텍처의 코드명이다.<br />
한개 CPU core에서 병렬로 동시에 처리될 수 있는 작업들이 생기는데, 이런걸 ‘ILP(Instruction Level Parallelism)’이라 부른다.<br />
이 ILP들을 찾아내 성능을 높이는 CPU를 Superscalar Processor라 한다.<br />
물론, 이 ILP 찾는건 아주 빠르게 이뤄져야 하기 때문에 만들기 Superscalar Processor를 만드는게 어렵다.<br />
하지만 이걸 해내면, clock도 안바꿨는데 CPU가 빨라진다!<br />
<br />
마지막 방식은 새로운 명령어를 ISA에 추가하는거다. 특정 상황에서 아주 빠르게 동작할 수 있는 신규 명령어,<br />
그걸 위한 하드웨어들을 추가하는거다.<br />
예시로는 인텔 펜티엄의 MMX, 펜티엄3의 SSE, 코어 시리즈의 AVX등의 SIMD(Single Instruction Multiple Data) 명령어들이 있다.<br />
한개 Instruction으로 여러개 숫자열 상태가 바뀐다.<br />
<br />
근데 이 방식은 다른 두 방식과 다르게, 프로그래머가 이 새로운 명령어를 써줘야 동작이 빨라진다.<br />
그리고, 과거 CPU들은 이 새로운 명령어를 이해 못하기에 프로그램을 돌릴 수 없게 된다.<br />
ex) 386은 MMX가 적용된 프로그램을 못돌린다.<br />
<br />
새로운 명령어가 추가되면 Compiler를 새로 만들어야 하는데, 인텔이 스스로 Compiler(ICC)를 만들어 팔았다.<br />
그래서 프로그램 개발이 그렇게 어렵지 않았고, 어차피 이런거까지 써야 하는 하이엔드 성능이 필요한 프로그램이면 옛날 CPU에서 안돌아가는게 문제가 되지 않았다.<br />
<br />
그래픽 작업이 병렬인 이유: 모니터에 표시되는 픽셀들은 서로 상호작용하지 않기 때문에 병렬이다.<br />
<br />
인텔은 램버스와 협력해 RD램을 만들어 다시 한번 메모리 시장에 들어가 PC 플랫폼에 대한 영향력을 늘리려 했지만, 메모리 기업들과의 원가 싸움에서 또 털리고 만다.<br />
<br />
20세기 말, 컴퓨터 성능과 용량이 기하급수적으로 늘어나고 있었다.<br />
32비트 기반이었던 x86은 한번에 접근 가능한 메모리 영역이 4GB였는데, 메모리 용량이 점점 커져서 곧 메모리에 한번에 접근하지 못하게 될 위기였다.<br />
접근 자체는 가능한데, 여러 단계를 거쳐야 하니 효율성이 떨어지는 상황이었다.<br />
<br />
인텔은 CPU를 64비트로 바꾸고 최대 메모리 주소를 늘려야 하는 상황이었는데, 20년간 유지해온 하위호환 정책에 대해 다시 생각해보게 됐다.<br />
<br />
신형 칩들 clock은 계속 올라서 2000년에는 1GHz를 넘었고, 실리콘 웨이퍼가 버틸 수 있는 최대 clock인 4~5GHz 근처로 빠르게 다가가고 있었다.<br />
<br />
그리고, 만들어진지 20년이 넘은 인텔의 x86은 각 명령어마다 길이가 달랐기 때문에(CISC),<br />
그 후 생긴 모든 명령어의 길이가 같은(RISC) ARM 프로세서보다 비순차 수행에 적합하지 못했다.<br />
<br />
비순차수행에서는 명령어들의 순서를 바꾸는 과정이 필요한데, 명령어들의 길이가 같으면 쉽지만 다르면 어렵다.<br />
<br />
그리고, CPU가 추출해낼 수 있는 동시 수행 가능성도 한계였다.<br />
<br />
인텔은 여기서, x86을 포기하고 앞으로 나아가기로 한다. 이때 HP와 협력한다.<br />
<br />
HP는 1980년부터 RISC도 CISC도 아닌 대안 아키텍처를 고민하고 있었다.<br />
<br />
2001년, 인텔은 HP와 협력하여 새로운 CPU 아키텍처인 Itanium을 발표했다.<br />
기존 CPU들은 비순차 수행 방식으로 성능을 끌어올렸는데, 인텔의 새로운 CPU는 그런 복잡한 비순차 수행 엔진(명령어 배치 장치)을 만들 웨이퍼 면적을 추가 연산장치에 투자하는 방향으로 갔다.<br />
<br />
그럼 Itanium은 동시에 명령어 수행을 못하나?<br />
Compiler에서 동시에 실행할 수 있는 명령어를 모아서 실행파일을 만들게 했다.<br />
이러면 Compiler의 부담이 커지긴 하지만, Compiler는 하넌 고생해서 만들어두면 수십년 쓰니까.<br />
<br />
이렇게 비순차 수행 엔진 치우고 그 자리에 연산장치를 놓으면 성능 좋아지고, 원가 내려가고, 전력소모가 내려갈 것이다!<br />
<br />
근데 이렇게 하려면, compiler는 CPU가 바뀌면 그때마다 매번 새로운 실행파일을 만들어줘야 한다.<br />
<br />
ex) CPU에 원래 연산장치가 4개였는데, 신형은 6개가 됐다 -&gt; 기존 실행파일 쓰면 2개가 논다. 그래서 실행파일을 새로 만들어줘야 한다.<br />
기존 슈퍼스칼라 프로세서였다면 알아서 새로운 2개 잘 썼을텐데!<br />
<br />
소프트웨어 회사들은 Itanium 아키텍처를 쓰려면 CPU마다 다른 실행파일을 만들어야 한다.<br />
<br />
하위호환성도 있기는 했지만, 영 좋지 않았다.<br />
기존 x86 실행파일 -&gt; CPU 에뮬레이터 -&gt; IA-64(아이태니엄 언어) -&gt; CPU 본체 과정으로 실행해야 했다.<br />
이렇게 복잡하게 실행하니, 당연히 성능이 기존 대비 아주 안좋았다.<br />
<br />
결국 아이태니엄은 잘 안팔렸다.<br />
<br />
2003년, 지금까지 인텔의 ISA를 라이센싱해서 사업하던 AMD가 완벽한 하위호환을 유지하며 64비트 확장을 성공시키고, 성능까지 높인 새로운 명령어 세트인 AMD 64 (= x86-64)를 내놓고, 서버용 CPU 슬렛지해머를 출시했다.<br />
이 CPU는 AMD의 서버용 CPU 라인업인 Opteron에 포함되는 제품이다. Opteron은 x86으로 구성된 기존 명령들을 완벽히 수행할 수 있었다.<br />
<br />
인텔 아이태니엄 생태계가 자리잡지 못한 상태에서, 기업들이 아이태니엄 말고 x86-64로 갈아타버리면 인텔은 망하고 말 것이다.<br />
인텔은 위기를 느끼고, x86-64를 받아들이며 CPU 개발계획을 크게 전환했다.<br />
그럼에도, 아직 HP와의 계약이 남아 아이태니엄을 완전히 접을 수는 없었다.<br />
<br />
그래서 이제는 인텔이 AMD에 x86-64의 사용료를 지급해야 하게 됐고, 지금까지도 내고 있다.<br />
<br />
짐 켈러가 AMD에 있을때 만든게 AMD64랑 하이퍼트랜스포트다. 하이퍼트랜스포트는 멀티코어 프로세서의 핵심 기술이다.<br />
<br />
아이태니엄은 인텔에게 계속 비용을 발생시켰다. HP에게 HP-UX 시스템용 CPU를 공급하는 장기 계약을 했기 때문이다.<br />
<br />
2004년, 인텔의 신형 CPU인 펜티엄4 프레스캇이 나왔는데, 초기에는 x86-64 지원 안한다고 하더니 얼마 후부터 갑자기 x86-64를 지원한다는 문구를 달고 나오기 시작했다.<br />
<br />
그 사이에 x86-64 지원하도록 재설계했다고 보기에는 너무 짧은 기간이고, 애초에 x86-64를 위한 공간을 만들어뒀던 것으로 보인다.<br />
<br />
AMD는 이후 듀얼코어 맨체스터를 개발하며 시장의 우위를 점했고, 인텔은 비순차 처리 엔진보다 clock 상승에 집중했다가 clock에 비해 낮은 성능을 냈고, 발열도 심해져 펜티엄4 프레스캇은 ‘프레스핫’이라는 별명까지 얻었다.<br />
<br />
2005년 AMD의 시장 점유율은 40%가 넘었으며, CPU 성능은 최저가 라인업부터 최고가 라인업까지 모두 AMD가 인텔보다 높았다.<br />
<br />
고가 라인업에서는 펜티엄이 애슬론FX에 밀렸고, 저가에서는 셀러론이 셈프론에 밀렸다.<br />
<br />
인텔은 정신차리고 ‘펜티엄’이라는 브랜드를 폐기하고, ‘인텔 코어2 프로세서’라는 브랜드를 내놓고, 2006년 신형 마이크로아키텍처 Conree를 내놓는다.<br />
<br />
인텔의 Conree는 clock이 2.4GHz밖에 되지 않았지만, 성능이 상당히 좋았다.<br />
Conree는 33만원이었는데, 성능은 당시 100만원이 넘던 AMD의 최고급 CPU인 애슬론 64 FX62를 밀어냈다.<br />
인텔이 개발 방향을 clock 중심에서 아키텍처 확장으로 전환한 덕분이었다.<br />
<br />
인텔이 정신차리자 AMD가 시장점유율을 잃어버리기 시작했다.<br />
게다가 TLB 버그라는 하드웨어 결함까지 발견되며 시장을 거의 잃어버린다.<br />
<br />
2000년대 초, AMD와 인텔 모두 한개 코어 성능을 키우는 데에는 한계가 있다는 것을 알고 있었다.<br />
그래서 CPU 하나에 여러개 코어를 박는 멀티코어를 만들어야 했다.<br />
<br />
근데 문제는, 코어끼리 통신하는 데에 시간이 엄청 걸린다는거다.<br />
공유해야 하는 정보가 CPU 공유 캐시에 있으면 명령어 100개 이상, D램에 있으면 1만개 이상이 필요하다.<br />
그래서 멀티코어가 어렵다. 통신이 오래 걸린다.<br />
<br />
4코어 사용 프로그램을 열어보면 4개 코어가 1~100% 사이에서 다양한 사용률을 보인다.<br />
다른 코어와 협업하기 위해 답장을 기다리는 동안 아무것도 못하니까.<br />
<br />
AMD는 승부수를 던져보기로 했다.<br />
AMD가 쓰던 아키텍처인 K10은 이미 수명을 다해가고 있었고, 대형 CPU 설계에서 인텔의 노하우를 이기는건 어려우니<br />
계속 커져가고 있던 CPU 코어의 크기를 줄이고, CPU 코어의 갯수를 늘리자는 것이었다.<br />
<br />
이렇게 하면 비순차 실행장치의 크기도 줄어들고, 연결되어야 하는 회로의 갯수도 줄어들어 설계의 어려움이 감소했다.<br />
비순차 실행장치의 성능은 크기에 정비례하지 않으니, 크기 감소로 인한 코어당 성능 감소가 작을 것이라고 생각한 것이다.<br />
<br />
프로그래밍 시장도 점점 많은 코어를 활용하는 방향으로 가고 있었고, 애초에 서버 시장에서는 코어당 성능보다 코어 갯수가 중요했다.<br />
<br />
AMD는 큰 코어를 작은 코어 2개로 쪼개고, 일부 하드웨어를 공유하는 ‘모듈’개념을 도입했다. 이걸 CMT(Cluster Multithreading)라 부른다.<br />
이렇게 개발한 마이크로아키텍처가 Bulldozer였다. Bulldozer는 INT가 2개고, DEC과 FP를 공유한다.<br />
*CPU 안의 DEC, DISPATCH, INT, FP가 뭔지 확인<br />
<br />
인텔은 암달의 법칙(Amdahl’s law)에 주목했다. 프로그램 실행 성능은 단일코어 성능과 멀티코어 성능 중 나쁜 쪽이 결정한다는거다.<br />
한 코어가 아주 빠르다고 해서 전체가 빠른게 아니다. CPU 코어들이 나눠서 할 수 없는 작업들도 있을거니까.<br />
<br />
그래서, 인텔은 코어를 늘리기보다는 코어당 성능을 올리고, 한개의 고성능 코어가 다른 작업도 처리할 수 있게 했다. 이 방식은 SMT라 불렸다.<br />
이 분야는 IBM이 1968년에 연구했던 분야다. 인텔은 이걸 hyperthreading이라 불렀다.<br />
<br />
이렇게, 두 회사의 CPU 설계 방향이 크게 갈리게 되었다.<br />
<br />
2011년, 최초의 Bulldozer 마이크로아키텍처 CPU인 4모듈, 8코어 잠베지가 출시됐지만, 성능이 아주 안좋았다.<br />
<br />
인텔은 4개 코어로 구성된 차기 마이크로아키텍처 샌디브리지를 발표했다.<br />
이전 세대 대비 CPU 단일코어 성능이 30%까지 높아져 있었다.<br />
<br />
잠베지랑 샌디브리지는 단일코어 성능이 2배 가까이 차이났다.<br />
이건 단일 코어에서만 돌아가는 옛날 프로그램을 실행하면 성능 차이가 2배 난다는 이야기다.<br />
<br />
그럼 서버용 CPU 시장에서는 AMD가 이겼나? 서버는 코어 수가 중요하다고 했잖아.<br />
서버에서 멀티코어가 중요한건 트래픽이 몰릴때 이야기고, 평소에는 서버용 CPU도 간단한 일만 하니까 이때는 단일코어 성능이 중요하다.<br />
서버 PC들은 굳이 AMD로 넘어갈 이유를 찾지 못했다.<br />
<br />
AMD의 서버시장 점유율은 계속 하락해서, 2012년에는 거의 끝장났다.<br />
그 후에는 개발되는 서버용 프로그램들은 모두 인텔 CPU에 최적화되고 호환성도 맞춰져 재진입이 어려워졌다.<br />
<br />
AMD가 거의 망해버리자, AMD 칩 제조를 담당하던 파트너사 글로벌파운드리즈도 경영 위기에 들어간다.<br />
AMD의 암흑기는 CMT를 포기하고 인텔의 SMT를 적용한 Zen이 등장하고서야 끝났다.<br />
<br />
인텔은 이렇게 두번의 위기를 극복해냈다.<br />
인텔의 강력한 단일코어 성능은 모바일 혁명 이후 기세가 붙은 ARM의 서버 시장 도전을 막아내는 강력한 방패로 작용했다.<br />
<br />
인텔은 마이크로소프트와 더불어 반도체시장을 독점하는 기업들 중 하나였지만, 눌러앉이 않고 계속 제품개발을 했다.<br />
162페이지에 인텔 연대표<br />
<br />
삼성전자, 인텔은 IDM(Integrated Device Manufacturer)이다. 하나의 회사가 반도체 설계, 생산을 모두 하는게 IDM이다.<br />
설계에 쓰는 프로그램은 EDA라 부른다.<br />
<br />
고성능 로직은 마스크가 20장 이상 필요하기도 하다.<br />
<br />
삼성전자는 메모리, 인텔은 CPU회사라 미세화 및 성능 향상의 이득이 그대로 들어나는 분야에 있다.<br />
그래서 1등기업이어도 계속 연구개발에 투자해야 한다. CPU는 수명도 길어서, 계속 더 좋은걸 만들어 팔지 않으면 굶어죽는다.<br />
<br />
MCU도 CPU와 비슷한 구조를 갖고 있지만, 고성능 연산을 포기하고 더 작은 연산장치나 간소화된 메모리 시스템을 갖고 있다.<br />
SSD, HDD 안에도 MCU가 있다.<br />
<br />
MCU가 하는 일은 간단하다. ‘A버튼이 눌리면 어떻게 행동한다’ 정도다. 자판기 생각하면 된다.<br />
그리고 자판기에는 고성능 연산이 필요 없다. 조금 빨라져서 뭐하나.<br />
<br />
MCU는 성능 요구치가 낮으니 단가도 낮다. MCU는 칩 하나에 메모리, CPU 등 전부 합친거고, DSP는 MCU보다는 비교적 낮은 처리능력을 가진 로직이다.<br />
하여간 얘네 시장규모는 메모리 시장의 20% 정도다.<br />
<br />
이런 제품들은 가격도 높지 않고, 추가 성능에 대해 고객들이 더 많은 돈을 지급할 용의도 없다.<br />
그래서 회사들이 굳이 설계~제조라인을 완벽히 갖추고 원가와 성능을 쥐어짜낼 이유가 없다.<br />
<br />
반도체시장 초기에는 많은 회사들이 자신의 fab을 갖고 있었지만, 미세공정이 점점 힘들어지자 회사들은 고민에 빠졌다.<br />
이제 미세공정 구현 능력과 원가 경쟁이 중요해졌는데,<br />
이걸 못따라간 회사들은 공장을 포기하고 팹리스 회사가 되거나,<br />
아예 매각/흡수합병되는 회사들도 많았다.<br />
<br />
소형 로직/마이크로컨트롤러 제조업체들의 경쟁력은 설계 노하우 및 인력이었고, 용도가 제한적이거나 특수했기 때문에 매각이나 인수가 쉬웠다.<br />
근데 메모리 업계는 전세계 회사들이 같은 제품을 서로 원가 깎아가며 만드는 경쟁이라,<br />
메모리 업계에서는 부도난 회사의 경쟁력 없는 노하우를 원하는 인수자가 없다.<br />
<br />
2011년, 일본 메모리업체 엘피다는 10조 부채를 남기고 파산했다.<br />
<br />
하여간, 이렇게 팹리스/파운드리 구조로 시장이 변했다. 각자의 이득은 p.174에 있다.<br />
팹리스: 공장에 투자하는 부담이 사라졌다.<br />
파운드리: 공정에 집중할 수 있다, 과거 공정을 계속 쓸 수 있다.<br />
<br />
ARM은 칩 설계의 일부만, 또는 칩 설계 없이 ISA만 팔았다. 설계하다 막히는 곳 있으면 우리꺼 사다 써라!<br />
약간 팹리스들의 팹리스가 되고 싶어 한다.<br />
<br />
TSMC는 첨단~옛날 공정 다 유지한다. 100nm 넘는 공정을 지금도 갖고 있다. 시장이 작아서 새로운 설계를 하는게 부담인 제품들도 있으니까.<br />
<br />
원래는 TSMC 첨단공정이 인텔보다 훨씬 안좋았는데, 나중에 역전되게 된다.<br />
<br />
21세기에 들어서자, 데너드 스케일링이 흔들리기 시작했다.<br />
수십nm 수준으로 미세화되자, Cell과 배선 사이의 parasitic effect의 영향이 늘어나 leakage current가 증가하게 됐다.<br />
전성비(전력 대 성능비)가 잘 늘어나지 않게 됐고, 반도체의 열밀도가 올라가게 됐다.<br />
<br />
이제 반도체 회사들은 집적도를 올릴 때마다 동일 면적에서 일어나는 발열이 커지는 것을 감내해야 했다.<br />
데너드 스케일링 말대로 집적도 올리면 트랜지스터당 사용하는 전력이 감소하는게 맞긴 한데,<br />
이제는 leakage current가 늘어나서 옛날만큼 감소하지도 않고, 옛날이랑 같은 면적이어도 열이 더 많이 발생하게 됐다.<br />
그래서 반도체 특성 관리, 냉각, 전력공급이 어려워졌다.<br />
그래서 칩 전체를 가동하는게 점점 힘들어졌고, 당장 필요없는 회로는 잠깐 꺼야 할 수도 있었다.<br />
<br />
노광장치의 발전도 느려지기 시작했다. 노광장치의 핵심은 빛의 파장을 최대한 줄여 해상도를 높이는 것이었고,<br />
거대한 볼록거울로 그 짧은 파장의 빛을 모으는 방식이었다.<br />
<br />
근데 문제는, 빛의 파장이 짧아질수록 흡수율이 올라간다는 것이었다.<br />
거울에서 반사가 잘 안되고 뚫고들어가버린단 뜻인듯? X선이 그렇듯이 말이다<br />
근데 밑에서 렌즈도 못쓴다는거 보니까 진짜 말 그대로 흡수인가?<br />
<br />
하여간, 이제 광원에서 발생한 빛이 웨이퍼에 전달되는 비율이 내려가고 있었다.<br />
<br />
Hglamp -&gt; KrF -&gt; ArF로 바뀌며 짧은 파장 광원을 열심히 찾아다니긴 했지만, ArF 레이저로는 20nm 근처 lithography가 불가능했다.<br />
그래서 광원 파장은 그대로 두고, 렌즈 아래에 굴절률 높은 액체를 배치해 광원의 파장을 줄이는 Immersion(액침) 방식을 사용했다.<br />
<br />
이렇게 해서 얼마동안은 노광기를 발전시킬 수 있었다.<br />
<br />
하지만 이후에는 EUV 영역으로 나아가야 했는데, EUV는 볼록렌즈를 쓸 수 없을 정도로 흡수율이 높다.<br />
파장이 너무 짧아서 인간이 마주치는 모든 물질이 극자외선을 흡수할 수 있다.<br />
<br />
그래서 EUV 장치 내부는 거의 완벽한 진공이어야 하고, 볼록렌즈 대신 특수하게 설계된 다중 반사판을 써야 한다.<br />
근데 뭐 어떻게 만들어도 loss가 심해서, EUV 장비 개발은 계속 늦어지고 있었다.<br />
<br />
그 동안에도 반도체 회사들은 미세공정이 필요했다. 그래서 ‘멀티 패터닝’이라는걸 사용했다.<br />
한개 패턴을 생산할 때 한개 마스크 대신 여러개 마스크를 써서 가느다란 배선을 만드는 방식이었다.<br />
<br />
근데 이렇게 하면 필요한 마스크의 수가 엄청나게 많아졌고, 사실 그렇게 정확하지도 않았다.<br />
lithography가 정확하지 않으면 제품 성능이 균일하지가 못하니, 결국 EUV를 쓰긴 해야 했다.<br />
<br />
그래서 EUV 장비를 사오면?<br />
EUV는 loss가 심해서 출력이 아주 낮다. 그래서 여러개를 사와야 하는데, 개당 2천억원 이상이다.<br />
그니까, 이거 안사오면 성능/품질에 문제가 생기고, 이거 사왔다가 제대로 가동 못하면 진짜 망하는거다.<br />
<br />
삼성전자는 칩 전체에 EUV를 쓰지 않고, 고해상도가 필요한 핵심 회로에만 EUV를 썼다.<br />
EUV만 쓰면 웨이퍼 처리가 너무 느리기 때문이다. 출력이 낮으니까.<br />
<br />
전체를 EUV로 찍는것보다, 일부만 EUV로 찍는게 필요한 기계 수도 적었다.<br />
<br />
인텔은 칩간 2차원 연결을 지원하는 EMIB 기술과 3차원 연결을 하는 포베로스 계획을 발표했다.<br />
미세공정의 영향을 덜 받는 I/O는 구세대 공정으로 만들고, 고성능 인터커넥트인 EMIB로 칩 사이를 연결하는 Chiplet 방식을 함께 도입하겠다는거다.<br />
<br />
CPU, 그래픽 등 성능이 중요한 부분에는 고성능의 10나노 공정, 입출력단이나 메모리 컨트롤러 등에는 구세대 공정을 사용하고 다 연결하는거다.<br />
영어로는 monolithic chip 대신 chiplet들의 결합이다.<br />
모든 부분에 비싸고 느린 공정을 쓸 필요는 없으니까. 구세대 공정 재활용도 가능하고.<br />
<br />
하여간, 이렇게 해서 노광장비와 함께하는 반도체 장비들의 가격도 함께 엄청나게 올라버려서, IDM, 파운드리 분야에 새로운 회사가 들어오는건 어렵게 되었다.<br />
<br />
설계 분야에서도 어려움이 생겼다. 트랜지스터가 많아지긴 했는데, 이걸 다 써먹는 것은 어려운 일이었다.<br />
일단 코어 갯수를 늘렸는데, 코어를 다 활용할 수 없을 정도로 코어가 많아지자 내장 VGA를 넣게 되었다.<br />
<br />
코어가 많다고 왜 못쓰냐? 식당에 손님이 8명 올때 요리사 1-&gt;2명은 차이 크지만 2-&gt;16명은 의미가 없다? 책 다시 봐야될듯 여기는<br />
<br />
설계는 공정의 한계도 어느정도 떠맡아야 했다. 미세공정 때문에 SRAM의 신뢰성이 급격히 떨어져서,<br />
전압의 순간적인 변화, 온도에 의한 특성 변화, 우주 방사선 등에 의해 값이 바뀌는 문제들이 생겼다.<br />
<br />
미세 공정때문에 Cell의 크기가 줄어들었기 때문에, 1개 bit를 표현하기 위해 쓰는 전자 수도 줄어들었다.<br />
옛날에는 전자가 50개 이상이면 1, 아니면 0이었던게 전자 10개로 기준이 바뀌는 식이었다.<br />
이러면 외부에 의해 전자가 1개 추가됐을때, 오차 2%였던게 cell이 작아졌기 때문에 10% 오차가 되는거다. 당연히 오차가 더 생기게 됐다.<br />
<br />
SRAM 말고, DRAM도 회로 미세화에 의해 Rowhammer라는 결함을 겪게 된다.<br />
특정 위치 cell에 접근할 때 흐르는 전류때문에 인접한 데이터들이 변조되는 문제다.<br />
이것도, 미세화 때문에 한 비트를 저장하는 데에 쓰이는 Cell 크기가 줄어들어서 생긴거다.<br />
이 문제를 해결하는 데에도 설계 변화와 추가 트랜지스터가 필요했다.<br />
<br />
로직 관점에서는 ECC(Error Correnction Code)로 문제를 막았다.<br />
당연히 ECC를 구현하려면 추가 트랜지스터, 웨이퍼를 써야 했다.<br />
설계 관점에서는 미세공정 진행하면서도 비트당 저장되는 전하의 양을 유지해야 했다. -&gt; Cell을 위로 쌓게 되었다.<br />
<br />
높은 탑을 쌓는건 어려웠지만, 그래도 DRAM은 로직보다 설계가 간단한 편이라 그럭저럭 용량이 늘어나고 있었다.<br />
<br />
그리고, clock이 빨라짐에 따라, 하나의 회로 안에서 신호가 제대로 전달되지 않는 문제들이 점점 커지게 되었다.<br />
회로 스위칭이 전달되려면 전류가 그만큼 빠르게 변할 수 있어야 하는데, Si의 한계에 부딪혔다고 한다. 의미는 찾아봐야겠다.<br />
<br />
하여간 그래서 회로 크기를 줄여야 했다. 회로가 너무 크면 같은 회로 내에서 다른 상태를 갖게 되니까.<br />
근데, 하드웨어 크기를 줄이면 또 성능이 떨어졌다.<br />
이때, 회로를 쪼갠 작은 단위를 ‘파이프라인’이라 부른다.<br />
<br />
CPU 제조사들은 이로 인한 성능 저하를 막기 위해 캐시메모리를 늘리고 분기예측기를 추가했다.<br />
하지만, 이런 식으로 하드웨어를 추가하자 회로 배선이 더 어려워졌고, 비순차수행 구현도 더 어려워졌다.<br />
<br />
연결 관계가 복잡해져 상호작용하는 block 수가 늘어나자 칩 전체의 문제를 파악하는 것도 힘들어졌고, 공정이 미세화되면서 같은 회로도 특성이 나빠져 원래 설계를 못쓰게 되는 경우도 생겼다.<br />
Meltdown, Spectre 등 하드웨어 수준의 보안 결함은 언젠가 터질 수밖에 없는 일이었다.<br />
<br />
시장에도 변화가 생겼다. 배치해야 할 트랜지스터가 늘어났기에 회로 설계 인력, 검증 인력, 개발 기간 모두 늘어나게 됐다.<br />
그리고 파운드리의 제조 역시 복잡해졌기 때문에, 실물 칩을 받게 되는데에 걸리는 시간도 길어졌다.<br />
<br />
소규모 회사들은 한두번 실수하면 회사가 망할 수준이 된거다. 개발 비용이 그만큼 올라갔으니까.<br />
대기업이라고 해도, 옛날보다 칩을 많이 팔아야 수익을 낼 수 있었다.<br />
<br />
그래서 파운드리 뿐 아니라 팹리스 기업들도 대기업과 중소기업 차이가 엄청나게 벌어진다.<br />
이미 큰 고객을 갖고 있으면 매년 조금씩 개선해서 팔면 되는데, 새로 진입하려는 회사들은 첨단 공정, 커다란 칩 설계에서 발생할 수 있는 온갖 문제를 다 해결해야 한다.<br />
<br />
이렇게 하드웨어 발전이 어려워지고 느려지게 되자, 소프트웨어 회사들은 어떻게 했을까?<br />
일단 단일코어 성능이 더 이상 빠르게 발전할 수 없음을 받아들이고, 다중코어(멀티코어)에 맞는 프로그래밍을 새로 해야 했다.<br />
그러다보니 수많은 다중코어 버그, 최적화 문제들을 해결해야 했다. 작은 회사들에게는 큰 부담이 되었다.<br />
<br />
프로그래밍 회사들은 CPU 말고 다른걸 찾아나서기 시작했다.<br />
필요하다면 CPU 말고, 특별한 목적만을 위해 설계된 가속기를 도입하려고 했고, 시장에는 물리 연산 전용 카드가 등장했다.<br />
대규모 단순 수치 계산을 위해 VGA를 사용하려는 회사가 늘어났고, 엔비디아 CUDA라는 VGA 기반 프로그래밍 라이브러리를 제공하기 시작했다.<br />
<br />
극단적으로 규모가 큰 소프트웨어 회사들은 자신들이 쓸 하드웨어를 스스로 설계하려는 욕심을 갖게 됐다.<br />
예를 들어, 구글은 알파고에 필요한 연산을 가속할 카드인 TPU를 직접 설계해 사용했다.<br />
구글에 따르면, TPU는 알파고를 위한 연산에서는 전성비가 동시대 CPU, GPU에 30~80배였다.<br />
당연히 다른 작업들에서는 깡통이었지만 문제가 되지는 않았다.<br />
<br />
이렇게 되자, 인텔의 입지가 좀 위험해졌다. 소프트웨어 회사들이 CPU 말고 다른 하드웨어를 알아보고 있잖아!<br />
VGA가 중요해졌고, 회사들이 자기에게 맞는 칩을 만들게 되면서 설계의 중요성이 커지게 됐다. 팹리스의 영향력이 커지게 된 것이다.<br />
<br />
반면, 메모리 회사들은 상대적으로 적은 위협을 받았다. 뭐 메모리는 써야 할 것 아닌가.<br />
<br />
전세계 핸드폰 시장을 노키아가 주름잡던 2007년, 아이폰이라는 것이 등장했다.<br />
그 후로는 사람들이 핸드폰을 오래 켜놓고 사용하기 시작했고, 핸드폰에는 전력 절감에 집중한 부품들과 커다란 배터리가 들어가게 되었다.<br />
Seagate, Western Digital(WD)같은 회사들은 아이팟에게 겪었던 수모를 또 겪게 된다. (HDD 회사들)<br />
HDD가 핸드폰에 들어갈 수는 없으니까.<br />
<br />
대신, 메모리 회사들은 플래시메모리를 팔면 되니까 상관 없었다.<br />
<br />
메모리 회사들은 전력소모를 줄인 모바일 DRAM을 설계해 이걸 엄청 팔아먹으며 개꿀을 빨았다. 물량도 많이 팔리고, 부가가치도 많이 붙여서 팔았으니까.<br />
<br />
모바일 시장 최초의 AP는 ARM 기반이었다. 스마트폰 시장이 확장되자, 핸드폰 제조사들은 자신에게 맞는 AP를 찾아나섰다.<br />
퀄컴, 브로드컴, 삼성전자 LSI사업부 등이 AP 설계 분야에 들어왔다.<br />
<br />
삼성전자의 엑시노스9와 애플의 A11은 모바일 회사들의 CPU 설계가 상당히 발전해 있다는걸 보여준다.<br />
두 CPU는 각각 ARM 기반, x86-64 기반이다.<br />
<br />
스마트폰이 PC보다 안좋은 점 중 하나는 입력장치다. 그래서 비밀번호 입력 등이 PC보다 힘들다.<br />
그래서 스마트폰은 개인 인증을 위해 홍채, 지문 등 생체 인식과 영상, 음성 인식 등 AI 인식을 도입했다.<br />
이런 인증에서 중요한건 반응 속도와 정확도인데, 스마트폰 회사들은 AI 연산 가속용 칩은 NPU를 탑재해 전력을 아끼면서도 인식이 잘 되도록 했다.<br />
<br />
스마트폰 수요가 넘쳐나게 되자, 수요를 바탕으로 막대한 자본 투자가 가능해졌다.<br />
원래 파운드리 기업들의 미세공정 수준은 인텔에게 2~3배정도 밀렸는데, 스마트폰 수요가 폭증하고 저전력 고성능 AP의 수요가 늘어나기 시작하자 파운드리들이 자본투자를 바탕으로 인텔을 따라잡기 시작했다.<br />
<br />
사람들이 스마트폰으로 정보를 주고받으면서, 데이터 저장 수요 또한 폭증했다.<br />
유튜브도 많이들 보니까 영상도 저장해둬야 하고, 사람들이 찍는 사진 화질도 계속 올라갔다.<br />
<br />
근데 이러면, 서버에 정보를 기록하는 성능보다 정보를 빠르게 읽는 성능이 더 중요하다.<br />
<br />
영상이든 사진이든, 업로드 초기에 가장 시간당 조회수가 많고, 그 뒤로는 계속 떨어지게 된다.<br />
그래서 업로드 초기에는 고성능 저장소에 뒀다가, 나중에 저성능 보관소로 옮기면 비용을 아낄 수 있다.<br />
<br />
메모리 회사들과 팹리스 회사들은 그에 맞는 제품들을 내놨다.<br />
가장 빠른 저장소는 DRAM이니까 서버용 DRAM을 내놓고,<br />
DRAM보다는 느리지만 그래도 빠른 TLC 기반 SSD를 내놨다.<br />
그리고 이런 서버들의 수요를 맞추기 위해 RI(Read Intensive) 제품을 내놨다.<br />
<br />
메모리 제조사들은 1개 방에 4개 데이터를 저장할 수 있는 QLC 기술도 개발해 제품을 만들어내기 시작했다.<br />
<br />
2007년 발매된 아이폰에 의해, 반도체 시장을 다 먹고 있던 인텔이 크게 흔들렸다.<br />
저전력, 휴대성에 대한 사용자 요구가 엄청나게 늘어났다.<br />
그래도, 인텔은 서버 시장 성장을 타고 돈을 꽤 벌긴 했다.<br />
<br />
AP가 중요해져서 팹리스 회사들이 크게 성장했고, AP에는 미세공정도 중요하니 파운드리도 크게 발전했다.<br />
메모리 회사들은 DRAM과 낸드플래시 메모리를 팔아먹을 스마트폰 시장이 생겨서 좋았고, 서버에도 메모리를 엄청나게 팔아먹게 된다.<br />
<br />
2016년에 알파고가 이세돌을 이겼다. 그 뒤로 인공지능이 계속 언급되며 발전했는데, 인공지능 학습을 위해 새로운 하드웨어들이 필요해졌다.<br />
학습능력은 초당 얼마나 많은 자료를 학습시킬 수 있는가라서 고용량, 고대역폭 메모리가 필요한데, 메모리 회사들이 연구하던 HBM이 이 목적에 적합했다.<br />
<br />
수많은 회사들이 HBM을 사고싶어했으며, HBM이 없을 때에는 그래픽용 메모리인 GDDR도 고려했다.<br />
하이엔드 VGA도 AI 가속을 염두에 뒀기 때문에 HBM이 필요했다.<br />
<br />
일단 HBM을 쓰면 대규모 데이터를 빠르게 들여올 수 있는데, 이 대규모 데이터를 빠르게 처리하는게 또 중요해졌다.<br />
그래서 엔비디아같은 GPU 제조사들에게 기회가 생겼다.<br />
<br />
CPU는 수많은 조건들을 따져야 하는 복잡한 작업을 빠르게 하고, 그걸 GPU한테 던져주면 GPU가 대규모 데이터 작업을 한다.<br />
<br />
현대 슈퍼스칼라 CPU는 상당한 웨이퍼 면적을 분기예측기 등 조건문 실행시 성능을 향상시켜줄 수 있는 하드웨어에 할당하고 있고,<br />
DRAM으로부터 받아와 실행했던 수많은 분기문들을 명령어 캐시(I-Cache)메모리에 저장하고 있다.<br />
<br />
CPU에도 정수 연산장치가 있긴 하지만, GPU 정수연산 성능이 압도적이다.<br />
<br />
모니터의 각 픽셀들은 서로에게 영향을 주지 않기 때문에, 1천만개 픽셀이 있다면 1천개씩 1만번 계산해도 된다.<br />
<br />
엔비디아는 개발자들이 엔비디아 제품을 쓰도록 생태계를 조성하기 위해 CUDA를 출시했다.<br />
CUDA는 개발자들이 GPU 기반 프로그래밍을 할 때, CPU 위 C언어 등에서 개발할 때같은 익숙한 느낌으로 개발할 수 있게 해놨다.<br />
CUDA 문법이 C 문법이랑 비슷하다.<br />
<br />
CUDA 덕분에, 프로그래머들은 자신이 어떤 칩을 사용하는지 신경쓰지 않아도 되게 되었다.<br />
프로그래머 입장에서는 엔비디아가 제조한 VGA만 사용하면 된다.<br />
일단 CUDA 기반으로 프로그램을 짜면, 이후에 GPU를 교체하더라도 추가 작업을 할 필요가 없게 된 것이다.<br />
<br />
옛날에 인텔 CPU 기반 프로그래머들이 인텔 CPU 기반으로 프로그램 만들면 나중에 다시 짜야 할까봐 걱정할 필요 없었듯이,<br />
CUDA 개발자들도 엔비디아를 믿고 걱정을 덜었다. 그래서 CUDA 덕분에 코드 참조나 이직도 쉬워졌다.<br />
<br />
근데 스마트폰에서도 안면인식, 지문인식을 위해 AI연산이 필요했다.<br />
물론 스마트폰은 PC와 환경이 많이 달랐다.<br />
<br />
엔비디아가 만들던건 대형 기판에서 100W 이상 먹는 거대한 칩들이라,<br />
핸드폰에 들어가는 NPU들은 엔비디아가 아니라 삼성전자, 애플 등 대형 스마트폰 제조사와 퀄컴같은 팹리스 업체들이 만들게 됐다.<br />
<br />
FPGA의 원래 목적은, 웨이퍼로 실물 칩을 만들기 전에 설계를 검증해보는거다.<br />
만약 칩 설계 단계에서 실수가 일어나게 되면 제조 공정을 통째로 엎어버려야 하기 때문에 실수의 대가가 크다.<br />
심할 경우에는, 제조사가 문제 원인을 알면서도 해결을 포기한다.<br />
하드웨어 한두개 크기가 바뀌어 배선도 바꿔야 하고, 결국 칩 형태가 크게 변해 칩 특성과 수율을 처음부터 다시 맞춰야 할 수도 있다.<br />
<br />
인텔도 멜트다운, 스펙터 결함 수정을 거부한 바 있다.<br />
<br />
어쨌든, 이런 설계 실수는 막아야 한다. 이를 위해 수많은 방법론들이 나왔다.<br />
일단 컴퓨터 시뮬레이션이 있다. 편하긴 한데, 문제는 시뮬레이션이 아주 느리다는 거다. 1초 보려고 며칠 돌리는 식이다.<br />
<br />
게다가, 칩의 동작은 그 칩에서 동작하는 소프트웨어와 큰 연관이 있다.<br />
그렇다고 이미 느린 시뮬레이션에 거대한 소프트웨어까지 올려서 시뮬레이션하면 더 느려져서 아무것도 확인 못한다.<br />
<br />
이건 팹리스 업체들에게 큰 부담이다. 팹리스 업체들이라고 해도 칩만 만들어 파는게 아니라, 해당 칩에서 사용 가능한 펌웨어까지는 같이 내놔야 하기 때문이다.<br />
근데 컴퓨터 시뮬레이션이 너무 느리면, 시뮬레이션으로 소프트웨어를 개발하거나 동작을 확인하는게 불가능하다.<br />
<br />
그렇다고 실제로 칩을 만들어봐? 이건 비싸고, 오래걸리고, 뭐 하나 수정하면 또 만들어봐야 하는데 그건 현실적으로 말이 안된다.<br />
<br />
그래서 FPGA를 쓴다. PC에서는 프로그램 형태로 시뮬레이션이 돌아가지만, FPGA에서는 실제 웨이퍼에서 로직이 동작하듯 돌아간다.<br />
물론 실제 칩보다는 느리긴 하지만, 그래도 이걸 써서 소프트웨어를 돌려보고, 구조를 바꿔서도 돌려보는 것으로 설계를 검증하고 펌웨어를 개발할 수 있다.<br />
하지만 가격이 CPU보다 비싸다.<br />
<br />
원래 FPGA는 이렇게 칩 시뮬레이션을 위해 쓰는 것이었는데, 이걸 AI 연산 등 특정 연산을 위해 쓰려는 사람들이 나타났다.<br />
<br />
2017년, 중국계 채굴 업체 BitMain은 채굴 전용 칩 Antminer를 설계해, 1600W 전력으로 14TH/s를 달성했다.<br />
엔비디아 GPU를 8개 연결한 채굴기들은 비슷한 전력으로 1GH/s밖에 달성하지 못했다. 1만배가 넘는 차이다.<br />
<br />
그럼 무조건 ASIC(Application Specific Intergrated Circuit)이 이득인가?<br />
위험할 수 있는게, 채굴에 필요한 알고리즘이 변하면 원래 쓰던 ASIC를 쓸 수 없게 된다.<br />
이더리움도 채굴용 하드웨어, 전용 칩 사용을 막기 위해 알고리즘을 바꾼 적이 있다.<br />
<br />
그래서, 하드웨어 수정이 필요할 수도 있는 곳에는 FPGA를 쓴다. 아예 ASIC로 만들면 수정이 안되니까.<br />
전무님이 Hardwire로 구워버리면 싸다고 한게 이 얘기인가?<br />
결국 FPGA가 쓰이게 된건 ASIC 대신이고, ASIC는 GPU보다 특정 연산을 위해 쓰는거였다.<br />
<br />
결국, 컴퓨팅 성능 발전이 느려지고 AI가 대두되어 GPU를 많이 쓰는 거였으니,<br />
컴퓨팅 성능 발전이 느려져 FPGA를 쓰게 됐다고도 볼 수 있다.<br />
<br />
클라우드 서비스가 나오면서(아마존 AWS, 마이크로소프트 Azure), 기업들이 모두 HPC(고성능컴퓨터)를 살 필요 없이 클라우드를 이용하면 되게 되었다.<br />
<br />
RDIMM(고성능 DRAM 모듈)의 수요가 증가했고, 서버용 SSD 수요가 늘어나 낸드플래시 매출이 올라갔다.<br />
메모리 회사들도 돈벌고, 가상화 호스팅 업체들도 돈 많이 벌었다.<br />
<br />
휴대용 디바이스 수요 증가-&gt; 그거에 맞는 칩 설계하는 팹리스 성장 -&gt; 파운드리 매출도 성장<br />
<br />
전통적인 CPU 성능 증가에 한계, AI 등 응용프로그램 등장 -&gt; GPU, FPGA 등 특정 연산에 강점을 보이는 칩들이 연산용 반도체의 주도권을 갖게 됐다.<br />
<br />
기존 회사들은 CPU에 최적화된 프로그램을 버리고, 전용 가속기에 맞는 프로그램을 설계하게 됐다.<br />
<br />
메모리 구조가 바뀔 필요는 없었다. 메모리 회사들은 그냥 팔면 됐고,<br />
대규모 병렬 처리가 많이 필요해지면서 HBM같은 고부가가치 고성능 메모리에 대한 수요가 생겼다.<br />
<br />
전통적인 로직 반도체 회사들은 모바일 플랫폼 발전+GPU 수요 상승에 편승 못하고 매출에 타격을 입었지만,<br />
AWS 등 서버 서비스가 확장되어 서버용 CPU를 팔며 이득을 보긴 했다.<br />
<br />
그럼에도, 인텔은 ARM 서버들의 도전을 모두 물리치고 여전히 최강자로 군림하고 있다.<br />
여전히 기업 서버 시장에서 인텔을 대체할 회사는 없다.<br />
<br />
인텔은 FPGA 회사 Altera를 인수했다. 이게 반격의 실마리가 될 수도 있다.<br />
어쩌면 FPGA도 가상화하여 전세계에 임대하는 사업 모델을 만들어볼 수도 있을 것이다.<br />
이렇게 되면 칩 설계 회사들도 앱 개발 회사들처럼 작은 규모로도 운영할 수 있게 될거다.<br />
<br />
20세기 말, IT 붐이 불었을 때 전세계에는 수많은 검색엔진 회사들이 나타났다.<br />
야후, 마이크로소프트, 구글, 라이코스 등이 나타났다. 모두 나름의 알고리즘, 검색 랭킹 시스템, 기능이 있었다.<br />
<br />
검색엔진은 사람이 더 많은 쪽으로 몰리게 되어 있다. 사람들이 많이 검색하면 데이터가 더 많이 쌓여서 알고리즘이 더 정확해지고, 사람들이 또 몰리기 때문이다.<br />
<br />
야후는 이것저것 띄워서 종합 포탈사이트로 갔고, 구글은 그냥 검색엔진만 띄워놨다. 결국에는 구글로 다들 모였다.<br />
<br />
구글은 회사규모가 커지자 다른걸 해보기 시작했다. 알파고도 처음에는 76개 GPU를 썼지만, 이세돌이랑 바둑둘때는 48개 TPU를 썼다.<br />
이게 되자 구글은 자신감을 얻고, 하드웨어를 다 스스로 만들기로 했다.<br />
<br />
아마존은 Graviton이라는 ARM 기반 CPU를 발표했다.<br />
Graviton은 지금은 AWS에서만 쓰이고 있지만, 나중에 어디 쓰일지는 모른다.<br />
<br />
마이크로소프트도 자체 제작 CPU를 만들 것이라고 발표했다.<br />
<br />
스마트폰 시대가 오자, 피처폰 + 특수목적 폰 + 업무기능이 되는 PDA를 만들려던 Motorola와 Blackberry는 존재감을 잃어버렸고,<br />
노키아는 시장의 최강자였으나 애플/삼성전자에게 시장을 빼앗기고 마이크로소프트에게 매각됐다.<br />
그러고는 윈도우폰을 만들더니 죽어버렸다.<br />
<br />
스마트폰으로 전환해 살아남은 회사들도 고민이 많았다. 일단 기능이 많아져 배터리가 중요해졌다.<br />
메모리도 중요해졌는데, 스마트폰 회사들의 메모리 발주량이 워낙 많다보니 메모리 회사들도 스마트폰 회사가 원하는대로 메모리를 만들어주고, 전담 지원팀도 만들어줬다.<br />
<br />
메모리 회사 입장에서는 고객사와 친해져서 중국 등 다른 회사들이 잘 못들어오게 되어 좋았고,<br />
스마트폰 회사 입장에서는 메모리 회사들이 내가 원하는 스펙의 메모리만 찍어내주니 다른 중소 경쟁사들이 못들어와서 좋았다.<br />
중소 기업들은 주문량이 적어 메모리 회사들이 지원을 안해준다.<br />
<br />
애플이 아이폰을 처음 만들때, 처음에는 인텔쪽 CPU들을 알아봤다고 한다. 하지만 너무 크고 전력도 많이 먹었다.<br />
성능이야 좋았는데, 당시 해봐야 인터넷 검색 정도 할건데 그정도 성능이 필요하지는 않았다.<br />
<br />
이때, 애플은 삼성전자의 S5L8900을 발견했다.<br />
성능은 인텔에 비하면 구데기였지만, 전력 소모량과 크기를 맞출 수 있는 AP였다.<br />
AP에 eD램도 부착되어있어 별도 DRAM도 필요 없었고, 그렇기에 공간을 아낄 수 있었다.<br />
설계, 파운드리, 패키징 다 하던 삼성전자라 가능한 일이었다.<br />
<br />
S5L8900은 x86 대신 ARM의 ISA를 쓰고 있었다.<br />
원래 ARM은 저전력에 많이 쓰였다.<br />
그래서 스마트폰은 이후로도 ARM 기반으로 성장했다.<br />
<br />
2010년, 애플의 아이폰을 따라 삼성갤럭시가 나온다. 갤럭시와 함께 구글이 모바일 시장에 들어온다.<br />
모두 ARM 기반이라 호환성 문제도 없었고, ARM 기반 소프트웨어 개발을 위한 수많은 프로그램들이 나와 수많은 벤처기업들이 사업에 뛰어들었다.<br />
<br />
인텔은 미세공정의 우위 + 노키아를 인수한 마이크로소프트와의 동맹 관계로 베이트레일 + 윈도우폰 조합으로 반격을 준비했으나 잘 안됐다.<br />
<br />
미세공정도 AMD가 인텔을 따라잡아버렸다.<br />
<br />
인텔이 메모리 시장 진출을 위해 내세웠던 SCM(Storage Class Memory) 사업도 큰 곤란을 겪고 있다.<br />
3D  XPoint라는 물건을 내놨다고 한다.<br />
<br />
3D XPoint 자체의 성능은 D램보다 나쁘지만, 용량이 커서 하드디스크나 SSD에 덜 다녀와도 된다.<br />
하지만 이걸 만드는게 생각보다 어려웠다고 한다. 그래서 잘 안됐다. Optane? Lehi?<br />
<br />
2015년 7월, 인텔은 167억 달러(거의 1년치 순이익)로 FPGA 회사 Altera를 샀다.<br />
2017년에는 AMD에서 그래픽을 총괄하던 Raja Koduri를 영입하고, Xe라는 그래픽 칩 개발을 시작했다.<br />
<br />
인텔이 왜 이런 짓을 했냐? 칩간 연결이라고 보고 있다.<br />
파이썬, C언어 하는 사람이 FPGA 쓰려고 Verilog 해야 한다고 하면 힘들거다.<br />
하지만 칩들이 통합된 개발 환경이 생긴다면, ‘FPGA에서 뭔가를 돌려라’ 정도 명령으로 FPGA를 쓸 수 있다.<br />
<br />
인텔은 CPU 회사라서, CPU랑 상호작용 필요한 부품들을 함께 패키징해 성능을 끌어올릴 수 있다.<br />
<br />
인텔이 포베로스같은 칩간 연결기술에 투자한것도 이런 통합을 위해서다.<br />
<br />
라자 코두리가 인텔에 와서 설계하기 시작한 Ponte Vecchio 칩은 신형공정/구형공정, 고성능/저성능 연결기술 모두가 사용됐다.<br />
포베로스 3D 패키징도 사용됐다고 한다.<br />
<br />
이런 하드웨어 수준의 통합을 소프트웨어로 만들고자 하는게 인텔의 OneAPI다.<br />
그냥 프로그램 하나 만들어두고, 새로운 하드웨어를 추가하면 그걸 자동으로 써서 최종 소프트웨어 성능이 자동으로 늘어나게 하는거다.<br />
<br />
인텔은 2021년에 파운드리 사업에 재진출하며, 인텔 파운드리 고객들에게  x86 칩 설계 IP를 제공하겠다고 했다.<br />
x86은 지금도 세계에서 가장 강력한 영향력을 행사하는 IP로, x86에 접근해서 커스텀한다면 서버 회사들은 엄청난 효율의 서버를 만들어낼 수 있을 것이다.<br />
<br />
2021년 3월, 인텔 행사에서 마이크로소프트 CEO는 인털 설계로 마이크로소프트만의 커스텀 칩을 만들겠다고 발표했다.<br />
<br />
인텔이 발표한 신규 제품 Alder Lake는 인텔 최초로 ARM의 big.LITTLE에 대응하는 이종 혼합 코어가 들어가 있다.<br />
여기 포함된 고효율 코어(Efficient core, E코어)는 인텔의 고효율코어(Performance core, P코어)와 비교했을 때,<br />
면적당 성능이 2배정도 되는 것으로 보인다. E코어 발전속도가 지금 좀 빠르다.<br />
*P코어 고효율코어 맞나?<br />
<br />
인텔은 어쨌든 스마트폰도 놓치고 공정도 따라잡혔지만, 생태계에 많은 투자를 해놨다.<br />
<br />
ARM은 다른 팹리스처럼 칩을 설계하고 위탁 제조해서 파는것도 아니고,<br />
타사의 칩을 받아 원하는 대로 만들어주는 것도 아니고,<br />
자사 칩, 칩의 일부, 자사 칩이 수행 가능한 ISA를 판다.<br />
<br />
ex)<br />
1코어 라이선스(ARM이 직접 설계한 코어 라이센스 + ARM ISA 사용 권리):<br />
삼성전자 엑시노스 LITTLE 코어, 퀄컴 스냅드래곤 LITTLE 코어, 화웨이 기린 980코어들<br />
<br />
아키텍처 라이선스(ARM ISA 사용 권리, 칩 설계는 직접):<br />
삼성전자 엑시노스 M 아키텍처, 퀄컴 고성능 Kryo 아키텍처, 애플 자체 아키텍처<br />
<br />
모바일 시대가 도래하자, 애플을 시작으로 ARM의 설계는 불티나게 팔려나가기 시작했다.<br />
<br />
수많은 회사들이 ARM의 코어 디자인을 구입하고, 자신만의 주변회로를 만들어 CPU를 만들었다.<br />
삼성전자, 퀄컴은 ISA만 구입하고, 코어는 스스로 설계했다.<br />
ARM과 거래하면 일단 AP를 만들어볼 수는 있었다.<br />
<br />
ARM에게는 피처폰 시절부터 사용해온 Mali라는 그래픽 솔루션도 있었다.<br />
<br />
시장이 계속 발전하면서, 발전 속도가 느려지기 시작했다. ARM은 고객들과 계속 대화해보며 문제를 들어보고, 수정 계획을 세웠다.<br />
<br />
ARM의 전성비 문제를 해결하기 위한 해결책이었던 빅리틀은, 웨이퍼 위에 최고 전성비를 갖는 구간이 다른 2개 CPU를 배치한다.<br />
가벼운 작업에서는 가벼운 작업에 효율이 좋은 가벼운 코어를 쓰고, 무거운 작업에서는 거대한 슈퍼스칼라 코어를 쓴다.<br />
간단해 보이지만, 또 쉬운 일은 아니었다.<br />
<br />
ARM은 이 계획을 미리 회사들과 공유했다. OS가 좀 고생해줘야 하는 방식이라 그렇다.<br />
OS가 작업의 경중을 잘 따져 코어들에게 분배해야 하기 때문이다.<br />
<br />
ARM은 이렇게, x86 진영이 해내지 못했던 이종 코어간의 컴퓨팅 모델을 만들었다.<br />
<br />
인텔은 2020년에야 최초로 Lakefield라는 이종 아키텍처 CPU를 소량 내놓았고,<br />
2021년 말에야 데스크탑용으로 알더레이크라는 CPU를 내놓았다.<br />
인텔은 이때서야 이종 아키텍처 CPU 시대에 합류하게 됐다.<br />
<br />
ARM이 차지하지 못한 시장도 있었다. 서버 시장은 전통적으로 최상단에 메인프레임이라 부르는 시스템이 존재하며,<br />
여기는 IBM의 POWER 아키텍처가 강력하게 자리잡고 있었다. 여기는 인텔도 못들어간 시장이다.<br />
괜히 메인프레임 바꿨다가 서버가 몇초라도 멈추면 엄청난 돈을 물어내야 한다.<br />
<br />
메인프레임 아래에는 서버 시장이 있고, 여기는 인텔이 먹어치운 시장이다.<br />
퀄컴, 캐비엄, Applied Micro 등 회사들이 ARM기반 칩을 내놓긴 했지만, 시장의 선택을 받지는 못했다.<br />
<br />
ARM 서버들은 Throughput(최대 처리 용량)은 뛰어났지만, 반응성(개발 코어의 최대 속도)이나 메모리 성능에서는 인텔을 이기지 못했다.<br />
<br />
게다가 페이스북, 마이크로소프트, 구글 등 거대 고객사가 ARM기반 CPU로 움직이려면,<br />
그들이 수십년간 x86위에서 개발해온 소프트웨어가 ARM기반에서 작동하도록 다시 새로 Compile해야 한다는 뜻이다.<br />
<br />
괜히 바꿨다가 온갖 버그가 발생해 서비스가 느려지거나 멈추기라도 하면 그건 엄청난 손실이다.<br />
<br />
2021년, ARM의 서버 진출은 AWS의 극히 일부 서비스(EC2 A1), CloudFlare의 edge server 정도로 한정되어 있다.<br />
<br />
데스크탑 시장은? ARM기반 CPU에서 쓸 프로그램이 없으니 필요가 없다.<br />
스마트폰에서 돌아가는 ARM 기반 프로그램들은 x86환경에서 크로스 컴파일러를 이용해 만들어진다.<br />
저전력 환경에서 돌아가는 수많은 프로그램들은 저전력 환경에서는 못만드는 것이다.<br />
<br />
ARM의 모바일 GPU는 성능, 시장 점유율에서 퀄컴, 파워VR을 이기지 못하고,<br />
AMD의 GPU 설계인 RDNA는 삼성전자와의 협업을 통해 모바일에 진출하려 한다.<br />
<br />
결국 ARM은 모바일 CPU 말고는 제대로 먹은 시장이 없는데, 스마트폰 출하량은 감소하기 시작했다.<br />
그리고 퀄컴, 화웨이, 삼성전자같이 AP 대부분을 스스로 설계하는 회사들 비중이 커지면 ARM도 위험해질 수 있다.<br />
ISA만 라이센싱하는건 설계 자체를 라이센싱하는것보다 훨씬 이익이 적다.<br />
<br />
ARM은 어쨌든 새로운 시장을 찾아내야 하는 상황이다.<br />
그래도 ARM은 자체 칩을 제조하지 않기 때문에 경쟁하기보다는 협력하려는 회사가 많다.<br />
엔비디아가 ARM을 합병하려 하기도 했었는데, 잘 안된 것으로 알고 있다.<br />
<br />
원래 GPU는 프로그램 수행의 결과대로 움직여 영상을 띄워주는 정도의 일을 했다.<br />
그러다가 CPU 성능 향상이 한계에 다다르고, 대안으로 떠오른 기계학습이 주목받게 됐다. 그런데 GPU가 기계학습에 적합했다.<br />
CPU도 4~20개 등 다중코어 시스템이 되긴 했지만, 여전히 거대한 디코더와 비순차 수행기 등 거대한 하드웨어가 필요했다.<br />
<br />
엔비디아가 행운을 공짜로 얻은건 아니고, 오래 전부터 GPU로 대규모 연산이 필요한 시장에 들어가려는 노력을 해 왔다.<br />
<br />
엔비디아는 2008년 Ageia라는 물리연산 가속기 전문 회사를 인수해 대규모 물리학 시뮬레이션이 필요한 분야에 진출하려고 했다.<br />
물리 연산도 특정 정지된 시간에 공간상에 표시된 물체들의 속도 등을 각자 따로 계산하면 되는 작업이기 때문이다.<br />
<br />
CUDA는 2006년에 처음 출시됐다. 엔비디아 그래픽카드기만 하면 쓸 수 있었고, 물리학 시뮬레이션과 인코딩/디코딩 등에 쓰였다.<br />
그러다가 AI가 주목받자 CUDA가 더 중요해졌고, 엔비디아는 GPU를 GPGPU라고 재명명하기도 했다.<br />
GPU는 단순히 모니터에 그림을 띄우는 칩이 아니라는 엔비디아의 선언이었다.<br />
<br />
GPU 경쟁사로는 CPU와 GPU 둘 다 하는 AMD 정도가 있었는데, AMD는 CPU를 말아먹고 죽어가던 상태였다.<br />
<br />
인텔, ARM도 GPU에서 엔비디아를 이길 수 없었다. ARM은 대형 하드웨어를 두고 고성능 컴퓨팅을 시도해본 적이 없었고,<br />
인텔은 Larrabee라는 x86 기반 병렬 프로세서를 개발하려 했지만, 제대로 된 성능을 내지 못했다.<br />
<br />
인텔의 그래픽카드는 2017년 라자 코두리가 AMD에서 옮겨오고 나서야 개선되기 시작했고,<br />
2020년 들어서야 쓸만한 물건이 나오게 된다.<br />
<br />
이렇게 엔비디아는 그래픽카드에 있어 독보적인 위치를 갖게 되었으며, 돈을 많이 벌어 TSMC의 최첨단 공정을 쓰게 됐다.<br />
<br />
GPU 가격은 100~200만원 정도인데, 머신러닝 전용 카드들의 가격은 천만원이 넘는다.<br />
그리고 이제는 단일카드만 파는게 아니라, 일종의 소형 슈퍼컴퓨터같은 형태로 팔기도 한다.<br />
여러개의 엔비디아 A100 카드를 엮어 만드는 엔비디아 DGX A100의 초기 출시가는 19만9천달러였다.<br />
근데 이런 비싼 제품들도 늘 공급부족이다.<br />
<br />
이런 가격대가 가능한건, 머신러닝이 엄청난 부가가치를 창출해 여기 참여한 회사들의 매출, 실적이 폭등해 부자가 되었기 때문이다.<br />
그러니 비싼돈 내고 이런것들을 사간다.<br />
<br />
엔비디아는 기존 PC 게이밍 시장은 그대로 유지한 채로, 슈퍼컴퓨팅, 서버, 자율주행차 등 수많은 분야들을 차지했다.<br />
Tegra라는 모바일 시장용 칩은 실패하긴 했다.<br />
<br />
근데 Tegra는 아예 뒤진건 아니고, 모바일 시장 말고 인공지능 에지(Edge AI) 시장을 개척하고 있다.<br />
엔비디아의 머신러닝 솔루션인 Jetson 시리즈의 칩으로 사용된 것이다.<br />
<br />
현재 Tegra는 초저전력부터 고성능까지 다양한 형태로 설계되어,<br />
전자는 머신러닝 입문자들이 좋아하는 Jetson 나노에,<br />
후자는 자율주행에도 사용 가능한 고급형 모델인 Xavier에 투입하고 있다.<br />
<br />
그리고, 지금 엔비디아가 갖춘 수많은 개발 인프라와 개발자 집단, 하드웨어 성능을 따라잡는 회사가 나오기는 힘들어 보인다.<br />
직업 시장에는 CUDA 프로그래밍을 전문으로 하는 프로그래머들이 이미 많이 자리잡았다.<br />
<br />
이 지위를 유지하기 위해 엔비디아는 지속적으로 CUDA에 새로운 기능들을 도입하고 있으며,<br />
Jetson Nano같이 CUDA를 사용하는 개발자 보드를 개발해 초보자들이 계속 들어오도록 만들고 있다.<br />
Jetson Nano는 카메라까지 사도 20만원이면 살 수 있다고 한다.<br />
<br />
엔비디아는 Jetson Nano에 약 10줄정도 코드만 짜면 이미지 인식 등을 시킬 수 있도록 ‘Jetson Inference’의 예시를 무료로 제시하고 있다.<br />
우리와 함께하면 인공지능 쉽다! 같은 느낌이다.<br />
<br />
하지만 엔비디아에게도 위협은 존재한다. 엔비디아의 새로운 상품들은 대부분 GPGPU를 중심으로 하고 있으며,<br />
필요한 경우 기존 GPGPU 근처에 ARM 프로세서를 결합하여 제어 능력을 부여한 것이 대부분이다.<br />
엔비디아는 이런 소형 프로세서의 자체 설계를 갖고 있지 않다.<br />
<br />
그래서, 만약 인텔같은 거대한 기업이 고성능 로직 프로세서와 기존 x86 생태계에 FPGA나 GPGPU를 결합하고,<br />
이를 통해 강력한 부가가치를 만들어내기 시작하면 엔비디아에게 큰 위협이 될 수 있다.<br />
<br />
지금은 GPU 분야에서 힘을 쓰지는 못하고 있지만, x86의 2인자 AMD도 엔비디아에게 위협이 될 수 있다.<br />
<br />
아니면 구글같은 강력한 소프트웨어 기업이 알파고에서 했던 것처럼 자체 가속기를 설계해 사용하고,<br />
나아가 해당 가속기에 맞는 소프트웨어 환경을 구축하기 시작할 수도 있다.<br />
이렇게 되면 엔비디아는 고객도 잃어버리고 생태계 주도권도 잃어버리게 된다.<br />
<br />
엔비디아 역시 텐서 연산기를 칩에 내장하는 등의 방식으로 성능 우위를 유지하려 한다.<br />
하지만, 결국 필요한 연산의 종류를 결정하는건 소프트웨어 기업이라 언제나 한발 늦게 될 수도 있다.<br />
<br />
그래서 엔비디아가 ARM 인수를 시도했을 것이다.<br />
<br />
TSMC는 B2B가 중심이라 이름이 알려진 기업도 아니었고, MCU, PMIC 등을 위탁생산하는 기업이었다.<br />
대만의 UMC와 경쟁하긴 했지만, 잘 알려지지는 않은 상태였다.<br />
<br />
이러다가 모바일 혁명이 시작됐다. 사람들은 스마트폰에 원하는게 더 많아졌고,<br />
결국 스마트폰 AP에는 저전력 + 고성능이 필요해졌다.<br /></p>

<p>저전력+고성능을 달성하려면 공정 미세화가 필요했다. -&gt; TSMC는 스마트폰 시대가 오자 큰 돈을 벌었다.
머신러닝 유행도 TSMC에게 큰 도움이 되었다.
GPU는 팹리스 업체인 엔비디아가 설계하는건데, 얘네도 제품 만들려면 TSMC에 와야 했다.</p>

<p>TSMC의 자본투자 규모는 2015년에 인텔을 넘어섰다. TSMC는 엄청나게 투자를 하는 기업이다.</p>

<p>TSMC는 구세대 공정도 많이 열어놓고 있다. 작은 회사들은 칩 설계를 매년 바꾸기 힘들다.
ex) 7nm에서는 하이엔드용 스마트폰 AP, CPU, 엔비디아 GPGPU등을 생산하고,
14~28nm에서는 중간급 스마트폰 AP, SSD용 마이크로컨트롤러 등 생산</p>

<p>중소기업들은 안전하게 옛날 공정들 많이 쓴다.
괜히 새로 설계했다가 문제생기면 감당 안되고, 새로 설계하는 것도 힘들다.</p>

<p>시스템 전체에서 사용하는 에너지가 너무 커서, 전력을 조금 아끼더라도 티가 안나는 분야에서는 10년 전 공정도 쓴다. 자동차 ECU 등.</p>

<p>2012년만 해도, TSMC 뿐 아니라 글로벌파운드리, 삼성전자, UMC, SMIC 등이 있었지만,
TSMC, 삼성전자만 남게 되었다. 그래서 얘네한테 주문이 다 몰리게 되었다.</p>

<p>글로벌파운드리가 생산하던 IBM의 POWER CPU들은 삼성전자에게 넘어갔고,
AMD의 물량은 TSMC에게 넘어갔다.</p>

<p>그렇다고 TSMC가 가격경쟁으로 삼성전자 파운드리를 말려죽이기에는 삼성전자 파운드리가 너무 큰 기업이다.
아마 이대로 계속 갈 것이다.</p>

<p>TSMC의 단점?
먼저 뭔가 만들어내지는 못하는 회사다. TSMC가 뭐 설계하려고 하면 고객들이 항의할거다.</p>

<p>구글은 검색엔진 회사로 시작해 이것저것 하는 회사다. 모바일에서는 안드로이드 OS로 마이크로소프트 OS 독점을 깼고,
폐쇄적이던 애플을 대신해 플레이스토어 중심의 새로운 ARM 기반 소프트웨어 생태계를 만들었다.</p>

<p>구글은 원래 반도체 엄청나게 사들이는 회사였다. 메모리, CPU, SSD 등 사가서 데이터센터를 지었다.
이제는 스스로 설계도 한다. 구글의 TPU는 구글이 스스로 설계하고 TSMC 28nm 공정으로 만든 칩이다.</p>

<p>이제는 소프트웨어 대기업들이 하드웨어 설계를 소프트웨어처럼 자신들의 지적 자산으로 인식하기 시작했다.
CPU 성능 향상이 느려지자, 소프트웨어로 할 수 있는 일이 느려져서 소프트웨어 회사들이 불편을 느꼈다.
소프트웨어 회사 규모도 커졌으니, 그냥 직접 만들자는게 이들의 생각이었다.</p>

<p>원래 칩을 설계하면 칩 제조사랑 소프트웨어 회사가 대화를 많이 해야 한다.
근데 구글은 그냥 같은 회사 내에서 대화하면 됐기 때문에, 이 과정이 훨씬 빨랐다.
그리고 원래 제조사들은 칩의 모든것을 보여주지 않고 한정된 스펙만 보여주지만,
구글 내에서는 모든 성능을 다 보면서 스펙에 맞는 소프트웨어를 만들 수 있었다.</p>

<p>구글은 TPU를 만들고 나서, 이 칩의 성능, 전력소모량, 아키텍처까지 공개했다.
이 칩에 적용할 사용자 라이브러리인 Tensorflow를 open source로 전환했고,
구글이 썼던 학습 데이터 세트도 공개해놨다.</p>

<p>전세계 모두가 이걸 쓰게 되고, 쓰면서 버그 리포트까지 해준다면 구글도 손해보는건 아니다.</p>

<p>구글은 TPU에서 멈추지 않고, 2021년 VCU라는 칩을 공개했다.
이 칩은 유튜브 영상 압축 및 변환만 전문으로 하는 칩이다.
이걸 쓰면 인텔 CPU 기반 서버 쓸때보다 훨씬 비용이 절감된다고 한다.</p>

<p>구글은 자신들이 선호하는 압축 방식(H264 VP9)을 선택해서, CPU가 주는 유연함이 필요 없는 상황이다.</p>

<p>삼성전자는 지난 40년간 엄청나게 발전했다.
메모리에서는 NEC, 히타치, 도시바 등 일본 메모리 업체들을 이겼고,
디스플레이에서는 Sharp를 넘어섰고,
가전에서는 소니를 제쳤다.
노키아는 스마트폰 시대가 시작되자 삼성전자에게 밀려 사라졌다.</p>

<p>기존 가전 및 IT 업체들과 삼성전자의 차이는 압도적인 수직 계열화를 통한 최적화 및 부가가치 흡수 능력이다.
삼성전자는 강력한 자체 공급망을 갖고 있다.</p>

<p>스마트폰 시장: 저장소, DRAM, 디스플레이, AP설계, AP제조, 전자부품, OS인데 삼성전자는 이중 OS 빼고는 다 스스로 만든다.</p>

<p>수직 계열화의 힘은 스마트폰 저장소(eMMC, UFS 등)에서도 나타난다.
eMMC, UFS같은 제품이 나오려면 반도체 공장에서 일단 낸드 기술이 완성되어야 한다.
이 raw NAND를 신형 컨트롤러에 부착하고, 그에 맞는 펌웨어(제어프로그램)을 탑재하고 수일~수개월간의 테스트를 거쳐야 한다.</p>

<p>그래서 UFS 만들 때에는 정말 다양한 회사들이 서로 소통해야 하는데,
삼성전자는 모든게 회사 안에서 이루어질 수 있다. 이게 삼성전자의 장점이다.</p>

<p>삼성전자는 메모리의 표준을 정하는 JEDEC, NVMe 표준 제정 및 수정에 깊게 관여하고 있다. 뭔 kv-SSD라는 것도 만들었다.</p>

<p>하지만 삼성전자도 소프트웨어는 제대로 못만들고 있다.
스마트폰용 바다OS는 시장에 제대로 나가보지도 못하고 사라졌다.
타이젠은 일부 스마트워치 제품들에서만 라이센스 비용 절약 목적으로 사용된다.
원래 목적이었던 사물인터넷 진출이나 디바이스간 연결은 제대로 안되고 있다.</p>

<p>삼성 그룹으로 보면 자체 브랜드 아파트까지 있기 때문에, 집-가전제품-스마트폰 연결성을 실현하기에 좋은 환경을 갖고 있다.
하지만 제대로 못해내고 있다.</p>

<p>빅스비도 제대로 안되고 있다.
OS, AI 모두 시장 초기 진입이 중요한 소프트웨어들이다보니, 삼성전자의 fast follower 전략의 한계일 수도 있다.</p>

<p>소프트웨어 개발자는 OS를 통한 유통망을 쓰게 된다. 소프트웨어 개발자 혼자 유통망을 만들기는 너무 어렵다.
개발자는 안드로이드, iOS에서만 잘 작동하게 만들면 유통은 플레이스토어, 앱스토어에서 해준다.</p>

<p>새로운 OS를 만들어서 여기 끼어들려고 해도, 소프트웨어 개발자들은 얼마나 버틸지도 모르는 새로운 OS를 위해 소프트웨어를 만들 이유가 없다.</p>

<p>AI는 쌓인 데이터가 그 성능을 결정하게 된다.
신경망 구조에 대한 연구는 거의 끝났고, 이제는 학습용 데이터의 양과 질이 중요하다.
특히 구글은 검색 엔진을 다 먹어버렸기에, 데이터에서는 경쟁자들이 구글을 따라가기 어렵다.</p>

<p>구글은 CAPTCHA(로봇이 아닙니다)같은 무료 보안 프로그램, QuickDraw같은 놀이용 프로그램으로 AI 능력을 향상시키고 있다.
이렇게 전세계 사람들이 구글의 AI성능을 올려주고 있는데, 삼성전자가 AI 인력 몇명을 채용해봐야 이걸 뒤집기는 어려울거다.</p>

<p>대리인 문제: 일을 시키는 사람과 일을 하는 사람 사이 정보 비대칭으로 생기는 업무 효율성 감소, 감시비용 증가
구글은 대리인 문제도 없이 AI를 키우고 있다.</p>

<p>그래도, 삼성전자는 OS, AI를 제외한 애플리케이션들에서는 성과가 있다.
삼성전자는 일반적인 어플 개발 회사들과는 달리, 강력한 하드웨어적 지원이 가능하다.</p>

<p>간편 페이:
MST: 대리점 수가 많지만, 스마트폰에 하드웨어가 필요하다. 플라스틱 카드들이 MST 방식이다.
NFC: 이미 폰에 탑재되어 있지만, 가맹점 단말기가 부족하다.
QR코드: 하드웨어가 필요 없지만, 개별 제휴가 필요하다.</p>

<p>삼성전자는 하드웨어 업체 LoopPay를 인수해 MST 방식 관련 기술을 얻었고, 2015년 삼성페이를 출시했다.</p>

<p>삼성전자는 삼성페이로 수수료 장사를 하지 않을 것이라 말해 은행권과 카드사의 지지를 이끌어냈다.</p>

<p>그 외에도 삼성전자는 다른 소프트웨어 분야를 선점하기 위한 노력을 하고 있다.
예를 들어 기어VR을 출시하면서 동시에 개발자용 VR기기를 발매했다.
아직 표준화가 안된 VR 시장에 투자한건데, 사실 VR 시장이 활성화되지는 못했다.</p>

<p>삼성전자는 2019년 세계 최초로 7나노 EUV를 도입했지만, EUV 출력 부족과 공정특성 부족으로 고생했다.
애플, 퀄컴은 삼성전자오 제품 포트폴리오가 겹쳐 갈등을 빚었고, 삼성전자는 일단 파운드리를 별도 사업부로 분리했다.</p>

<p>TSMC는 3nm에서도 FinFET을 유지하기로 했는데, 삼성전자는 3nm에서 GAAFET을 선제적으로 도입해 도전장을 내놓을 계획이다.</p>

<p>생태계를 미리 만들어놓기 위해, 삼성전자는 2019년 SAFE(Samsung Advanced Foundry Ecosystem)라는 파운드리 생태계를 런칭하고, 2022년에 양산될 3nm공정의 PDK를 2019년에 설계 회사들에게 배포하는 등 노력을 했다.</p>

<p>삼성전자는 지금도 거대한 규모를 가진 기업이다. 메모리, 파운드리, 스마트폰을 다 갖고 있다.
소프트웨어까지 갖추게 된다면 진짜 차원이 다른 기업이 될 것이다.</p>

<p>삼성페이는 초기에 MST를 통해 사용자를 끌어모았지만, 해외 사용자 확보는 미흡했다.
어쨌든 MST나 갤럭시 S6의 엣지 전용 앱 등 삼성만이 할 수 있는 일들을 하다보면 언젠가 흥할 것이다.</p>

<p>2015년, 중국 전국인민대표회의에서 ‘중국제조2025’라는 산업 계획을 발표했다.
향후 핵심동력이 될 10대 산업을 선정하고, 2025년까지 중국이 제조업 수준을 독일, 일본 수준으로 끌어올리겠다는거다.</p>

<p>그 분야 중 하나가 ‘차세대 정보기술’인데, 이 과정에서 중국 정부는 반도체의 핵심 설계기술을 확보하고 핵심 칩을 생산할 것임을 분명히 했다.</p>

<p>한국은 대중 수출 비중이 크고, 첨단 제조업이 국민 경제에서 차지하는 비중이 높아
중국이 첨단 제조업 국가가 될 경우 피해가 클 수 있다.</p>

<p>중국은 그 뒤 반도체에 돈을 쏟아붓기 시작했다.
중국은 서방 국가에서 반도체 회사가 매물로 나올때마다 언제나 적극적으로 M&amp;A를 시도한다.</p>

<p>2015년, 미국의 낸드 및 낸드 기반 솔루션 업체인 샌디스크가 매물로 나왔을 때도 인수합병을 시도했고, 마이크론을 인수하려고 시도한 적도 있다.</p>

<p>이게 잘 안되자, 샌디스크를 인수하려 했던 미국의 HDD 전문업체인 웨스턴디지털에 지분참여를 시도했다.</p>

<p>그 다음해에 도시바가 회계부정 사건으로 도시바 메모리를 분리하려 매각하려 했을 때도 컨소시엄 형태로 입찰에 참여했다.</p>

<p>이런 인수 시도들에서 미국이나 서방세계의 견제로 큰 효과를 보지는 못했지만, Spreadtrum같은 회사들의 지분은 꾸준히 매입하고 있다.</p>

<p>중국은 거대 IT기업들도 많고 스마트폰 제조사들도 많지만, 반도체 부품, 개발 툴은 서방 국가들에게 의존하고 있다.
그래서 이런 반도체 굴기를 하는 것일 것이다.</p>

<p>2019년 1월, 수원지법은 한국에서 D램설계를 주도했던 김모씨의 CXMT(창신 메모리 테크놀로지스) 이직을 막아달라는 삼성전자의 요구를 받아들여 11월까지 거기서 일하지 말라고 했다.</p>

<p>이때, 법원은 중국 반도체 회사들의 D램 설계 기술이 3~10년정도 뒤처진 것으로 보인다고 했다.
메모리에서 3~10년이면 아예 상대가 안되는 수준이다.</p>

<p>칩이 커질수록 수율이 떨어진다. 그래서 작은 칩은 수율 96%, 큰 칩은 수율 85% 이런 식일 수 있다. 그러면 그만큼 원가 경쟁력이 떨어진다.</p>

<p>그래서 거대한 칩은 결함 없이 개발하는게 사실상 불가능하다.
엔비디아에서는 일단 대형 칩을 기준으로 제조한 뒤, 불량이 난 영역을 죽이거나 잘라서(Cut-chip) 하위 라인업의 그래픽카드에 쓰는 식으로 대응하고 있다.</p>

<p>하지만 D램은 8기가비트 칩의 일부를 죽여 7기가바이트나 6기가바이트로 판매할 수 없다. 무조건 8기가비트를 맞춰야 한다.
그래서 제조과정에 추가 cell들을 집어넣고, 웨이퍼 제조가 끝나면 레이저 퓨즈를 이용해 특성이 나쁜 cell들을 다시 mapping해주는 과정(레이저 수리)을 진행한다.</p>

<p>하지만 제조 노하우가 부족한 중국이 이걸 능숙하게 하는 것도 어렵고, 모든 결함이 레이저 수리로 고쳐질 수 있는 것도 아니다.</p>

<p>중국은 기술이 딸려서 칩 크기를 줄이지도 못하고 있다. 그래서 수율 밀도 모두 상대가 안된다.</p>

<p>낸드는 기술이 D램보다 간단하고, 중국과의 기술 격차가 3~4년인 것으로 파악되고 있다.
D램처럼 칩당 용량대결을 위해 대형화할 필요 없이, 작은 사이즈로 생산해 여러개 겹치는 것이 가능하다.</p>

<p>SSD는 D램과 달리 각 칩이 CPU와 직결될 필요가 없으며,
CPU가 직접 나노초 단위 고속 접근을 요청하지도 않기 때문이다.</p>

<p>게다가, 컨트롤러의 도움을 받아 낸드의 불량한 특성을 어느정도 상쇄할 수도 있다.
낸드 양산 특성이 불량하더라도 소프트웨어 알고리즘으로 어느정도 상쇄할 수 있고, 대만 등에서 컨트롤러를 구하는 것은 어렵지 않다.
그래서 중국은 낸드 분야에서 더 빠른 발전을 보이고 있다.</p>

<p>하지만 시장을 차지하기에는 여전히 어려움이 있다.
수율이 여전히 낮고, 중국 기술력이 발표에 미치지 못한다는 간접적인 증거가 상당히 많다.</p>

<p>2018년 6월, 중국의 YTC는 초고석 낸드용 인터페이스인 Xtacking 기술을 발표했다.
YMTC는 이 기술로 전체 낸드 면적의 20~30%를 차지하는 peripheral(주변회로)들을 cell 아래로 감출 수 있고,
기존 낸드들의 400~800MBps를 4배로 넘는 3GBps를 달성할 수 있다고 말했다.
대단해 보이지만, 사실 알고보면 좀 이상한 기술이다.</p>

<p>일단, 2개의 웨이퍼가 필요한 기술이다. 한쪽 웨이퍼에는 기존 낸드 칩, 한쪽 웨이퍼에는 CMOS를 만들어 둘을 결합한 방식이다.
아니 근데, 기존 메모리 시장은 같은 양의 웨이퍼에 더 많은 cell을 올리려고 노력하고 있었다.
당연히, 웨이퍼 하나 더 가져와서 별도 칩을 만들면 성능 개선이 가능한건 다른 낸드 제조사들도 다 알고 있었다.</p>

<p>도시바, 하이닉스는 1개 웨이퍼만으로 이 문제를 해결했다.
웨이퍼 맨 아래쪽에 CMOS를 형성한 후, 그 위에 3D 낸드를 깎아내는 방식을 썼다.
이 기술을 COP(Cell Over Peripheral)이라 부르며, 하이닉스는 이 기술에 4D낸드라는 이름을 붙였다.
신기하게도, 두 기술 모두 Flash Memory Summit에서 동일한 날짜에 공개되었다.
이 회사들은 YMTC와 달리, 웨이퍼 추가 없이도 성능을 올린 것이다.</p>

<p>이상한 점 또 하나는, IO 성능은 낸드 전체 성능에 생각보다 영향이 적다는 점이다.
낸드의 동작은 두 단계로 이루어져 있다.</p>

<p>쓰기: IO를 통해 data를 받아들인 후, Cell에 강한 전압을 가해 기록한다.
읽기: Cell에 적당한 전압을 가해 읽고, IO를 통해 데이터를 내보낸다.</p>

<p>근데, 여기서 IO를 통해 data가 왔다갔다 하는 시간보다는 Cell에서 data를 읽고 쓰는 시간이 훨씬 오래 걸린다.
Cell &lt;-&gt; Page buffer &lt;-&gt; 외부 이런 구조다.</p>

<p>그래서 IO속도만 빨라봐야 의미가 없다.
게다가, 중국산 낸드 성능은 다른 글로벌 회사들보다 떨어질테니, IO 속도는 더욱 의미가 없어진다.</p>

<p>그리고 요즘은 낸드가 단품으로 팔리는 경우가 줄어들고 컨트롤러와 합쳐진 형태로 많이 팔리는데, 이건 중국 업체들에게는 장점이 아닌 리스크다.</p>

<p>현재 SSD, eMMC 등 낸드 기반 스토리지 시장은 서서히 낸드 팹을 가진 회사들을 중심으로 재편되고 있다.
2015 Q4 ~ 2017 Q4동안 전체 SSD 용량 출하량은 81% 늘어났는데,
낸드를 제조하지 않는 업체의 SSD 출하량은 25% 가까이 감소했다.</p>

<p>이건 낸드가 복잡해지자 특성이 제조사별로 크게 차이나기 시작해서 그렇다.
아무리 코드, 컨트롤러를 뛰어나게 설계하고 준비해놓았다고 해도,
사오는 낸드의 특성이 크게 변해버리면 또 새로 만들어야 한다.</p>

<p>근데 IDM 업체들은 자사 낸드의 특성을 미리 알고 준비해둘 수 있어서, 제품 출시도 빠르고 성능도 좋다.
결국 SSD가 아닌 일반 낸드를 사서 자기들 컨트롤러 붙여 파는, 작은 컨트롤러 전문 업체들이 줄어들게 된다.
이러면 낸드 찍어내는 회사 입장에서는, 자기들이 개발한 낸드 특성이 안좋을때 낸드 떨이로 팔아버릴 수 있는 잠재적 고객들이 줄어든게 된다.</p>

<p>IDM 업체들은 타사의 raw NAND를 사서 SSD를 만들지는 않는다.
예외적으로, 인텔이 하이닉스의 TLC 낸드를 사서 소비자형 SSD를 만든 사례가 있는데,
이건 인텔 공장 라인업이 대부분 데이터센터에 치중되어있어, 단가 따져보니 소비자형 SSD 만들거면 낸드 사와서 파는게 낫다고 판단했던 예외적인 경우다.</p>

<p>일반적으로, 낸드 제조사가 아닌 SSD 제조사들은 컨트롤러를 팹리스에서 구매한 뒤,
낸드 완제품을 사와서 PCB에 조립해 판매한다.</p>

<p>근데 팹리스가 파는 컨트롤러들은 일반적으로 최대 연결 가능한 낸드 갯수에 제한이 있다.
컨트롤러에 낸드와의 연결 통로를 만드는 일이 어려운 일이라, 컨트롤러 크기가 커지는 원인이 되기 때문이다.</p>

<p>근데 SSD는 일반적으로 1개 컨트롤러에 여러개 낸드가 붙어 만들어지기 때문에,
SSD 파는 입장에서는 컨트롤러 하나당 더 많은 낸드가 연결될수록 원가 경쟁에서 유리하다.</p>

<p>어쨌든 낸드 회사의 영향력이 강해짐에 따라, 컨트롤러 회사들은 특정 회사 낸드 기준으로 컨트롤러를 설계하고 성능을 측정하게 되었다.
새로운 낸드를 썼다가는 성능 보장도 힘들고, 생각지 못한 펌웨어/컨트롤러 문제가 생길 수 있다.</p>

<p>반도체 기술 뿐 아니라, 웨이퍼 양도 문제가 된다.
중국은 기술이 딸려서, 한국 업체들보다 훨씬 많은 웨이퍼를 투입해야 같은 비트를 생산할 수 있다. cell 밀도를 못따라가니까.
여기에, 중국 업체들 칩 크기가 더 크니까 수율까지 낮을거다. 수율을 커버치려면 그만큼 더 많은 웨이퍼를 써야 한다.</p>

<p>그래서 기술이 딸리면 웨이퍼를 엄청 쓰게 되는데, 이때 웨이퍼에 비례해 소비가 늘어난다.
자재값, 전기, 소모품(식각용 물질, 포토레지스트 등)이 소모되기에, 웨이퍼 값만 나가는게 아니라 온갖 비용이 다 나간다.</p>

<p>그래서 기술이 정말 중요한 업계고, 기술이 있다고 해도 돈이 엄청나게 드는 사업이다.</p>

<p>근데 중국이 여기 들어올 수 있겠나? 아까 말한대로, 동일 물량을 생산해도 웨이퍼가 몇배로 필요하고, 거기에 비례해 온갖 소모비용이 따라오고, 심지어 시장에 새로 들어오는거니까 판관비도 기존 기업들보다 더 써야 한다.
DRAM이든 낸드든 해당되는 얘기다.</p>

<p>중국이 그나마 기대해볼만한 부분은 대용량화+공정미세화가 멈추는거다.
미세화가 멈추면, 현재 압도적 진입장벽인 비트당 원가 차이가 좁혀질 수 있다. 미세화가 멈춰서 원가 더 못줄이니까.
대용량화가 멈추면 칩을 작게 만들어도 될거다. 그럼 웨이퍼당 수율 압박에서 벗어날 수 있다.
실제로, DRAM의 연간 원가 감소율은 옛날에 비해 상당히 감소했다.</p>

<p>근데 그렇다 해도 기존 선두 사업자들은 만들기 어려운 HBM등을 개발하며 동일 공정하에서도 기술 격차를 유지할거다.</p>

<p>일반적으로, 모바일 시장은 물리적 공간의 제약이 커서 DRAM을 4개까지만 겹쳐서 사용할 수 있다.
그 이상 겹치려고 하면 전력 및 발열 제어, 칩 높이 조정, 와이어 연결 등의 문제가 발생하게 된다.
지금도 하이엔드 스마트폰은 공간이 부족해, 발열 덩어리들은 AP, DRAM, UFS를 한개 패키지 안에 쌓아놓은 방식(ePOP 기술)을 쓴다.</p>

<p>AP에 DRAM을 추가로 연결하려면 추가 Pin이 필요한데, 이러면 면적이 더 필요하다. 그래서 한번 달때 용량 큰 DRAM을 달아줘야 한다.
저가형이라고 용량 작은 DRAM을 달아봐도, 메모리가 먹는 전력량은 하이엔드랑 다를게 없다. 그니까 진짜 DRAM은 용량 큰거 달아줘야 한다.
*핸드폰들 스펙 확인해보기</p>

<p>그래서 중국 회사들이 용량 작은 DRAM을 생산해도 저가형 핸드폰에조차 못들어간다.
그렇기에, 중국이 DRAM을 내수화하면 중국은 구데기 핸드폰밖에 못만든다.</p>

<p>PC는? PC도 메인보드 보면 D램 장착 가능 갯수에 제한이 있다.</p>

<p>뭐 문서작업, 웹서핑용 컴퓨터라면 DRAM 용량이 그렇게 중요하지는 않다. 문제생겨서 껐다 켜도 그렇게 큰 문제는 아니니까.
근데 서버같은 경우에는 DRAM 용량이 극단적으로 중요하다. 그래서 서버컴퓨터에는 메모리 꽂을 자리가 항상 부족하다.
그래서 여기서도, 중국의 용량 작은 DRAM은 쓸모가 없다.</p>

<p>그럼 애초에 큰 용량이 필요하지 않은 가전 등에 들어가는 DRAM은?
일단 가전은 시장 규모가 훨씬 작고, 메모리 성능이 중요하지도 않기 때문에 비트당 가격이 낮다.
그리고, 이런 시장에 머무르면 발전을 못한다.</p>

<p>삼성전자, 하이닉스 등 메모리 업체들은 매해 단순히 미세공정 개선으로 cell 크기만 줄이고 있는게 아니다.
반도체 개발의 시작은 시장조사다. 작년에 특정 회사와 비즈니스가 잘 안됐거나 높은 가격을 받지 못했다면 그 이유를 생각해보는거다.
전력 소모가 컸기 때문이라면 차기 메모리를 설계할대는 전력을 아끼는 형태의 cell을 만들어야 하고,
그 설계에 맞는 노공기와 에칭 장비들을 정해야 한다.</p>

<p>성능 부족이 원인이었다면, 성능에 관계되는 센스앰프 등의 하드웨어를 강화하는 방향으로 설계를 수정하고 이에 맞는 새로운 장비들을 도입해야 한다.
그리고 이런 장비들이 100%에 가까운 가동률로 굴러가도록 프로세스를 구성해야 한다.
이런 식으로 개선하는 일들은 노하우가 많은 업체들도 하기 힘들어하는 일인데, 신생업체가 하기는 더 어렵다.</p>

<p>중국기업은 JHICC, YMTC 정도가 있다.</p>

<p>여기까지는 메모리반도체 관련이었는데, 설계 파운드리 분야는 중국이 들어올 수 있을까?</p>

<p>설계 분야에서, 모바일 분야에서는 중국이 이미 상당한 수준에 올라와 있다.
대표적인 회사는 화웨이의 자회사인 HiSilicon으로, 이미 Kirin이라는 자체 AP도 갖고 있다.
다만, 얘네는 삼성전자 퀄컴과 다르게 자체 ARM 코어 설계를 만들지는 않고, ARM이 제공하는 코어 설계를 쓴다.
그래서 칩 판매당 순이익이 더 적을 것으로 추정된다.</p>

<p>HiSilicon은 하이엔드부터 로우엔드, 최근에는 서버까지 커버할 정도의 라인업을 갖고 있다.
Kirin이라는 AP, Kunpeng이라는 서버 CPU, Ascend라는 AI 가속기까지 있다.
이렇게 정말 다양한 칩들을 설계하고 있다.</p>

<p>설계가 어려운 일이긴 하지만, 그렇다고 공장처럼 대규모 투자가 필요한 일은 아니니까.</p>

<p>유니그룹도 중국의 설계 회사다. 유니그룹도 ARM에게서 코어 디자인을 라이센싱해와서 AP를 설계해 판매한다.
얘네는 HiSilicon처럼 모회사에 스마트폰 사업부가 있는 것은 아니고, 그냥 외부에 판매한다.</p>

<p>유니그룹은 중저가형 폰에 들어가는 AP를 만드는 회사다. 검증된 구 공정을 써서 만든다. 그래서 엄청 싸다.
삼성전자도 초저가형 라인업에 이 회사의 칩을 사용한 사례가 있다.</p>

<p>하이실리콘과 유니그룹 정도가 세계 top10에 들어가는 중국의 팹리스 회사들이다.
중국 팹리스 업체들은 성장하고 있긴 하지만, 대부분 물량이 중국 스마트폰 업체들에게 팔리고 있다.
다른 곳에는 잘 안팔리는거다.</p>

<p>그리고, 두 회사 모두 AP 설계시 ARM에서 설계한 코어를 쓴다. 아직 중국 팹리스에서는 고성능 로직을 자체설계하지 못하고 있다는거다.
최근 화웨이가 발표한 ARM기반 64코어 프로세서인 Kunpeng 920도 ARM에서 Ares라는 디자인을 사와서 설계한거다.</p>

<p>한국에서는 삼성전자 LSI사업부가 나머지 국내 팹리스 업체들 합친것보다 큰 매출을 내고 있다.</p>

<p>세계 팹리스 매출액 1등은 브로드컴이다.</p>

<p>파운드리는 중국이 따라잡기가 더 어렵다.
2018년 8월, 세계 2위 파운드리였던 글로벌파운드리는 차기 공정이었던 7nm의 양산을 포기한다고 발표했다.
대만 UMC도 7nm에 안끼어들고 14나노(성숙한 공정)에 집중하겠다고 했다. 얘네는 왜 7nm 경쟁에 안들어왔을까?</p>

<p>파운드리 투자액도, 메모리 시장처럼 기하급수적으로 늘어났고, 고객과의 관계도 중요하다.
EUV장비 대당 1500억정도 하는데, 이거 수십개 들여놓고 고객들 주문 받아서 기기 가동률 확보해 계속 돌리지 않으면 바로 망한다.</p>

<p>고객사 입장에서는 괜히 파운드리 바꿨다가 문제가 생길 수 있으니 안바꾼다.
파운드리를 바꾸면 없던 문제가 생길 수도 있고, 발열/전력소모량이 늘어날 수도 있다.
실제로, 아이폰이 TSMC와 삼성전자 양쪽에 맡겼을때 칩이 조금 다르게 나왔다고 한다.</p>

<p>효율적이었던 특정 트랜지스터가 파운드리 바꾸면 효율이 나빠지거나, 셀 설계 차이로 인해 회로의 지연 시간 등이 달라질 수 있다.
웨이퍼 실물을 받아 확인하는 데에만 3달정도 걸리니까, 경쟁을 해야 하는 기업들에게는 큰 부담이다.</p>

<p>그리고 고객들 역시 파운드리 굴리는게 어려운 일이라는걸 알고 있기에,
웬만하면 성공 가능성이 높은 파운드리와 일하려고 한다. 그래서 대형 파운드리로 몰린다.</p>

<p>그래서 SMIC는 1등이 될 수 있을 것인가?
TSMC 삼성전자로부터 고객을 뺏어와야 할텐데, 신뢰성 / 기존설계 재사용성 때문에 기존 고객들은 거의 안움직일거다.</p>

<p>글로벌파운드리 사업 재조정 선언 후, AMD CPU는 TSMC에게 갔고, IBM 메인프레임 프로세서는 삼성전자에게 갔다.</p>

<p>반도체산업 만드는게 돈만으로 되는게 아니다. 돈 많은 중동에서 해낸건 글로벌파운드리 인수뿐이다.</p>

<p>일본 회사들, HDD 회사들은 시장의 혁신가들이 원하는 것을 제공하지 못했기 때문에 시장을 잃어버렸고,
인텔은 혁신가들이 원하는걸 적절한 시간에 내놓았다. 하위호환성, 소프트웨어 개발 표준 등.</p>

<p>인텔 x86은 폐쇄적인 설계였다. 인텔 말고는 아무도 x86설계 만드는 법을 몰라서,
아무도 저전력, 소형 x86을 못만들었다. 인텔도 그런거 필요 없다고 생각해서 안만들다가 ARM이 시장을 먹어버렸다.</p>

<p>마이크로소프트의 사티아 나델라는 Azure를 통해 마이크로소프트를 살려냈다.</p>

<p>인텔이 2015년 발표한 3D Xpoint: 비트당 가격이 DRAM보다 훨씬 낮아질것이며, DIMM당 용량은 DRAM보다 압도적이다.
대신 반응속도가 좀 나쁘긴 하지만, 그래도 DRAM 용량이 중요한 곳이 있을 것이다.</p>

<p>넓은 영역 메모리에 가끔 접근해야 하는 대형 DB라면, DRAM + SSD보다 TCO가 뛰어날 수도 있다.</p>

<p>SADP, SAQP: 최근 미세공정에 사용하는 기술. Spacer Assisted Double(Quadruple) Patterning</p>

<p>공정의 전반적인 프로세스: 보호막을 전체에 씌운다, 모양 바꾸고 싶은 곳의 보호막을 뚫는다, 다른 물질을 뿌린다.
이게 끝나면 BEOL 과정으로 넘어가, 수많은 금속 배선들을 소자와 연결한다.</p>

<p>BEOL이 끝나면 웨이퍼를 테스트 장비로 보내서, 잘 만들어졌나 확인한다.
테스트용 Probe, 고온/저온 chamber등이 필요하다.</p>

<p>패턴이 아주 미세한 FEOL 단계에서 사용되는 노광기는, 패턴이 매우 큰 BEOL 마지막단계에서 사용되는 노광기와는 다를 수 있다.</p>

<p>식각도 맨 아래 소자층에 쓰이는 장비와 금속 상단부에 쓰이는 장비는 다르다.
하부는 정확도 &gt; 처리량이지만, 상부는 처리량 &gt; 정확도로 비용을 낮춘다.</p>

<p>장비중에서는 노광기가 제일 비싸다. 실제로 웨이퍼 위에 뭔가 그리는건 노광기 뿐이고,
나머지 기기들은 웨이퍼를 어딘가에 통째로 넣고 담그거나 산화시키거나 한다.</p>

<p>EDA tool로 기능 중심의 설계를 할 수 있다.
팹리스에서는 이걸 Front-end design이라 부르고,
Synopsis, Cadence같은 회사들이 tool을 제공한다.
여기서 쓰이는 언어로는 Verilog가 있다.</p>

<p>당연히, Verilog code에는 이 칩이 삼성전자 10nm인지 TSMC 5nm인지 안적혀있다.</p>

<p>팹리스는 파운드리에서 PDK를 받아와서, 자기 코드와 PDK로 GDS(Graphic Design System) 파일을 만들어낸다.
GDS 파일은 포토마스크를 만들때 사용되고, 파운드리를 통해 칩으로 만들어진다. 이걸 팹리스에서 back-end design이라 부른다.</p>

<p>결국 칩의 기능 만드는 영역과 칩을 실제로 만드는 영역이 있는거다.</p>

<p>팹리스는 외부 IP를 라이센싱 받아오기도 한다.
이때 코드 형태로 받아오면 soft macro, 물리적 실체로 받아오면 hard macro다.</p>

<p>soft macro는 특성을 예상하기 힘든 대신, 자사 칩과 완전히 한 덩어리로 만들어 넣을 수 있다.</p>

<p>hard macro는 칩 내에서 자유로운 모습으로 변경은 불가능하지만, hard macro 부분의 성능, 전력 사용량, 면적 등은 쉽게 추측할 수 있다.</p>

<p>이렇게 칩을 만들었으면, 패키징 과정에서 다리를 붙이든가 BGA로 만들든가 해야 기판에 납땜되거나 달라붙어 동작을 할 수 있다.</p>

<p>물론, 팹리스는 이 작업을 해주는 업체를 찾아 맡겨야 한다.</p>

<p>파운드리 입장에서는 EDA tool에서 잘 돌아가도록 PDK를 잘 만들어야 한다. 이건 쉬운 일은 아니다.</p>

<p>Silvaco라는 회사는 파운드리한테 공정정보를 받아 PDK를 만들어주는 일을 한다.</p>

<p>Renesas(르네사스) 여기 뭐하는 곳이더라?</p>

<p>7nm LPP(Low Power Plus): 삼성전자 공정, TSMC N7: TSMC 공정</p>

<p>파운드리는 남들 다 쓰는 회사로 모이는 경향이 있다. 협력사의 IP, EDA tool 등 인프라가 많으니까.</p>

<p>미세공정 난이도가 올라가면서, 수많은 팹리스들이 다른 종류의 칩을 제조한 후 후공정에서 결합하는 방식을 쓰고 있다.
그래서 파운드리도 칩 제조 뿐 아니라 칩간 결합과 패키징까지 하는 회사가 되어가고 있다.</p>

<p>인텔도 Foveros, EMIB같이 새로운 결합기술을 개발하기 시작했고,
TSMC도 플립칩같은 공간 절약 기술, CoWoS(Chip on Wafer on Substrate, 반도체 칩을 기판 대신 Si 웨이퍼 위에서 연결)등의 고성능 연결 기술을 개발했다.</p>

<p>이런 연결기술은 중요하다. 2020년 발매된 NVIDIA A100은 GPU에 HBM을 6개나 결합해야 했다.
하지만 당시 삼성전자는 최대 4개 HBM만 결합할 수 있었다. 엔비디아는 TSMC에게 갈 수밖에 없었다.</p>

<p>결합 기술은 생각보다 어렵고 복잡하며, 칩의 전력 사용량 등에 큰 영향을 준다.</p>

<p>파운드리는 생태계가 중요하다. 가져다 쓸 수 있는 IP가 많은게 좋다.
TSMC: IP Alliance Program, 삼성전자: SAFE, 인텔: x86 공개
등으로 생태계 구축 노력을 하고 있다.</p>

<p>기존 파운드리들의 자체 IP는 SRAM같은 단순한 구조뿐이었는데, 인텔 파운드리는 x86 IP를 제공한다. 이걸로 승부보려고 한다.</p>]]></content><author><name></name></author><category term="History" /><summary type="html"><![CDATA[HiSilicon: 화웨이의 칩 설계 회사 메모리 회사: 어느 정도 용량의 메모리가 가장 수요가 많을지, 어느 정도 전력 소비까지 고객이 감내할 수 있는지 확인 인텔같은 로직 회사: 고객이 어느 정도의 CPU 성능을 요구할지, 노트북 고객들은 어느 정도의 배터리 수명을 요구하고 CPU로부터 어느 정도의 전력 및 발열까지 감내할 수 있는지를 조사한다. D램은 사용자로부터 명령과 주소를 받는 부분, 주소를 해독하는 부분, 데이터 저장소, 읽어온 데이터를 잠시 보관해두는 latch로 비교적 간단하게 구성된다. D램은 설계 기술이 상대적으로 적게 필요하고, 대신 면적의 대부분을 차지하는 데이터 저장소의 면적을 줄이는게 중요하다. 현대 CPU들은 다층의 캐시메모리, 디코더, ROB, 대기소, 연산포트, ALU등 다양한 기능을 하는 많고 작은 하드웨어들로 연결되어 있다. chip의 설계를 도와주는 EDA, 설계를 제조기술에 맞춰주는 PDK 하드웨어 버그 예시로는 인텔 CPU의 보안 결함? 테스트 후에는 원래 패키징해서 팔지만, 공간이 중요해지며 아예 웨이퍼 단위로 사고팔기도 한다. 요즘은 여러개 CPU나 CPU+GPU 등의 구조로 Heterogeneous 칩을 고속 인터커넥트로 연결해 한 패키지로 팔기도 한다. Dhrystone: 1984년에 제안된 컴퓨터 성능 측정의 지표 인텔 11900K는 1초에 4,110억개 명령어를 처리한다(Dhrystone 기준) 컴퓨터: CPU, 메모리, 보조기억장치(HDD, SSD, USB메모리 등) 반도체 제조업이 다른 제조업드로가 구분되는 가장 큰 차이점은 기술력이 미치는 영향이 막대하다는 점이다. 1980년 1MB 메모리 가격: 6480달러 2015년 1MB 메모리 가격: 0.0042달러 (백만배정도 싸졌다) 다른 제조업들은 이정도로 못줄인다 Dennard Scaling: 같은 면적에 집적된 트랜지스터는 전력 소모량이 같다. 그래서 기술이 앞서는 회사는 전력소모 특성도 앞서나간다. 같은 면적 안에 트랜지스터가 100개든 100만개든 전력 소모는 같으니까. 그리고 반도체는 다른 제조업에 비해 부피가 작고 부가가치가 높은 편이라서, 한 회사가 만들어 전 세계로 수출할 수 있다. 그래서 빵집마냥 다른 동네로 도망가서 장사하는게 안된다. 전세계랑 경쟁해야된다. 제조원가에서 설비 투자 비용이 차지하는 비중이 압도적으로 크다는 점도 다른 제조업들과 다르다. 그래서 매년 엄청난 돈을 장비에 써야 한다. 즉, 고정비용이 크다. 고정비용이 크다는건, 그 해 생산을 안하더라도 아낄 수 있는 비용이 얼마 안된다는 뜻이다. 게다가, 반도체 공장은 공장 재가동에 시간이 오래 걸린다. 일반적으로 반도체는 포토마스크 한장을 처리하는 데에 하루 이상이 걸리며, 구조가 간단하다는 DRAM도 2000년에 이미 20장 넘는 마스크를 사용했다. 2013년에는 DRAM 웨이퍼 한장이 필요로 하는 마스크 수가 40장을 넘었다. 그만큼 오래 걸린다. 결국, 제품 가격이 떡락했다고 해도, 생산을 멈추면 손해다. 변동비용이 매출액보다 커지면 그때는 생산을 멈춰야 하겠지만, 반도체 생산에서 변동비용은 상당히 작다. 고정비용이 그만큼 커서 그렇다. 여기서 말하는 변동비용은 인건비, 웨이퍼 가격, 재료 가격 그럼 제품 가격이 떡락하면 회사를 팔아버리는건? 회사마다 공장에서 쓰는 장비가 많이 달라서 어렵다. 삼성전자는 지금까지 단 한번도 메모리 회사를 인수한 적이 없다. 그래서, 물량으로 시장 점유율을 가져왔더라도, 가격이 떡락하면 떡락을 막을 수가 없다. 어떻게어떻게 버텨서 살아남으면? 바닥회사들이 죽어서 싸움이 끝난건데, 이 경우 1등 기업과 겨우 살아남은 회사의 사정은 크게 차이난다. 기술력에 의한 원가 절감 차이가 엄청나니까. 그래서 1등을 절대 못따라잡는다. 그래서 메모리 반도체 시장에서는, 설비투자를 해버렸으면 연구개발과 재무운영 외에는 해볼 수 있는게 없다. 인텔 4004: 최초의 마이크로프로세서. ALU, 레지스터 등을 모두 웨이퍼조각 하나 위에 올려놨다. 4bit연산까지밖에 안됐지만, 이거 이후로 컴퓨터 가격이 떨어지기 시작했다. 1975: IBM 5100(1.9MHz CPU, 64kB 메모리) 1987: IBM XT(4.77MHz CPU, 640kB 메모리) 1983년 삼성전자의 도쿄 선언: 메모리 사업 시작하겠다 일본 메모리는 품질 위주, 삼성전자는 원가 위주 -&gt; 딱 시장에 필요한 성능만 내고 나머지는 원가절감 하겠다. 일본 메모리는 품질을 위해 제조공정이 길고, 장비 종류도 많았다. 1989년, DRAM 용량이 4MBit를 넘어가자 DRAM의 형태가 문제되기 시작했다. 지금까지는 평면에 트랜지스터와 DRAM을 함께 늘어놓는 식이었는데, 밀도를 높이려면 다른 방식을 써야 했다. Trench형과 Stack형을 두고 논쟁이 많았는데, 진대제, 권오현이 Stack이 낫다고 해서 이건희가 Stack으로 가자고 했다. 이때 IBM, 도시바, NEC 등이 다 트렌치 하는동안 삼성전자만 Stack을 했고, 옳은 결정이었기에 4MBit Dram이 대박을 친다. *Stack방식의 장점 적어놓기. 불량분석 이득이었던듯? 반도체 산업이 처음 시작됐을때는 100mm 웨이퍼를 썼다. 1980년대 들어서는 150mm, 90년대 들어서는 200mm가 주류가 됐다. 웨이퍼가 커질수록 버려지는 부분도 줄어드니까, 면적비 이상의 이득이 난다. 100mm 웨이퍼와 300mm웨이퍼는 웨이퍼당 칩 갯수가 10배 넘게 차이난다. 유명한 빅 칩중 하나인 IBM의 POWER9는 25x27mm다. 2001년, 웨이퍼 업체들이 300mm 양산기술을 확보했다. 메모리업체들 입장에서는 당연히 바꾸는게 장기적으로 이득이지만, 불황 시기에 엄청 비싼 장비들을 사와야 한다는게 부담으로 다가왔다. 공정 최적화도 또 해야 하는데. 이 불황은 2000년 10월의 닷컴버블 붕괴 때문이다. 버블 고점에서는 64MBit DRAM이 20달러였는데, 2001년 2월에에는 3.8달러가 됐다. 하여간, 회사들은 돈이 없어 300mm를 바로 도입 못했다. 안그래도 불황인데 여기서 또 문제생기면 그땐 진짜 망할 수도 있으니까. 근데 삼성전자는 빠꾸 안치고 2001년 10월에 300mm 웨이퍼 공정을 도입했다. 그래서 2001년에는 메모리 업체들 중 삼성전자만 흑자였다는데, 10월에 도입했는데?? DRAM Cell은 원래 8F^2 구조였다가 6F^2 구조로 개선됐다. 6F^2구조가 밀도가 더 높다. 마이크론이 2006년 처음 했고, 바로 삼성전자도 시작했다. 일본 기업들은 1등을 뺏겼지만, 삼성전자는 6F^2 cell, 300mm공장등을 해내며 계속 1등을 이어갔다. 결국 원가를 깎아서 삼성전자가 이긴거다. 비싸고 오래가는 메모리는 필요 없는 시대가 됐다. 삼성전자가 기술이 훌륭했다기보다는 시대 흐름을 잘 탄거다. 컴퓨터의 구성 요소: CPU, 메모리, 보조기억장치(HDD, SSD) 보조기억장치는 초기 컴퓨터 이론에 존재하지 않았다. 폰 노이만 구조에는 메모리, CPU만 존재한다. (그림) 뭐 전원 꺼지면 메모리가 날아간다 라고 폰 노이만 구조에 적혀 있지는 않은데, 실제로는 DRAM, SRAM으로 메모리를 만들게 되어 전원이 꺼지면 메모리가 날아가게 되었다. 이건 좀 문제가 됐다. 전원이 꺼져도 데이터를 보존하는 기억장치가 필요해졌다. 이때, CPU가 메모리 이외 공간에 데이터를 보낸다는건 그 데이터는 당분간 필요 없다는 뜻일거다. 따라서, 데이터 보존 장치의 성능은 좀 떨어져도 될 것이다. DRAM은 CPU가 직접 접근해야 하기 때문에, 메모리의 모든 방이 CPU에서 바로 접근 가능해야 했다. 이는 메모리 설계에서 큰 부담으로 작용했다. 모든 데이터 방에 금속을 설치해서 연결해야 하기 때문이다. 결국 큰 단위로 데이터를 저장하고, 데이터가 필요할 때는 그 단위를 통째로 불러와 필요한 것만 찾아 쓰는 방식이 되었다. 이게 더 원가가 낮았다. 처음에는 메모리로 자기 테이프를 썼는데, 원하는 데이터가 테이프 시작과 끝에 흩어져 있으면 테이프를 여러번 감았다 풀었다 하며 읽어야 했다. 이 경우 성능이 안좋았다. 1956년, IBM이 하드디스크를 내놓는다. 하드디스크는 구조상 최고성능-최악성능 차이가 적다. 테이프는 엄청 크다. 그림으로 설명 가능 즉, 카세트테이프는 순차접근만 가능, 하드디스크는 무작위 접근이 가능하다. HDD는 자기 테이프보다 비쌌지만, 성능도 좋고 관리도 쉬웠다. 세계 HDD 출하량: 1996년 1억대, 2013년 8억대 근데, CPU 성능은 2001년(펜티엄 1.2GHz) ~ 2012년(샌디브리지 3.3GHz)동안 24배 증가(코어성능 6배, 코어개수 4배)했고, 메모리 가격 대비 용량은 128배, 전송속도는 12배, 접근속도는 4배(2001년 PC-133, 2012년 DDR3-1600) 증가했다. CPU와 메모리의 발전은 이렇게 빨랐는데, HDD의 발전은 느렸다. 웨스턴디지털의 HDD 성능, 용량 변화: 2001(WB100EB): 전송속도 40, 반응속도 12.1, 용량 10GB 2012(WD10EZEX): 전송속도 150, 반응속도 8.9, 용량 1TB *단위 확인 필요 하드디스크를 보면 용량은 100배 늘었지만 최대 전송속도가 3.75배밖에 안올랐고, 반응속도는 그냥 모터를 5400RPM에서 7200RPM으로 높인게 다다. 그리고, 데이터 안정성 때문에 3.5인치 이상 디스크에서는 회전속도를 7200RPM 이상으로 올리는게 힘들었다. 게다가 컴퓨팅 기술이 발전해서 각종 프로그램, OS가 발전해 파일들의 크기가 커졌고, 그러면 사용자가 프로그램을 실행하면 하드디스크 여러곳을 탐색하며 다음 파일을 읽어야 하는데, 여기에서 큰 성능 하락이 발생했다. 1995년에는 MS-DOS를 썼고, 2010년에는 윈도우7을 썼다. 1995년에는 MS-DOS를 용량 1.44MB짜리 3.5인치 플로피디스크에 담아 썼는데(MS-DOS의 용량이 몇인지는 모르겠다) 2010년 윈도우7은 용량이 2GB가 넘는다. DOS때는 부팅시 3~10개 파일만 로드하면 됐었는데, 윈도우 부팅에는 수천개 파일이 필요하다. DOS 시절 게임 용량은 MB단위였지만, 2000년대 중반 게임들은 로딩 시간이 1분은 필요하게 되었다. 결국 HDD가 데이터 입출력 병목을 만드는 상황이 되었다. 그리고, HDD는 기계장치 때문에 소형화하면 성능 저하가 크게 일어났고, 저전력 상태로 만드는 데에도 오래 걸려서 저전력 상태를 유지하기 어려웠다. 결국 HDD는 저전력과 고성능 양쪽에 방해됐다. 플래시메모리는 1980년 도시바에서 마스오카 후지오 박사가 만들었다. DRAM, SRAM은 전하를 매우 작은 도체로 구성된 공간에 집어넣어 1과 0을 구분했다. 플래시메모리는 더 출입이 어려운 절연 구역에 고전압을 가해 전자를 터널링시켜 가두는 것으로 1과 0을 구분했다. 당연히, 플래시메모리는 SRAM, DRAM보다 읽기쓰기 속도가 훨씬 느렸다. SRAM, DRAM은 도체랑 바로 붙어있으니까. DRAM은 수십나노초면 읽기쓰기가 가능했지만, 플래시메모리는 전압펌프를 가동해서 절연 공간에 전하를 집어넣을 때까지 수십~수천us가 필요했다. 대신, 전자가 갇혀있게 되어 전원을 꺼도 메모리가 유지됐다. 플래시메모리는 Block 단위로 데이터를 읽고 쓴다. Block 단위로 데이터가 뿅 하고 사라지기 때문에 ‘플래시’메모리라 부른다. 근데 플래시메모리는 공정상의 한계때문에, 칩 내의 block 중 1% 이상이 결함을 갖고 나온다. 그래서 에러 정정 장치를 붙여서 사용해야 했다. 플래시메모리 제조사는 ‘0번 block은 결함이 없을 것’과 ‘칩 내 불량이 몇개 이하일 것’만 보장해 판매한다. 플래시메모리는 컨트롤러와 결합하여 운영되며, 컨트롤러 안에 작은 소프트웨어를 탑재해 문제들을 해결한다. 값싼 원가의 대가로 얻은 낮은 질을 소프트웨어로 해결하려는거다. NAND, NOR가 있는데 NOR는 Block이 작다. 수 바이트정도 된다. NAND는 Block이 수kB 수준이다. 그래서 NOR는 각 방에 연결되는 도체를 더 많이 깔아야 하고, 그래서 더 비싸다. 결국 반응속도는 NOR가 더 빠르고, 가격은 NAND가 더 싸다. 쓰기는 NAND가 한번에 뭉텅이로 써서 더 빠르다. 읽기가 NOR가 더 빠른거다. 후지오 마스오카 박사는 플래시메모리로 마그네틱 기반 저장소를 모두 대체하고 싶어했으나, 도시바는 그 아이디어에 관심이 없었다. 인텔이 더 먼저 행동했다. 인텔은 CPU 사업을 하고 있었는데, CPU가 구동하기 위해서는 일련의 구동 코드들 및 BIOS를 마더보드에 저장해둘 외부 공간이 필요했다. 그때까지의 컴퓨터들은 이 코드들을 PROM, EPROM, EEPROM에 저장해 왔다. 하지만 PROM은 물리적으로 퓨즈를 끊어 0과 1을 구분하는 방식이었기 때문에, 실수하면 칩을 못쓰게 될 위험이 존재했다. EPROM은 데이터를 적기는 쉬운데, 지우려면 자외선을 조사하는 과정이 필요했다. EEPROM은 사용하기는 쉬웠지만 용량이 너무 작았다. 플래시메모리는 이런 단점들이 없단느걸 인텔이 알아챘다. 이 코드들은 컴퓨터 부팅시에만 사용되며, 프로그램 성능에는 영향을 주지 않ㄴ느다. 용량은 수십kB ~ 수십MB만 있으면 된다. 데이터를 바꾸는 일은 BIOS 업데이트를 할 때 정도고, 사용자 환경과 분리되어 운영되기 때문에 데이터 접근 성능에 대한 걱정도 필요없다. 물론 매년 필요한 용량이 증가하긴 했지만 얼마 안됐다. 결국, 그냥 간단하게 사용 가능하고 비휘발성이면 되는거였다. 여기에 NOR 플래시메모리가 쓰이게 된다. 읽는게 빠르니까. NAND 플래시의 메모리 특성이 더 안좋았기 때문에, 컨트롤러는 NAND 플래시용 컨트롤러가 더 컸다. 그래도 가격은 NAND가 더 쌌다. 인텔, 암드는 NOR 플래시메모리에 집중했다. 마더보드 위의 ROM을 대체하기 위함이었으니까. 도시바는 후발주자가 되어버렸는데, 1991년 낸드플래시 개발을 완료하고 시장 진출을 선언했다. 그리고 1992년, 삼성전자에게 낸드플래시 기술을 라이센싱해주게 된다. 그 후, 삼성전자는 미래 디지털 제품 기술 확보를 위해 DRAM, NAND, CDMA에 역량을 쏟아붓겠다고 선언한다. 지금 보면 꽤 성공했다. Sandisk는 메모리를 직접 제조하지는 않고, 플래시메모리에 들어갈 컨트롤러만 만들었다. 당시 이름은 Sundisk였는데, 나중에 Sandisk로 바꾼거다. 핸드폰도 발전하기 시작해서, 저장장치가 필요해졌다. 플로피디스크는 성능 향상이 힘들었고, 신뢰하기 힘든 매체가 사용됐다. CD-ROM은 한번 적은 내용을 고치기 어려웠다. 휴대용 HDD는 크기가 너무 컸고, 충격으로 데이터를 잃어버릴 수도 있었다. 여기에도 플래시메모리가 쓰이게 된다. 도시바는 SD(Secure Digital)라는 새로운 규격을 만들었고, 이스라엘의 M-systems는 최초의 휴대용 USB를 만들었다. 자신들의 컨트롤러에 샌디스크의 낸드를 붙였다고 한다. 근데 아까 샌디스크는 컨트롤러만 만들었다며? 뭐지 당시 USB 용량은 8~16MB라 CD-ROM보다는 용량이 훨씬 작았지만, 휴대성과 안정성은 훨씬 좋았다. 삼성전자는 DRAM 시장에서 승리한 후, 낸드플래시 비트당 가격이 DRAM보다 높은걸 보고 낸드플래시에 대규모 투자를 하게 된다. 낸드플래시가 DRAM보다 싸지게 됐고, 가격경쟁이 심해지기 시작했다. 이때쯤, 애플은 iPod보다 발전한 iPod Nano를 만들고 싶어했다. 원래 아이팟에는 소형 HDD가 있었는데, 더 소형화하기 위해 플래시메모리를 쓰게 됐다. 당시 NOR 대비 NAND 플래시의 칩당 밀도가 10배정도 높아서, 애플은 NAND 플래시를 쓰게 됐다. 이때, 삼성전자의 NAND플래시를 쓰게 된다. 이 아이팟 나노가 엄청난 히트를 치게 되어, CD나 HDD 기반 음악감상 장치들이 도태되고 플래시메모리 기반 제품들이 많이 나오게 된다. 결국 플래시메모리가 많이 필요해졌고, 삼성전자는 엄청난 이득을 보게 된다. 다른 낸드 제조사들도 이득을 봤지만, 삼성전자에게 특히 유리했던 점은 삼성전자는 도시바와 달리 DRAM 사업부, 파운드리사업부도 있었다는 점이다. 아이팟 나노는 모바일 SD램과 일종의 CPU인 미디어 프로세서도 필요했는데, 삼성전자는 이걸 다 만들어줄 수 있었다. 그래서 애플에게 미디어 프로세서를 공급하던 팹리스 업체인 PortalPlayer는 2006년 애플과의 계약이 종료됐음을 발표하고, 이후 엔비디아에 인수된다. 그 계약들이 다 삼성전자한테 넘어간거다. 물론 모든 낸드 제조사들이 이득을 본건 아니었다. 일본의 르네사스 반도체는 2010년 12월, 더이상 낸드 개발을 하지 않겠다고 선언한다. 2005년, 애플 아이팟 나노 덕분에 돈이 많아진 삼성전자는 갑자기 새로운 사업을 발표한다. 1.8인치, 2.5인치 SSD시장에 진출하겠다는거다. SSD 1TB, HDD 10TB여도 SSD 1TB를 고를 수 있으니까. 10TB까지 필요 없잖아? 그리고 SSD는 내충격성, 성능이 높으니 충격 방지 설계 비용, CPU에 소모될 예산을 아낄 수 있으니 전체 비용이 줄고 경량화가 가능하다. 그리고 HDD컨트롤러보다 SSD컨트롤러가 더 만들기 쉬웠다. 기계적인거 신경 안써도 되니까. 2006년: 삼성전자가 최초의 양산형 SSD를 699달러에 발표한다. 32GB짜리였다. 이후 비슷한 SSD들이 컨트롤러 업체들에서 등장한다. 플렉스터, 퓨전IO, OCZ, Silicon Motion 등 업체들이었다. 이 팹리스 업체들은 낸드를 사와서 자사 컨트롤러에 붙여 팔기 시작했다. UBER: 수정 불가능한 에러가 발생할 확률. Uncorrectable Bit Error Rate 삼성전자는 DRAM때는 원가경쟁으로 경쟁업체들을 말려죽였고, 낸드 시장에서는 낸드와 다른 하드웨어들을 결합한 솔루션(노트북, 아이팟나노 등)들을 제공했다. NOR 대신 NAND쪽을 선택한 것도 옳은 선택이었다. (107페이지 삼성전자 연표) ISA: Instruction Set Architecture. 인텔, AMD CPU는 인텔의 ISA인 x86, x86-64를 쓴다. ARM CPU는 ARM V8을 쓴다. exe: 실행 파일. 사용자가 직접 클릭해서 실행할 수 있는 파일 dll: 동적 라이브러리. 다른 실행파일이 불러와서 사용할 수 있는 형태의 파일. Compiler별로 프로그램 성능이 많이 달라질 수 있다. How are you?를 ‘요즘 어떻게 지내니?’, ‘요즘 어때?’ 등으로 번역 가능한 것처럼 Compiler, 기계어는 인간이 다루기 어렵다. 1977년 발사된 보이저1,2호의 메모리는 68kB였다. 보이저의 제어 프로그램은 기계어와 포트란으로 만들어졌다. 이 정도는 인간이 기계어로 만들 수 있다. 근데, 예를 들어 윈도우 7은 4기가짜리다. 이런건 인간이 기계어로 못만든다. 옛날 컴퓨터에는 확장 슬롯 자체가 없거나, 공인된 하드웨어만 붙일 수 있었다. 1981년 IBM이 XT라는 컴퓨터를 만들었는데, 서드파티에게 컴퓨터를 오픈하고, 보조기억장치로 HDD와 플로피디스크를 갖추고, 확장 가능한 램 슬롯을 가진 구조였다. CPU는 인텔 8088, OS는 MS-DOS였다. 당시에는 컴퓨터 관련 표준이 거의 없어서, 당시 컴퓨터들은 CPU가 동일한 ISA를 사용하더라도, 아니면 아예 같은 CPU를 사용하더라도 프로그램이 제대로 안돌아가는 경우가 많았다. 요즘은 어느 회사 RAM을 쓰든, 어느 회사 GPU를 쓰든 잘 작동하잖아? 그때는 공식적으로 인정된 부품만 써야 했다. 그래서 IBM PC가 혁신이었다. 맘대로 하드웨어 갈아끼울 수 있다니! 당시 인텔8088은 가격이 저렴하고 성능이 뛰어났다. 그리고 인텔 x86-16은 CISC 명령어 체계를 사용하고 있었는데, x86-16에는 범용 레지스터(CPU 내부 연산 처리를 위한 초고속 저장공간)가 훨씬 많아서, 코드를 잘 짜면 경쟁자들을 압도할 수 있었다. 그리고 8088은 내부로는 16비트를 사용해서 성능을 끌어올렸지만, 외부로는 8비트 버스를 갖고 있었다. 당시 시장에 나온 하드웨어들은 다 8Bit였기 때문에 이게 나았다. 그 후 모든 회사들이 달려들어 IBM PC를 리버스엔지니어링 했다. 모두가 IBM PC에 맞는 규격의 하드웨어, 소프트웨어를 만들게 되었다. 그 전에는 다 규격이 달라서 거래처가 바뀌면 싹 다 다시 해야 했다. 하지만 이제는 IBM PC 규격이 표준이 되어 이거대로 만들면 되는 상황이 됐다. 그래서 IBM은 이득을 봤나? 그건 아니다. IBM도 경쟁자 중 하나가 되어버리고 말았다. 그래서 별로 이득 못봤다. 그래도 PC 사업은 IBM의 사업들 중 하나였기 때문에 큰 타격은 없었다. IBM은 지금도 메인프레임 시장의 절대자다. 이 시점에, 인텔은 두가지 결정을 내린다. 1. 일본과의 DRAM 경쟁을 포기하고 CPU에 집중한다. 2. 프로세서 생산을 타사에 맡기지 않는다. 인텔을 왕좌에 올린 8088 프로세서를 위탁 생산하던 회사는 AMD, NEC, 후지츠 등 10개 가까이 되었다. 이렇게 아웃소싱을 돌려서 웨이퍼 공장을 작게 유지하고 있었지만, Value chain의 상당 부분을 다른 회사들과 나눠 가져야 했고, 기술이 새어나갈 위험도 있었다. 인텔은 자기 제품을 타사에 맡기지 않게 되며, 전세계 PC CPU의 설계부터 제조까지 모든 Value Chain을 장악하게 됐다. 인텔이 자체 생산으로 물량을 돌릴 때, AMD는 인텔의 x86 ISA만 라이센싱하고 설계는 자체적으로 하기로 했다. 인텔이 CPU 시장을 먹어버리긴 했지만, CPU는 수명이 길기 때문에 인텔은 과거 제품보다 더 좋은 제품들을 계속 만들어내야 했다. 게다가 AMD도 쫓아오고 있었다. 그래서, 독점시장이어도 인텔은 가만히 있을 수가 없었다. 데너드 스케일링: 미세공정 발전으로 면적당 트랜지스터가 늘어나도 전력 소모는 늘어나지 않는다 인텔은 전력 소비량을 유지하며 더 많은 부품을 CPU에 빽빽히 꽂아넣어 성능을 늘릴 수 있었다. 또, 약간 밀도를 낮춰 동작 마진을 주는 것으로, 더 높은 clock으로 동작할 수 있게 했다. 인텔은 과거 CPU에서 동작하는 프로그램들은 모두 상위 CPU에서도 동작하게 만들었다. 뭐 ISA를 갈아엎는다거나 이런 행동을 하지 않은거다. 컴퓨터가 발전하고 메모리 용량이 커지자, 인텔은 x86-16에서 x86-32로 넘어갔다. x86-16은 64kB 이상 메모리를 인식할 수 없기 때문이다. 인텔 80386에서 이 변화가 발생했는데, 옛날 프로그램들 잘 돌아가도록 ‘Virtual 8086 mode’를 만들어놨다. 지금도 가상환경에서는 8086 명령어 다 돌아간다. 8086 프로세서는 트랜지스터 3만개인데, 요즘 프로세서들은 1억개 이상이다. 사실 아예 프로세서를 집어넣을 수도 있는거다. 이런 호환성 유지는 쉬운 일이 아니다. 요즘 잘 안쓰는 명령어도 들고 가야 하니 웨이퍼 면적, 전력 낭비가 어느 정도씩 발생하게 된다. 인텔은 이걸 감내하면서도 성능을 향상시킬 방법을 찾아나갔다. 성능을 높이는 한가지 방법은 clock을 높이는 것이었다. 최신 제조장비를 도입해서 미세공정 품질을 높이고, 이를 통해 cell의 크기를 줄이고 누설전류와 발열을 억제해 CPU의 최대 스위칭속도를 높이는거다. 이걸로 clock이 2배 높아지면 CPU 성능도 2배가 된다. 하지만 switching 속도가 너무 빨라지면 원래 같이 동작하는걸 전제로 만들었던 회로가 따로 동작할 수도 있다. (그림) 1GHz면 30cm 이동하니까, 길이가 30cm 넘어가면 다른 clock이 걸리게 된다. 4GHz면 7.5cm 그래서 한 덩어리였던 하드웨어 블록을 여러 덩어리로 쪼개야 한다. 이게 파이프라인이다. 다른 방법은 아키텍처를 넓히는 것이다. 인텔 Sandy Bridge, Haswell, Skylake 등이 이런 아키텍처의 코드명이다. 한개 CPU core에서 병렬로 동시에 처리될 수 있는 작업들이 생기는데, 이런걸 ‘ILP(Instruction Level Parallelism)’이라 부른다. 이 ILP들을 찾아내 성능을 높이는 CPU를 Superscalar Processor라 한다. 물론, 이 ILP 찾는건 아주 빠르게 이뤄져야 하기 때문에 만들기 Superscalar Processor를 만드는게 어렵다. 하지만 이걸 해내면, clock도 안바꿨는데 CPU가 빨라진다! 마지막 방식은 새로운 명령어를 ISA에 추가하는거다. 특정 상황에서 아주 빠르게 동작할 수 있는 신규 명령어, 그걸 위한 하드웨어들을 추가하는거다. 예시로는 인텔 펜티엄의 MMX, 펜티엄3의 SSE, 코어 시리즈의 AVX등의 SIMD(Single Instruction Multiple Data) 명령어들이 있다. 한개 Instruction으로 여러개 숫자열 상태가 바뀐다. 근데 이 방식은 다른 두 방식과 다르게, 프로그래머가 이 새로운 명령어를 써줘야 동작이 빨라진다. 그리고, 과거 CPU들은 이 새로운 명령어를 이해 못하기에 프로그램을 돌릴 수 없게 된다. ex) 386은 MMX가 적용된 프로그램을 못돌린다. 새로운 명령어가 추가되면 Compiler를 새로 만들어야 하는데, 인텔이 스스로 Compiler(ICC)를 만들어 팔았다. 그래서 프로그램 개발이 그렇게 어렵지 않았고, 어차피 이런거까지 써야 하는 하이엔드 성능이 필요한 프로그램이면 옛날 CPU에서 안돌아가는게 문제가 되지 않았다. 그래픽 작업이 병렬인 이유: 모니터에 표시되는 픽셀들은 서로 상호작용하지 않기 때문에 병렬이다. 인텔은 램버스와 협력해 RD램을 만들어 다시 한번 메모리 시장에 들어가 PC 플랫폼에 대한 영향력을 늘리려 했지만, 메모리 기업들과의 원가 싸움에서 또 털리고 만다. 20세기 말, 컴퓨터 성능과 용량이 기하급수적으로 늘어나고 있었다. 32비트 기반이었던 x86은 한번에 접근 가능한 메모리 영역이 4GB였는데, 메모리 용량이 점점 커져서 곧 메모리에 한번에 접근하지 못하게 될 위기였다. 접근 자체는 가능한데, 여러 단계를 거쳐야 하니 효율성이 떨어지는 상황이었다. 인텔은 CPU를 64비트로 바꾸고 최대 메모리 주소를 늘려야 하는 상황이었는데, 20년간 유지해온 하위호환 정책에 대해 다시 생각해보게 됐다. 신형 칩들 clock은 계속 올라서 2000년에는 1GHz를 넘었고, 실리콘 웨이퍼가 버틸 수 있는 최대 clock인 4~5GHz 근처로 빠르게 다가가고 있었다. 그리고, 만들어진지 20년이 넘은 인텔의 x86은 각 명령어마다 길이가 달랐기 때문에(CISC), 그 후 생긴 모든 명령어의 길이가 같은(RISC) ARM 프로세서보다 비순차 수행에 적합하지 못했다. 비순차수행에서는 명령어들의 순서를 바꾸는 과정이 필요한데, 명령어들의 길이가 같으면 쉽지만 다르면 어렵다. 그리고, CPU가 추출해낼 수 있는 동시 수행 가능성도 한계였다. 인텔은 여기서, x86을 포기하고 앞으로 나아가기로 한다. 이때 HP와 협력한다. HP는 1980년부터 RISC도 CISC도 아닌 대안 아키텍처를 고민하고 있었다. 2001년, 인텔은 HP와 협력하여 새로운 CPU 아키텍처인 Itanium을 발표했다. 기존 CPU들은 비순차 수행 방식으로 성능을 끌어올렸는데, 인텔의 새로운 CPU는 그런 복잡한 비순차 수행 엔진(명령어 배치 장치)을 만들 웨이퍼 면적을 추가 연산장치에 투자하는 방향으로 갔다. 그럼 Itanium은 동시에 명령어 수행을 못하나? Compiler에서 동시에 실행할 수 있는 명령어를 모아서 실행파일을 만들게 했다. 이러면 Compiler의 부담이 커지긴 하지만, Compiler는 하넌 고생해서 만들어두면 수십년 쓰니까. 이렇게 비순차 수행 엔진 치우고 그 자리에 연산장치를 놓으면 성능 좋아지고, 원가 내려가고, 전력소모가 내려갈 것이다! 근데 이렇게 하려면, compiler는 CPU가 바뀌면 그때마다 매번 새로운 실행파일을 만들어줘야 한다. ex) CPU에 원래 연산장치가 4개였는데, 신형은 6개가 됐다 -&gt; 기존 실행파일 쓰면 2개가 논다. 그래서 실행파일을 새로 만들어줘야 한다. 기존 슈퍼스칼라 프로세서였다면 알아서 새로운 2개 잘 썼을텐데! 소프트웨어 회사들은 Itanium 아키텍처를 쓰려면 CPU마다 다른 실행파일을 만들어야 한다. 하위호환성도 있기는 했지만, 영 좋지 않았다. 기존 x86 실행파일 -&gt; CPU 에뮬레이터 -&gt; IA-64(아이태니엄 언어) -&gt; CPU 본체 과정으로 실행해야 했다. 이렇게 복잡하게 실행하니, 당연히 성능이 기존 대비 아주 안좋았다. 결국 아이태니엄은 잘 안팔렸다. 2003년, 지금까지 인텔의 ISA를 라이센싱해서 사업하던 AMD가 완벽한 하위호환을 유지하며 64비트 확장을 성공시키고, 성능까지 높인 새로운 명령어 세트인 AMD 64 (= x86-64)를 내놓고, 서버용 CPU 슬렛지해머를 출시했다. 이 CPU는 AMD의 서버용 CPU 라인업인 Opteron에 포함되는 제품이다. Opteron은 x86으로 구성된 기존 명령들을 완벽히 수행할 수 있었다. 인텔 아이태니엄 생태계가 자리잡지 못한 상태에서, 기업들이 아이태니엄 말고 x86-64로 갈아타버리면 인텔은 망하고 말 것이다. 인텔은 위기를 느끼고, x86-64를 받아들이며 CPU 개발계획을 크게 전환했다. 그럼에도, 아직 HP와의 계약이 남아 아이태니엄을 완전히 접을 수는 없었다. 그래서 이제는 인텔이 AMD에 x86-64의 사용료를 지급해야 하게 됐고, 지금까지도 내고 있다. 짐 켈러가 AMD에 있을때 만든게 AMD64랑 하이퍼트랜스포트다. 하이퍼트랜스포트는 멀티코어 프로세서의 핵심 기술이다. 아이태니엄은 인텔에게 계속 비용을 발생시켰다. HP에게 HP-UX 시스템용 CPU를 공급하는 장기 계약을 했기 때문이다. 2004년, 인텔의 신형 CPU인 펜티엄4 프레스캇이 나왔는데, 초기에는 x86-64 지원 안한다고 하더니 얼마 후부터 갑자기 x86-64를 지원한다는 문구를 달고 나오기 시작했다. 그 사이에 x86-64 지원하도록 재설계했다고 보기에는 너무 짧은 기간이고, 애초에 x86-64를 위한 공간을 만들어뒀던 것으로 보인다. AMD는 이후 듀얼코어 맨체스터를 개발하며 시장의 우위를 점했고, 인텔은 비순차 처리 엔진보다 clock 상승에 집중했다가 clock에 비해 낮은 성능을 냈고, 발열도 심해져 펜티엄4 프레스캇은 ‘프레스핫’이라는 별명까지 얻었다. 2005년 AMD의 시장 점유율은 40%가 넘었으며, CPU 성능은 최저가 라인업부터 최고가 라인업까지 모두 AMD가 인텔보다 높았다. 고가 라인업에서는 펜티엄이 애슬론FX에 밀렸고, 저가에서는 셀러론이 셈프론에 밀렸다. 인텔은 정신차리고 ‘펜티엄’이라는 브랜드를 폐기하고, ‘인텔 코어2 프로세서’라는 브랜드를 내놓고, 2006년 신형 마이크로아키텍처 Conree를 내놓는다. 인텔의 Conree는 clock이 2.4GHz밖에 되지 않았지만, 성능이 상당히 좋았다. Conree는 33만원이었는데, 성능은 당시 100만원이 넘던 AMD의 최고급 CPU인 애슬론 64 FX62를 밀어냈다. 인텔이 개발 방향을 clock 중심에서 아키텍처 확장으로 전환한 덕분이었다. 인텔이 정신차리자 AMD가 시장점유율을 잃어버리기 시작했다. 게다가 TLB 버그라는 하드웨어 결함까지 발견되며 시장을 거의 잃어버린다. 2000년대 초, AMD와 인텔 모두 한개 코어 성능을 키우는 데에는 한계가 있다는 것을 알고 있었다. 그래서 CPU 하나에 여러개 코어를 박는 멀티코어를 만들어야 했다. 근데 문제는, 코어끼리 통신하는 데에 시간이 엄청 걸린다는거다. 공유해야 하는 정보가 CPU 공유 캐시에 있으면 명령어 100개 이상, D램에 있으면 1만개 이상이 필요하다. 그래서 멀티코어가 어렵다. 통신이 오래 걸린다. 4코어 사용 프로그램을 열어보면 4개 코어가 1~100% 사이에서 다양한 사용률을 보인다. 다른 코어와 협업하기 위해 답장을 기다리는 동안 아무것도 못하니까. AMD는 승부수를 던져보기로 했다. AMD가 쓰던 아키텍처인 K10은 이미 수명을 다해가고 있었고, 대형 CPU 설계에서 인텔의 노하우를 이기는건 어려우니 계속 커져가고 있던 CPU 코어의 크기를 줄이고, CPU 코어의 갯수를 늘리자는 것이었다. 이렇게 하면 비순차 실행장치의 크기도 줄어들고, 연결되어야 하는 회로의 갯수도 줄어들어 설계의 어려움이 감소했다. 비순차 실행장치의 성능은 크기에 정비례하지 않으니, 크기 감소로 인한 코어당 성능 감소가 작을 것이라고 생각한 것이다. 프로그래밍 시장도 점점 많은 코어를 활용하는 방향으로 가고 있었고, 애초에 서버 시장에서는 코어당 성능보다 코어 갯수가 중요했다. AMD는 큰 코어를 작은 코어 2개로 쪼개고, 일부 하드웨어를 공유하는 ‘모듈’개념을 도입했다. 이걸 CMT(Cluster Multithreading)라 부른다. 이렇게 개발한 마이크로아키텍처가 Bulldozer였다. Bulldozer는 INT가 2개고, DEC과 FP를 공유한다. *CPU 안의 DEC, DISPATCH, INT, FP가 뭔지 확인 인텔은 암달의 법칙(Amdahl’s law)에 주목했다. 프로그램 실행 성능은 단일코어 성능과 멀티코어 성능 중 나쁜 쪽이 결정한다는거다. 한 코어가 아주 빠르다고 해서 전체가 빠른게 아니다. CPU 코어들이 나눠서 할 수 없는 작업들도 있을거니까. 그래서, 인텔은 코어를 늘리기보다는 코어당 성능을 올리고, 한개의 고성능 코어가 다른 작업도 처리할 수 있게 했다. 이 방식은 SMT라 불렸다. 이 분야는 IBM이 1968년에 연구했던 분야다. 인텔은 이걸 hyperthreading이라 불렀다. 이렇게, 두 회사의 CPU 설계 방향이 크게 갈리게 되었다. 2011년, 최초의 Bulldozer 마이크로아키텍처 CPU인 4모듈, 8코어 잠베지가 출시됐지만, 성능이 아주 안좋았다. 인텔은 4개 코어로 구성된 차기 마이크로아키텍처 샌디브리지를 발표했다. 이전 세대 대비 CPU 단일코어 성능이 30%까지 높아져 있었다. 잠베지랑 샌디브리지는 단일코어 성능이 2배 가까이 차이났다. 이건 단일 코어에서만 돌아가는 옛날 프로그램을 실행하면 성능 차이가 2배 난다는 이야기다. 그럼 서버용 CPU 시장에서는 AMD가 이겼나? 서버는 코어 수가 중요하다고 했잖아. 서버에서 멀티코어가 중요한건 트래픽이 몰릴때 이야기고, 평소에는 서버용 CPU도 간단한 일만 하니까 이때는 단일코어 성능이 중요하다. 서버 PC들은 굳이 AMD로 넘어갈 이유를 찾지 못했다. AMD의 서버시장 점유율은 계속 하락해서, 2012년에는 거의 끝장났다. 그 후에는 개발되는 서버용 프로그램들은 모두 인텔 CPU에 최적화되고 호환성도 맞춰져 재진입이 어려워졌다. AMD가 거의 망해버리자, AMD 칩 제조를 담당하던 파트너사 글로벌파운드리즈도 경영 위기에 들어간다. AMD의 암흑기는 CMT를 포기하고 인텔의 SMT를 적용한 Zen이 등장하고서야 끝났다. 인텔은 이렇게 두번의 위기를 극복해냈다. 인텔의 강력한 단일코어 성능은 모바일 혁명 이후 기세가 붙은 ARM의 서버 시장 도전을 막아내는 강력한 방패로 작용했다. 인텔은 마이크로소프트와 더불어 반도체시장을 독점하는 기업들 중 하나였지만, 눌러앉이 않고 계속 제품개발을 했다. 162페이지에 인텔 연대표 삼성전자, 인텔은 IDM(Integrated Device Manufacturer)이다. 하나의 회사가 반도체 설계, 생산을 모두 하는게 IDM이다. 설계에 쓰는 프로그램은 EDA라 부른다. 고성능 로직은 마스크가 20장 이상 필요하기도 하다. 삼성전자는 메모리, 인텔은 CPU회사라 미세화 및 성능 향상의 이득이 그대로 들어나는 분야에 있다. 그래서 1등기업이어도 계속 연구개발에 투자해야 한다. CPU는 수명도 길어서, 계속 더 좋은걸 만들어 팔지 않으면 굶어죽는다. MCU도 CPU와 비슷한 구조를 갖고 있지만, 고성능 연산을 포기하고 더 작은 연산장치나 간소화된 메모리 시스템을 갖고 있다. SSD, HDD 안에도 MCU가 있다. MCU가 하는 일은 간단하다. ‘A버튼이 눌리면 어떻게 행동한다’ 정도다. 자판기 생각하면 된다. 그리고 자판기에는 고성능 연산이 필요 없다. 조금 빨라져서 뭐하나. MCU는 성능 요구치가 낮으니 단가도 낮다. MCU는 칩 하나에 메모리, CPU 등 전부 합친거고, DSP는 MCU보다는 비교적 낮은 처리능력을 가진 로직이다. 하여간 얘네 시장규모는 메모리 시장의 20% 정도다. 이런 제품들은 가격도 높지 않고, 추가 성능에 대해 고객들이 더 많은 돈을 지급할 용의도 없다. 그래서 회사들이 굳이 설계~제조라인을 완벽히 갖추고 원가와 성능을 쥐어짜낼 이유가 없다. 반도체시장 초기에는 많은 회사들이 자신의 fab을 갖고 있었지만, 미세공정이 점점 힘들어지자 회사들은 고민에 빠졌다. 이제 미세공정 구현 능력과 원가 경쟁이 중요해졌는데, 이걸 못따라간 회사들은 공장을 포기하고 팹리스 회사가 되거나, 아예 매각/흡수합병되는 회사들도 많았다. 소형 로직/마이크로컨트롤러 제조업체들의 경쟁력은 설계 노하우 및 인력이었고, 용도가 제한적이거나 특수했기 때문에 매각이나 인수가 쉬웠다. 근데 메모리 업계는 전세계 회사들이 같은 제품을 서로 원가 깎아가며 만드는 경쟁이라, 메모리 업계에서는 부도난 회사의 경쟁력 없는 노하우를 원하는 인수자가 없다. 2011년, 일본 메모리업체 엘피다는 10조 부채를 남기고 파산했다. 하여간, 이렇게 팹리스/파운드리 구조로 시장이 변했다. 각자의 이득은 p.174에 있다. 팹리스: 공장에 투자하는 부담이 사라졌다. 파운드리: 공정에 집중할 수 있다, 과거 공정을 계속 쓸 수 있다. ARM은 칩 설계의 일부만, 또는 칩 설계 없이 ISA만 팔았다. 설계하다 막히는 곳 있으면 우리꺼 사다 써라! 약간 팹리스들의 팹리스가 되고 싶어 한다. TSMC는 첨단~옛날 공정 다 유지한다. 100nm 넘는 공정을 지금도 갖고 있다. 시장이 작아서 새로운 설계를 하는게 부담인 제품들도 있으니까. 원래는 TSMC 첨단공정이 인텔보다 훨씬 안좋았는데, 나중에 역전되게 된다. 21세기에 들어서자, 데너드 스케일링이 흔들리기 시작했다. 수십nm 수준으로 미세화되자, Cell과 배선 사이의 parasitic effect의 영향이 늘어나 leakage current가 증가하게 됐다. 전성비(전력 대 성능비)가 잘 늘어나지 않게 됐고, 반도체의 열밀도가 올라가게 됐다. 이제 반도체 회사들은 집적도를 올릴 때마다 동일 면적에서 일어나는 발열이 커지는 것을 감내해야 했다. 데너드 스케일링 말대로 집적도 올리면 트랜지스터당 사용하는 전력이 감소하는게 맞긴 한데, 이제는 leakage current가 늘어나서 옛날만큼 감소하지도 않고, 옛날이랑 같은 면적이어도 열이 더 많이 발생하게 됐다. 그래서 반도체 특성 관리, 냉각, 전력공급이 어려워졌다. 그래서 칩 전체를 가동하는게 점점 힘들어졌고, 당장 필요없는 회로는 잠깐 꺼야 할 수도 있었다. 노광장치의 발전도 느려지기 시작했다. 노광장치의 핵심은 빛의 파장을 최대한 줄여 해상도를 높이는 것이었고, 거대한 볼록거울로 그 짧은 파장의 빛을 모으는 방식이었다. 근데 문제는, 빛의 파장이 짧아질수록 흡수율이 올라간다는 것이었다. 거울에서 반사가 잘 안되고 뚫고들어가버린단 뜻인듯? X선이 그렇듯이 말이다 근데 밑에서 렌즈도 못쓴다는거 보니까 진짜 말 그대로 흡수인가? 하여간, 이제 광원에서 발생한 빛이 웨이퍼에 전달되는 비율이 내려가고 있었다. Hglamp -&gt; KrF -&gt; ArF로 바뀌며 짧은 파장 광원을 열심히 찾아다니긴 했지만, ArF 레이저로는 20nm 근처 lithography가 불가능했다. 그래서 광원 파장은 그대로 두고, 렌즈 아래에 굴절률 높은 액체를 배치해 광원의 파장을 줄이는 Immersion(액침) 방식을 사용했다. 이렇게 해서 얼마동안은 노광기를 발전시킬 수 있었다. 하지만 이후에는 EUV 영역으로 나아가야 했는데, EUV는 볼록렌즈를 쓸 수 없을 정도로 흡수율이 높다. 파장이 너무 짧아서 인간이 마주치는 모든 물질이 극자외선을 흡수할 수 있다. 그래서 EUV 장치 내부는 거의 완벽한 진공이어야 하고, 볼록렌즈 대신 특수하게 설계된 다중 반사판을 써야 한다. 근데 뭐 어떻게 만들어도 loss가 심해서, EUV 장비 개발은 계속 늦어지고 있었다. 그 동안에도 반도체 회사들은 미세공정이 필요했다. 그래서 ‘멀티 패터닝’이라는걸 사용했다. 한개 패턴을 생산할 때 한개 마스크 대신 여러개 마스크를 써서 가느다란 배선을 만드는 방식이었다. 근데 이렇게 하면 필요한 마스크의 수가 엄청나게 많아졌고, 사실 그렇게 정확하지도 않았다. lithography가 정확하지 않으면 제품 성능이 균일하지가 못하니, 결국 EUV를 쓰긴 해야 했다. 그래서 EUV 장비를 사오면? EUV는 loss가 심해서 출력이 아주 낮다. 그래서 여러개를 사와야 하는데, 개당 2천억원 이상이다. 그니까, 이거 안사오면 성능/품질에 문제가 생기고, 이거 사왔다가 제대로 가동 못하면 진짜 망하는거다. 삼성전자는 칩 전체에 EUV를 쓰지 않고, 고해상도가 필요한 핵심 회로에만 EUV를 썼다. EUV만 쓰면 웨이퍼 처리가 너무 느리기 때문이다. 출력이 낮으니까. 전체를 EUV로 찍는것보다, 일부만 EUV로 찍는게 필요한 기계 수도 적었다. 인텔은 칩간 2차원 연결을 지원하는 EMIB 기술과 3차원 연결을 하는 포베로스 계획을 발표했다. 미세공정의 영향을 덜 받는 I/O는 구세대 공정으로 만들고, 고성능 인터커넥트인 EMIB로 칩 사이를 연결하는 Chiplet 방식을 함께 도입하겠다는거다. CPU, 그래픽 등 성능이 중요한 부분에는 고성능의 10나노 공정, 입출력단이나 메모리 컨트롤러 등에는 구세대 공정을 사용하고 다 연결하는거다. 영어로는 monolithic chip 대신 chiplet들의 결합이다. 모든 부분에 비싸고 느린 공정을 쓸 필요는 없으니까. 구세대 공정 재활용도 가능하고. 하여간, 이렇게 해서 노광장비와 함께하는 반도체 장비들의 가격도 함께 엄청나게 올라버려서, IDM, 파운드리 분야에 새로운 회사가 들어오는건 어렵게 되었다. 설계 분야에서도 어려움이 생겼다. 트랜지스터가 많아지긴 했는데, 이걸 다 써먹는 것은 어려운 일이었다. 일단 코어 갯수를 늘렸는데, 코어를 다 활용할 수 없을 정도로 코어가 많아지자 내장 VGA를 넣게 되었다. 코어가 많다고 왜 못쓰냐? 식당에 손님이 8명 올때 요리사 1-&gt;2명은 차이 크지만 2-&gt;16명은 의미가 없다? 책 다시 봐야될듯 여기는 설계는 공정의 한계도 어느정도 떠맡아야 했다. 미세공정 때문에 SRAM의 신뢰성이 급격히 떨어져서, 전압의 순간적인 변화, 온도에 의한 특성 변화, 우주 방사선 등에 의해 값이 바뀌는 문제들이 생겼다. 미세 공정때문에 Cell의 크기가 줄어들었기 때문에, 1개 bit를 표현하기 위해 쓰는 전자 수도 줄어들었다. 옛날에는 전자가 50개 이상이면 1, 아니면 0이었던게 전자 10개로 기준이 바뀌는 식이었다. 이러면 외부에 의해 전자가 1개 추가됐을때, 오차 2%였던게 cell이 작아졌기 때문에 10% 오차가 되는거다. 당연히 오차가 더 생기게 됐다. SRAM 말고, DRAM도 회로 미세화에 의해 Rowhammer라는 결함을 겪게 된다. 특정 위치 cell에 접근할 때 흐르는 전류때문에 인접한 데이터들이 변조되는 문제다. 이것도, 미세화 때문에 한 비트를 저장하는 데에 쓰이는 Cell 크기가 줄어들어서 생긴거다. 이 문제를 해결하는 데에도 설계 변화와 추가 트랜지스터가 필요했다. 로직 관점에서는 ECC(Error Correnction Code)로 문제를 막았다. 당연히 ECC를 구현하려면 추가 트랜지스터, 웨이퍼를 써야 했다. 설계 관점에서는 미세공정 진행하면서도 비트당 저장되는 전하의 양을 유지해야 했다. -&gt; Cell을 위로 쌓게 되었다. 높은 탑을 쌓는건 어려웠지만, 그래도 DRAM은 로직보다 설계가 간단한 편이라 그럭저럭 용량이 늘어나고 있었다. 그리고, clock이 빨라짐에 따라, 하나의 회로 안에서 신호가 제대로 전달되지 않는 문제들이 점점 커지게 되었다. 회로 스위칭이 전달되려면 전류가 그만큼 빠르게 변할 수 있어야 하는데, Si의 한계에 부딪혔다고 한다. 의미는 찾아봐야겠다. 하여간 그래서 회로 크기를 줄여야 했다. 회로가 너무 크면 같은 회로 내에서 다른 상태를 갖게 되니까. 근데, 하드웨어 크기를 줄이면 또 성능이 떨어졌다. 이때, 회로를 쪼갠 작은 단위를 ‘파이프라인’이라 부른다. CPU 제조사들은 이로 인한 성능 저하를 막기 위해 캐시메모리를 늘리고 분기예측기를 추가했다. 하지만, 이런 식으로 하드웨어를 추가하자 회로 배선이 더 어려워졌고, 비순차수행 구현도 더 어려워졌다. 연결 관계가 복잡해져 상호작용하는 block 수가 늘어나자 칩 전체의 문제를 파악하는 것도 힘들어졌고, 공정이 미세화되면서 같은 회로도 특성이 나빠져 원래 설계를 못쓰게 되는 경우도 생겼다. Meltdown, Spectre 등 하드웨어 수준의 보안 결함은 언젠가 터질 수밖에 없는 일이었다. 시장에도 변화가 생겼다. 배치해야 할 트랜지스터가 늘어났기에 회로 설계 인력, 검증 인력, 개발 기간 모두 늘어나게 됐다. 그리고 파운드리의 제조 역시 복잡해졌기 때문에, 실물 칩을 받게 되는데에 걸리는 시간도 길어졌다. 소규모 회사들은 한두번 실수하면 회사가 망할 수준이 된거다. 개발 비용이 그만큼 올라갔으니까. 대기업이라고 해도, 옛날보다 칩을 많이 팔아야 수익을 낼 수 있었다. 그래서 파운드리 뿐 아니라 팹리스 기업들도 대기업과 중소기업 차이가 엄청나게 벌어진다. 이미 큰 고객을 갖고 있으면 매년 조금씩 개선해서 팔면 되는데, 새로 진입하려는 회사들은 첨단 공정, 커다란 칩 설계에서 발생할 수 있는 온갖 문제를 다 해결해야 한다. 이렇게 하드웨어 발전이 어려워지고 느려지게 되자, 소프트웨어 회사들은 어떻게 했을까? 일단 단일코어 성능이 더 이상 빠르게 발전할 수 없음을 받아들이고, 다중코어(멀티코어)에 맞는 프로그래밍을 새로 해야 했다. 그러다보니 수많은 다중코어 버그, 최적화 문제들을 해결해야 했다. 작은 회사들에게는 큰 부담이 되었다. 프로그래밍 회사들은 CPU 말고 다른걸 찾아나서기 시작했다. 필요하다면 CPU 말고, 특별한 목적만을 위해 설계된 가속기를 도입하려고 했고, 시장에는 물리 연산 전용 카드가 등장했다. 대규모 단순 수치 계산을 위해 VGA를 사용하려는 회사가 늘어났고, 엔비디아 CUDA라는 VGA 기반 프로그래밍 라이브러리를 제공하기 시작했다. 극단적으로 규모가 큰 소프트웨어 회사들은 자신들이 쓸 하드웨어를 스스로 설계하려는 욕심을 갖게 됐다. 예를 들어, 구글은 알파고에 필요한 연산을 가속할 카드인 TPU를 직접 설계해 사용했다. 구글에 따르면, TPU는 알파고를 위한 연산에서는 전성비가 동시대 CPU, GPU에 30~80배였다. 당연히 다른 작업들에서는 깡통이었지만 문제가 되지는 않았다. 이렇게 되자, 인텔의 입지가 좀 위험해졌다. 소프트웨어 회사들이 CPU 말고 다른 하드웨어를 알아보고 있잖아! VGA가 중요해졌고, 회사들이 자기에게 맞는 칩을 만들게 되면서 설계의 중요성이 커지게 됐다. 팹리스의 영향력이 커지게 된 것이다. 반면, 메모리 회사들은 상대적으로 적은 위협을 받았다. 뭐 메모리는 써야 할 것 아닌가. 전세계 핸드폰 시장을 노키아가 주름잡던 2007년, 아이폰이라는 것이 등장했다. 그 후로는 사람들이 핸드폰을 오래 켜놓고 사용하기 시작했고, 핸드폰에는 전력 절감에 집중한 부품들과 커다란 배터리가 들어가게 되었다. Seagate, Western Digital(WD)같은 회사들은 아이팟에게 겪었던 수모를 또 겪게 된다. (HDD 회사들) HDD가 핸드폰에 들어갈 수는 없으니까. 대신, 메모리 회사들은 플래시메모리를 팔면 되니까 상관 없었다. 메모리 회사들은 전력소모를 줄인 모바일 DRAM을 설계해 이걸 엄청 팔아먹으며 개꿀을 빨았다. 물량도 많이 팔리고, 부가가치도 많이 붙여서 팔았으니까. 모바일 시장 최초의 AP는 ARM 기반이었다. 스마트폰 시장이 확장되자, 핸드폰 제조사들은 자신에게 맞는 AP를 찾아나섰다. 퀄컴, 브로드컴, 삼성전자 LSI사업부 등이 AP 설계 분야에 들어왔다. 삼성전자의 엑시노스9와 애플의 A11은 모바일 회사들의 CPU 설계가 상당히 발전해 있다는걸 보여준다. 두 CPU는 각각 ARM 기반, x86-64 기반이다. 스마트폰이 PC보다 안좋은 점 중 하나는 입력장치다. 그래서 비밀번호 입력 등이 PC보다 힘들다. 그래서 스마트폰은 개인 인증을 위해 홍채, 지문 등 생체 인식과 영상, 음성 인식 등 AI 인식을 도입했다. 이런 인증에서 중요한건 반응 속도와 정확도인데, 스마트폰 회사들은 AI 연산 가속용 칩은 NPU를 탑재해 전력을 아끼면서도 인식이 잘 되도록 했다. 스마트폰 수요가 넘쳐나게 되자, 수요를 바탕으로 막대한 자본 투자가 가능해졌다. 원래 파운드리 기업들의 미세공정 수준은 인텔에게 2~3배정도 밀렸는데, 스마트폰 수요가 폭증하고 저전력 고성능 AP의 수요가 늘어나기 시작하자 파운드리들이 자본투자를 바탕으로 인텔을 따라잡기 시작했다. 사람들이 스마트폰으로 정보를 주고받으면서, 데이터 저장 수요 또한 폭증했다. 유튜브도 많이들 보니까 영상도 저장해둬야 하고, 사람들이 찍는 사진 화질도 계속 올라갔다. 근데 이러면, 서버에 정보를 기록하는 성능보다 정보를 빠르게 읽는 성능이 더 중요하다. 영상이든 사진이든, 업로드 초기에 가장 시간당 조회수가 많고, 그 뒤로는 계속 떨어지게 된다. 그래서 업로드 초기에는 고성능 저장소에 뒀다가, 나중에 저성능 보관소로 옮기면 비용을 아낄 수 있다. 메모리 회사들과 팹리스 회사들은 그에 맞는 제품들을 내놨다. 가장 빠른 저장소는 DRAM이니까 서버용 DRAM을 내놓고, DRAM보다는 느리지만 그래도 빠른 TLC 기반 SSD를 내놨다. 그리고 이런 서버들의 수요를 맞추기 위해 RI(Read Intensive) 제품을 내놨다. 메모리 제조사들은 1개 방에 4개 데이터를 저장할 수 있는 QLC 기술도 개발해 제품을 만들어내기 시작했다. 2007년 발매된 아이폰에 의해, 반도체 시장을 다 먹고 있던 인텔이 크게 흔들렸다. 저전력, 휴대성에 대한 사용자 요구가 엄청나게 늘어났다. 그래도, 인텔은 서버 시장 성장을 타고 돈을 꽤 벌긴 했다. AP가 중요해져서 팹리스 회사들이 크게 성장했고, AP에는 미세공정도 중요하니 파운드리도 크게 발전했다. 메모리 회사들은 DRAM과 낸드플래시 메모리를 팔아먹을 스마트폰 시장이 생겨서 좋았고, 서버에도 메모리를 엄청나게 팔아먹게 된다. 2016년에 알파고가 이세돌을 이겼다. 그 뒤로 인공지능이 계속 언급되며 발전했는데, 인공지능 학습을 위해 새로운 하드웨어들이 필요해졌다. 학습능력은 초당 얼마나 많은 자료를 학습시킬 수 있는가라서 고용량, 고대역폭 메모리가 필요한데, 메모리 회사들이 연구하던 HBM이 이 목적에 적합했다. 수많은 회사들이 HBM을 사고싶어했으며, HBM이 없을 때에는 그래픽용 메모리인 GDDR도 고려했다. 하이엔드 VGA도 AI 가속을 염두에 뒀기 때문에 HBM이 필요했다. 일단 HBM을 쓰면 대규모 데이터를 빠르게 들여올 수 있는데, 이 대규모 데이터를 빠르게 처리하는게 또 중요해졌다. 그래서 엔비디아같은 GPU 제조사들에게 기회가 생겼다. CPU는 수많은 조건들을 따져야 하는 복잡한 작업을 빠르게 하고, 그걸 GPU한테 던져주면 GPU가 대규모 데이터 작업을 한다. 현대 슈퍼스칼라 CPU는 상당한 웨이퍼 면적을 분기예측기 등 조건문 실행시 성능을 향상시켜줄 수 있는 하드웨어에 할당하고 있고, DRAM으로부터 받아와 실행했던 수많은 분기문들을 명령어 캐시(I-Cache)메모리에 저장하고 있다. CPU에도 정수 연산장치가 있긴 하지만, GPU 정수연산 성능이 압도적이다. 모니터의 각 픽셀들은 서로에게 영향을 주지 않기 때문에, 1천만개 픽셀이 있다면 1천개씩 1만번 계산해도 된다. 엔비디아는 개발자들이 엔비디아 제품을 쓰도록 생태계를 조성하기 위해 CUDA를 출시했다. CUDA는 개발자들이 GPU 기반 프로그래밍을 할 때, CPU 위 C언어 등에서 개발할 때같은 익숙한 느낌으로 개발할 수 있게 해놨다. CUDA 문법이 C 문법이랑 비슷하다. CUDA 덕분에, 프로그래머들은 자신이 어떤 칩을 사용하는지 신경쓰지 않아도 되게 되었다. 프로그래머 입장에서는 엔비디아가 제조한 VGA만 사용하면 된다. 일단 CUDA 기반으로 프로그램을 짜면, 이후에 GPU를 교체하더라도 추가 작업을 할 필요가 없게 된 것이다. 옛날에 인텔 CPU 기반 프로그래머들이 인텔 CPU 기반으로 프로그램 만들면 나중에 다시 짜야 할까봐 걱정할 필요 없었듯이, CUDA 개발자들도 엔비디아를 믿고 걱정을 덜었다. 그래서 CUDA 덕분에 코드 참조나 이직도 쉬워졌다. 근데 스마트폰에서도 안면인식, 지문인식을 위해 AI연산이 필요했다. 물론 스마트폰은 PC와 환경이 많이 달랐다. 엔비디아가 만들던건 대형 기판에서 100W 이상 먹는 거대한 칩들이라, 핸드폰에 들어가는 NPU들은 엔비디아가 아니라 삼성전자, 애플 등 대형 스마트폰 제조사와 퀄컴같은 팹리스 업체들이 만들게 됐다. FPGA의 원래 목적은, 웨이퍼로 실물 칩을 만들기 전에 설계를 검증해보는거다. 만약 칩 설계 단계에서 실수가 일어나게 되면 제조 공정을 통째로 엎어버려야 하기 때문에 실수의 대가가 크다. 심할 경우에는, 제조사가 문제 원인을 알면서도 해결을 포기한다. 하드웨어 한두개 크기가 바뀌어 배선도 바꿔야 하고, 결국 칩 형태가 크게 변해 칩 특성과 수율을 처음부터 다시 맞춰야 할 수도 있다. 인텔도 멜트다운, 스펙터 결함 수정을 거부한 바 있다. 어쨌든, 이런 설계 실수는 막아야 한다. 이를 위해 수많은 방법론들이 나왔다. 일단 컴퓨터 시뮬레이션이 있다. 편하긴 한데, 문제는 시뮬레이션이 아주 느리다는 거다. 1초 보려고 며칠 돌리는 식이다. 게다가, 칩의 동작은 그 칩에서 동작하는 소프트웨어와 큰 연관이 있다. 그렇다고 이미 느린 시뮬레이션에 거대한 소프트웨어까지 올려서 시뮬레이션하면 더 느려져서 아무것도 확인 못한다. 이건 팹리스 업체들에게 큰 부담이다. 팹리스 업체들이라고 해도 칩만 만들어 파는게 아니라, 해당 칩에서 사용 가능한 펌웨어까지는 같이 내놔야 하기 때문이다. 근데 컴퓨터 시뮬레이션이 너무 느리면, 시뮬레이션으로 소프트웨어를 개발하거나 동작을 확인하는게 불가능하다. 그렇다고 실제로 칩을 만들어봐? 이건 비싸고, 오래걸리고, 뭐 하나 수정하면 또 만들어봐야 하는데 그건 현실적으로 말이 안된다. 그래서 FPGA를 쓴다. PC에서는 프로그램 형태로 시뮬레이션이 돌아가지만, FPGA에서는 실제 웨이퍼에서 로직이 동작하듯 돌아간다. 물론 실제 칩보다는 느리긴 하지만, 그래도 이걸 써서 소프트웨어를 돌려보고, 구조를 바꿔서도 돌려보는 것으로 설계를 검증하고 펌웨어를 개발할 수 있다. 하지만 가격이 CPU보다 비싸다. 원래 FPGA는 이렇게 칩 시뮬레이션을 위해 쓰는 것이었는데, 이걸 AI 연산 등 특정 연산을 위해 쓰려는 사람들이 나타났다. 2017년, 중국계 채굴 업체 BitMain은 채굴 전용 칩 Antminer를 설계해, 1600W 전력으로 14TH/s를 달성했다. 엔비디아 GPU를 8개 연결한 채굴기들은 비슷한 전력으로 1GH/s밖에 달성하지 못했다. 1만배가 넘는 차이다. 그럼 무조건 ASIC(Application Specific Intergrated Circuit)이 이득인가? 위험할 수 있는게, 채굴에 필요한 알고리즘이 변하면 원래 쓰던 ASIC를 쓸 수 없게 된다. 이더리움도 채굴용 하드웨어, 전용 칩 사용을 막기 위해 알고리즘을 바꾼 적이 있다. 그래서, 하드웨어 수정이 필요할 수도 있는 곳에는 FPGA를 쓴다. 아예 ASIC로 만들면 수정이 안되니까. 전무님이 Hardwire로 구워버리면 싸다고 한게 이 얘기인가? 결국 FPGA가 쓰이게 된건 ASIC 대신이고, ASIC는 GPU보다 특정 연산을 위해 쓰는거였다. 결국, 컴퓨팅 성능 발전이 느려지고 AI가 대두되어 GPU를 많이 쓰는 거였으니, 컴퓨팅 성능 발전이 느려져 FPGA를 쓰게 됐다고도 볼 수 있다. 클라우드 서비스가 나오면서(아마존 AWS, 마이크로소프트 Azure), 기업들이 모두 HPC(고성능컴퓨터)를 살 필요 없이 클라우드를 이용하면 되게 되었다. RDIMM(고성능 DRAM 모듈)의 수요가 증가했고, 서버용 SSD 수요가 늘어나 낸드플래시 매출이 올라갔다. 메모리 회사들도 돈벌고, 가상화 호스팅 업체들도 돈 많이 벌었다. 휴대용 디바이스 수요 증가-&gt; 그거에 맞는 칩 설계하는 팹리스 성장 -&gt; 파운드리 매출도 성장 전통적인 CPU 성능 증가에 한계, AI 등 응용프로그램 등장 -&gt; GPU, FPGA 등 특정 연산에 강점을 보이는 칩들이 연산용 반도체의 주도권을 갖게 됐다. 기존 회사들은 CPU에 최적화된 프로그램을 버리고, 전용 가속기에 맞는 프로그램을 설계하게 됐다. 메모리 구조가 바뀔 필요는 없었다. 메모리 회사들은 그냥 팔면 됐고, 대규모 병렬 처리가 많이 필요해지면서 HBM같은 고부가가치 고성능 메모리에 대한 수요가 생겼다. 전통적인 로직 반도체 회사들은 모바일 플랫폼 발전+GPU 수요 상승에 편승 못하고 매출에 타격을 입었지만, AWS 등 서버 서비스가 확장되어 서버용 CPU를 팔며 이득을 보긴 했다. 그럼에도, 인텔은 ARM 서버들의 도전을 모두 물리치고 여전히 최강자로 군림하고 있다. 여전히 기업 서버 시장에서 인텔을 대체할 회사는 없다. 인텔은 FPGA 회사 Altera를 인수했다. 이게 반격의 실마리가 될 수도 있다. 어쩌면 FPGA도 가상화하여 전세계에 임대하는 사업 모델을 만들어볼 수도 있을 것이다. 이렇게 되면 칩 설계 회사들도 앱 개발 회사들처럼 작은 규모로도 운영할 수 있게 될거다. 20세기 말, IT 붐이 불었을 때 전세계에는 수많은 검색엔진 회사들이 나타났다. 야후, 마이크로소프트, 구글, 라이코스 등이 나타났다. 모두 나름의 알고리즘, 검색 랭킹 시스템, 기능이 있었다. 검색엔진은 사람이 더 많은 쪽으로 몰리게 되어 있다. 사람들이 많이 검색하면 데이터가 더 많이 쌓여서 알고리즘이 더 정확해지고, 사람들이 또 몰리기 때문이다. 야후는 이것저것 띄워서 종합 포탈사이트로 갔고, 구글은 그냥 검색엔진만 띄워놨다. 결국에는 구글로 다들 모였다. 구글은 회사규모가 커지자 다른걸 해보기 시작했다. 알파고도 처음에는 76개 GPU를 썼지만, 이세돌이랑 바둑둘때는 48개 TPU를 썼다. 이게 되자 구글은 자신감을 얻고, 하드웨어를 다 스스로 만들기로 했다. 아마존은 Graviton이라는 ARM 기반 CPU를 발표했다. Graviton은 지금은 AWS에서만 쓰이고 있지만, 나중에 어디 쓰일지는 모른다. 마이크로소프트도 자체 제작 CPU를 만들 것이라고 발표했다. 스마트폰 시대가 오자, 피처폰 + 특수목적 폰 + 업무기능이 되는 PDA를 만들려던 Motorola와 Blackberry는 존재감을 잃어버렸고, 노키아는 시장의 최강자였으나 애플/삼성전자에게 시장을 빼앗기고 마이크로소프트에게 매각됐다. 그러고는 윈도우폰을 만들더니 죽어버렸다. 스마트폰으로 전환해 살아남은 회사들도 고민이 많았다. 일단 기능이 많아져 배터리가 중요해졌다. 메모리도 중요해졌는데, 스마트폰 회사들의 메모리 발주량이 워낙 많다보니 메모리 회사들도 스마트폰 회사가 원하는대로 메모리를 만들어주고, 전담 지원팀도 만들어줬다. 메모리 회사 입장에서는 고객사와 친해져서 중국 등 다른 회사들이 잘 못들어오게 되어 좋았고, 스마트폰 회사 입장에서는 메모리 회사들이 내가 원하는 스펙의 메모리만 찍어내주니 다른 중소 경쟁사들이 못들어와서 좋았다. 중소 기업들은 주문량이 적어 메모리 회사들이 지원을 안해준다. 애플이 아이폰을 처음 만들때, 처음에는 인텔쪽 CPU들을 알아봤다고 한다. 하지만 너무 크고 전력도 많이 먹었다. 성능이야 좋았는데, 당시 해봐야 인터넷 검색 정도 할건데 그정도 성능이 필요하지는 않았다. 이때, 애플은 삼성전자의 S5L8900을 발견했다. 성능은 인텔에 비하면 구데기였지만, 전력 소모량과 크기를 맞출 수 있는 AP였다. AP에 eD램도 부착되어있어 별도 DRAM도 필요 없었고, 그렇기에 공간을 아낄 수 있었다. 설계, 파운드리, 패키징 다 하던 삼성전자라 가능한 일이었다. S5L8900은 x86 대신 ARM의 ISA를 쓰고 있었다. 원래 ARM은 저전력에 많이 쓰였다. 그래서 스마트폰은 이후로도 ARM 기반으로 성장했다. 2010년, 애플의 아이폰을 따라 삼성갤럭시가 나온다. 갤럭시와 함께 구글이 모바일 시장에 들어온다. 모두 ARM 기반이라 호환성 문제도 없었고, ARM 기반 소프트웨어 개발을 위한 수많은 프로그램들이 나와 수많은 벤처기업들이 사업에 뛰어들었다. 인텔은 미세공정의 우위 + 노키아를 인수한 마이크로소프트와의 동맹 관계로 베이트레일 + 윈도우폰 조합으로 반격을 준비했으나 잘 안됐다. 미세공정도 AMD가 인텔을 따라잡아버렸다. 인텔이 메모리 시장 진출을 위해 내세웠던 SCM(Storage Class Memory) 사업도 큰 곤란을 겪고 있다. 3D XPoint라는 물건을 내놨다고 한다. 3D XPoint 자체의 성능은 D램보다 나쁘지만, 용량이 커서 하드디스크나 SSD에 덜 다녀와도 된다. 하지만 이걸 만드는게 생각보다 어려웠다고 한다. 그래서 잘 안됐다. Optane? Lehi? 2015년 7월, 인텔은 167억 달러(거의 1년치 순이익)로 FPGA 회사 Altera를 샀다. 2017년에는 AMD에서 그래픽을 총괄하던 Raja Koduri를 영입하고, Xe라는 그래픽 칩 개발을 시작했다. 인텔이 왜 이런 짓을 했냐? 칩간 연결이라고 보고 있다. 파이썬, C언어 하는 사람이 FPGA 쓰려고 Verilog 해야 한다고 하면 힘들거다. 하지만 칩들이 통합된 개발 환경이 생긴다면, ‘FPGA에서 뭔가를 돌려라’ 정도 명령으로 FPGA를 쓸 수 있다. 인텔은 CPU 회사라서, CPU랑 상호작용 필요한 부품들을 함께 패키징해 성능을 끌어올릴 수 있다. 인텔이 포베로스같은 칩간 연결기술에 투자한것도 이런 통합을 위해서다. 라자 코두리가 인텔에 와서 설계하기 시작한 Ponte Vecchio 칩은 신형공정/구형공정, 고성능/저성능 연결기술 모두가 사용됐다. 포베로스 3D 패키징도 사용됐다고 한다. 이런 하드웨어 수준의 통합을 소프트웨어로 만들고자 하는게 인텔의 OneAPI다. 그냥 프로그램 하나 만들어두고, 새로운 하드웨어를 추가하면 그걸 자동으로 써서 최종 소프트웨어 성능이 자동으로 늘어나게 하는거다. 인텔은 2021년에 파운드리 사업에 재진출하며, 인텔 파운드리 고객들에게 x86 칩 설계 IP를 제공하겠다고 했다. x86은 지금도 세계에서 가장 강력한 영향력을 행사하는 IP로, x86에 접근해서 커스텀한다면 서버 회사들은 엄청난 효율의 서버를 만들어낼 수 있을 것이다. 2021년 3월, 인텔 행사에서 마이크로소프트 CEO는 인털 설계로 마이크로소프트만의 커스텀 칩을 만들겠다고 발표했다. 인텔이 발표한 신규 제품 Alder Lake는 인텔 최초로 ARM의 big.LITTLE에 대응하는 이종 혼합 코어가 들어가 있다. 여기 포함된 고효율 코어(Efficient core, E코어)는 인텔의 고효율코어(Performance core, P코어)와 비교했을 때, 면적당 성능이 2배정도 되는 것으로 보인다. E코어 발전속도가 지금 좀 빠르다. *P코어 고효율코어 맞나? 인텔은 어쨌든 스마트폰도 놓치고 공정도 따라잡혔지만, 생태계에 많은 투자를 해놨다. ARM은 다른 팹리스처럼 칩을 설계하고 위탁 제조해서 파는것도 아니고, 타사의 칩을 받아 원하는 대로 만들어주는 것도 아니고, 자사 칩, 칩의 일부, 자사 칩이 수행 가능한 ISA를 판다. ex) 1코어 라이선스(ARM이 직접 설계한 코어 라이센스 + ARM ISA 사용 권리): 삼성전자 엑시노스 LITTLE 코어, 퀄컴 스냅드래곤 LITTLE 코어, 화웨이 기린 980코어들 아키텍처 라이선스(ARM ISA 사용 권리, 칩 설계는 직접): 삼성전자 엑시노스 M 아키텍처, 퀄컴 고성능 Kryo 아키텍처, 애플 자체 아키텍처 모바일 시대가 도래하자, 애플을 시작으로 ARM의 설계는 불티나게 팔려나가기 시작했다. 수많은 회사들이 ARM의 코어 디자인을 구입하고, 자신만의 주변회로를 만들어 CPU를 만들었다. 삼성전자, 퀄컴은 ISA만 구입하고, 코어는 스스로 설계했다. ARM과 거래하면 일단 AP를 만들어볼 수는 있었다. ARM에게는 피처폰 시절부터 사용해온 Mali라는 그래픽 솔루션도 있었다. 시장이 계속 발전하면서, 발전 속도가 느려지기 시작했다. ARM은 고객들과 계속 대화해보며 문제를 들어보고, 수정 계획을 세웠다. ARM의 전성비 문제를 해결하기 위한 해결책이었던 빅리틀은, 웨이퍼 위에 최고 전성비를 갖는 구간이 다른 2개 CPU를 배치한다. 가벼운 작업에서는 가벼운 작업에 효율이 좋은 가벼운 코어를 쓰고, 무거운 작업에서는 거대한 슈퍼스칼라 코어를 쓴다. 간단해 보이지만, 또 쉬운 일은 아니었다. ARM은 이 계획을 미리 회사들과 공유했다. OS가 좀 고생해줘야 하는 방식이라 그렇다. OS가 작업의 경중을 잘 따져 코어들에게 분배해야 하기 때문이다. ARM은 이렇게, x86 진영이 해내지 못했던 이종 코어간의 컴퓨팅 모델을 만들었다. 인텔은 2020년에야 최초로 Lakefield라는 이종 아키텍처 CPU를 소량 내놓았고, 2021년 말에야 데스크탑용으로 알더레이크라는 CPU를 내놓았다. 인텔은 이때서야 이종 아키텍처 CPU 시대에 합류하게 됐다. ARM이 차지하지 못한 시장도 있었다. 서버 시장은 전통적으로 최상단에 메인프레임이라 부르는 시스템이 존재하며, 여기는 IBM의 POWER 아키텍처가 강력하게 자리잡고 있었다. 여기는 인텔도 못들어간 시장이다. 괜히 메인프레임 바꿨다가 서버가 몇초라도 멈추면 엄청난 돈을 물어내야 한다. 메인프레임 아래에는 서버 시장이 있고, 여기는 인텔이 먹어치운 시장이다. 퀄컴, 캐비엄, Applied Micro 등 회사들이 ARM기반 칩을 내놓긴 했지만, 시장의 선택을 받지는 못했다. ARM 서버들은 Throughput(최대 처리 용량)은 뛰어났지만, 반응성(개발 코어의 최대 속도)이나 메모리 성능에서는 인텔을 이기지 못했다. 게다가 페이스북, 마이크로소프트, 구글 등 거대 고객사가 ARM기반 CPU로 움직이려면, 그들이 수십년간 x86위에서 개발해온 소프트웨어가 ARM기반에서 작동하도록 다시 새로 Compile해야 한다는 뜻이다. 괜히 바꿨다가 온갖 버그가 발생해 서비스가 느려지거나 멈추기라도 하면 그건 엄청난 손실이다. 2021년, ARM의 서버 진출은 AWS의 극히 일부 서비스(EC2 A1), CloudFlare의 edge server 정도로 한정되어 있다. 데스크탑 시장은? ARM기반 CPU에서 쓸 프로그램이 없으니 필요가 없다. 스마트폰에서 돌아가는 ARM 기반 프로그램들은 x86환경에서 크로스 컴파일러를 이용해 만들어진다. 저전력 환경에서 돌아가는 수많은 프로그램들은 저전력 환경에서는 못만드는 것이다. ARM의 모바일 GPU는 성능, 시장 점유율에서 퀄컴, 파워VR을 이기지 못하고, AMD의 GPU 설계인 RDNA는 삼성전자와의 협업을 통해 모바일에 진출하려 한다. 결국 ARM은 모바일 CPU 말고는 제대로 먹은 시장이 없는데, 스마트폰 출하량은 감소하기 시작했다. 그리고 퀄컴, 화웨이, 삼성전자같이 AP 대부분을 스스로 설계하는 회사들 비중이 커지면 ARM도 위험해질 수 있다. ISA만 라이센싱하는건 설계 자체를 라이센싱하는것보다 훨씬 이익이 적다. ARM은 어쨌든 새로운 시장을 찾아내야 하는 상황이다. 그래도 ARM은 자체 칩을 제조하지 않기 때문에 경쟁하기보다는 협력하려는 회사가 많다. 엔비디아가 ARM을 합병하려 하기도 했었는데, 잘 안된 것으로 알고 있다. 원래 GPU는 프로그램 수행의 결과대로 움직여 영상을 띄워주는 정도의 일을 했다. 그러다가 CPU 성능 향상이 한계에 다다르고, 대안으로 떠오른 기계학습이 주목받게 됐다. 그런데 GPU가 기계학습에 적합했다. CPU도 4~20개 등 다중코어 시스템이 되긴 했지만, 여전히 거대한 디코더와 비순차 수행기 등 거대한 하드웨어가 필요했다. 엔비디아가 행운을 공짜로 얻은건 아니고, 오래 전부터 GPU로 대규모 연산이 필요한 시장에 들어가려는 노력을 해 왔다. 엔비디아는 2008년 Ageia라는 물리연산 가속기 전문 회사를 인수해 대규모 물리학 시뮬레이션이 필요한 분야에 진출하려고 했다. 물리 연산도 특정 정지된 시간에 공간상에 표시된 물체들의 속도 등을 각자 따로 계산하면 되는 작업이기 때문이다. CUDA는 2006년에 처음 출시됐다. 엔비디아 그래픽카드기만 하면 쓸 수 있었고, 물리학 시뮬레이션과 인코딩/디코딩 등에 쓰였다. 그러다가 AI가 주목받자 CUDA가 더 중요해졌고, 엔비디아는 GPU를 GPGPU라고 재명명하기도 했다. GPU는 단순히 모니터에 그림을 띄우는 칩이 아니라는 엔비디아의 선언이었다. GPU 경쟁사로는 CPU와 GPU 둘 다 하는 AMD 정도가 있었는데, AMD는 CPU를 말아먹고 죽어가던 상태였다. 인텔, ARM도 GPU에서 엔비디아를 이길 수 없었다. ARM은 대형 하드웨어를 두고 고성능 컴퓨팅을 시도해본 적이 없었고, 인텔은 Larrabee라는 x86 기반 병렬 프로세서를 개발하려 했지만, 제대로 된 성능을 내지 못했다. 인텔의 그래픽카드는 2017년 라자 코두리가 AMD에서 옮겨오고 나서야 개선되기 시작했고, 2020년 들어서야 쓸만한 물건이 나오게 된다. 이렇게 엔비디아는 그래픽카드에 있어 독보적인 위치를 갖게 되었으며, 돈을 많이 벌어 TSMC의 최첨단 공정을 쓰게 됐다. GPU 가격은 100~200만원 정도인데, 머신러닝 전용 카드들의 가격은 천만원이 넘는다. 그리고 이제는 단일카드만 파는게 아니라, 일종의 소형 슈퍼컴퓨터같은 형태로 팔기도 한다. 여러개의 엔비디아 A100 카드를 엮어 만드는 엔비디아 DGX A100의 초기 출시가는 19만9천달러였다. 근데 이런 비싼 제품들도 늘 공급부족이다. 이런 가격대가 가능한건, 머신러닝이 엄청난 부가가치를 창출해 여기 참여한 회사들의 매출, 실적이 폭등해 부자가 되었기 때문이다. 그러니 비싼돈 내고 이런것들을 사간다. 엔비디아는 기존 PC 게이밍 시장은 그대로 유지한 채로, 슈퍼컴퓨팅, 서버, 자율주행차 등 수많은 분야들을 차지했다. Tegra라는 모바일 시장용 칩은 실패하긴 했다. 근데 Tegra는 아예 뒤진건 아니고, 모바일 시장 말고 인공지능 에지(Edge AI) 시장을 개척하고 있다. 엔비디아의 머신러닝 솔루션인 Jetson 시리즈의 칩으로 사용된 것이다. 현재 Tegra는 초저전력부터 고성능까지 다양한 형태로 설계되어, 전자는 머신러닝 입문자들이 좋아하는 Jetson 나노에, 후자는 자율주행에도 사용 가능한 고급형 모델인 Xavier에 투입하고 있다. 그리고, 지금 엔비디아가 갖춘 수많은 개발 인프라와 개발자 집단, 하드웨어 성능을 따라잡는 회사가 나오기는 힘들어 보인다. 직업 시장에는 CUDA 프로그래밍을 전문으로 하는 프로그래머들이 이미 많이 자리잡았다. 이 지위를 유지하기 위해 엔비디아는 지속적으로 CUDA에 새로운 기능들을 도입하고 있으며, Jetson Nano같이 CUDA를 사용하는 개발자 보드를 개발해 초보자들이 계속 들어오도록 만들고 있다. Jetson Nano는 카메라까지 사도 20만원이면 살 수 있다고 한다. 엔비디아는 Jetson Nano에 약 10줄정도 코드만 짜면 이미지 인식 등을 시킬 수 있도록 ‘Jetson Inference’의 예시를 무료로 제시하고 있다. 우리와 함께하면 인공지능 쉽다! 같은 느낌이다. 하지만 엔비디아에게도 위협은 존재한다. 엔비디아의 새로운 상품들은 대부분 GPGPU를 중심으로 하고 있으며, 필요한 경우 기존 GPGPU 근처에 ARM 프로세서를 결합하여 제어 능력을 부여한 것이 대부분이다. 엔비디아는 이런 소형 프로세서의 자체 설계를 갖고 있지 않다. 그래서, 만약 인텔같은 거대한 기업이 고성능 로직 프로세서와 기존 x86 생태계에 FPGA나 GPGPU를 결합하고, 이를 통해 강력한 부가가치를 만들어내기 시작하면 엔비디아에게 큰 위협이 될 수 있다. 지금은 GPU 분야에서 힘을 쓰지는 못하고 있지만, x86의 2인자 AMD도 엔비디아에게 위협이 될 수 있다. 아니면 구글같은 강력한 소프트웨어 기업이 알파고에서 했던 것처럼 자체 가속기를 설계해 사용하고, 나아가 해당 가속기에 맞는 소프트웨어 환경을 구축하기 시작할 수도 있다. 이렇게 되면 엔비디아는 고객도 잃어버리고 생태계 주도권도 잃어버리게 된다. 엔비디아 역시 텐서 연산기를 칩에 내장하는 등의 방식으로 성능 우위를 유지하려 한다. 하지만, 결국 필요한 연산의 종류를 결정하는건 소프트웨어 기업이라 언제나 한발 늦게 될 수도 있다. 그래서 엔비디아가 ARM 인수를 시도했을 것이다. TSMC는 B2B가 중심이라 이름이 알려진 기업도 아니었고, MCU, PMIC 등을 위탁생산하는 기업이었다. 대만의 UMC와 경쟁하긴 했지만, 잘 알려지지는 않은 상태였다. 이러다가 모바일 혁명이 시작됐다. 사람들은 스마트폰에 원하는게 더 많아졌고, 결국 스마트폰 AP에는 저전력 + 고성능이 필요해졌다.]]></summary></entry><entry><title type="html">일본 반도체 내용</title><link href="http://localhost:4000/history/2023/10/04/History-%EC%9D%BC%EB%B3%B8-%EB%B0%98%EB%8F%84%EC%B2%B4-%EB%82%B4%EC%9A%A9.html" rel="alternate" type="text/html" title="일본 반도체 내용" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/history/2023/10/04/History%20-%20%EC%9D%BC%EB%B3%B8%20%EB%B0%98%EB%8F%84%EC%B2%B4%20%EB%82%B4%EC%9A%A9</id><content type="html" xml:base="http://localhost:4000/history/2023/10/04/History-%EC%9D%BC%EB%B3%B8-%EB%B0%98%EB%8F%84%EC%B2%B4-%EB%82%B4%EC%9A%A9.html"><![CDATA[<p>SEMI: 국제반도체장비재료협회</p>

<p>코로나때 반도체가 부족해서 자동차 공장이 못돌아갔고, 그래서 중고차 시장이 과열됐다</p>

<p>VSS: 차 속도 센서</p>

<p>TSMC는 일본 구마모토에 공장 짓고 있다</p>

<p>반도체는 대용품이 없다.
반도체는 엄청난 돈이 든다.
반도체 공급망은 위태롭게 균형 잡혀 있다.</p>

<p>2020년 10월 아사히카세히 반도체공장 (미야자키현 노베오카시)에서 화재</p>

<p>2021년 3월 르네사스반도체 나카공장(이바라키현 히타치나카시)에서 화재</p>

<p>2011년 7월부터 3달동안 태국에서 홍수
-&gt; 전자부품 공급 지연, 조명기구 구하기 어려웠음</p>

<p>여러나라에 공급망이 뻗어 있어서, 한곳에서라도 재해가 일어나면 최종제품이 못나온다.</p>

<p>중국에서는  코로나때 락다운으로 인해 반도체 제조공장 인력이 부족해져서 생산이 중단되기도 했다.</p>

<p>미국 정부는 2019년 5월 화웨이를 공격했다
2020년 12월에는 SMIC를 엔티티 리스트에 넣었다.
리스트에 포함된 기업에게, 미국 기업이 생산한 반도체를 못팔게 하거나 미국에서 발명된 기술을 못쓰게 만드는 정책이다</p>

<p>TSMC 삼성전자는 미국에 공장 세울 것을 요구받았다</p>

<p>일본은 소니세미컨덕터솔루션, 덴소 등 기업의 출자를 받아 구마모토에 TSMC 공장 세우도록 했다.</p>

<p>일본 반도체 = 히노마루 반도체</p>

<p>1990년대 초, 일본 버블경제가 붕괴했다.
이때쯤, 일본 철강 회사들이 반도체 사업에 뛰어든다.</p>

<p>NKK(현 JFE엔지니어링),
가와사키제철(현 JFE스틸),
고베제강소,
신일본제철(현 일본제철)</p>

<p>이렇게 4개 회사가 사업 다각화를 이유로 반도체 시장에 진입했다. 1990~1993년</p>

<p>가와사키제철은 ASIC 등 로직반도체,
나머지 회사들은 DRAM 양산에 들어갔다.</p>

<p>다들 메모리 사이클에 엄청난 피해를 입고 포기했다.
가와사키제철은 메모리반도체가 아니라서 조금 버티긴 했는데, 결국 포기했다.</p>

<p>급격한 가격변동과 제품의 빠른 세대교체를 못버텼다.
이렇게 새로운 회사가 들어오기 어렵다.</p>

<p>ASIC: 특정 용도로 맞춤 제작된 IC
ASSP: 특정 용도로 표준화된 IC</p>

<p>전력반도체: 전력모스펫, 다이오드, IGBT, 사이리스터 등</p>

<p>2010년대 들어 형광등, 백열전구가 LED로 대체되고 있다.</p>

<p>FPD: Flat Panel Display
LCD, OLED 등이 포함된다.
TFT가 화면을 채우고 있는 구조다.
브라운관을 대체했다.</p>

<p>변압기에 반도체가 들어가나?</p>

<p>설계: 내부 회로 설계
전공정: 회로 생성
후공정: 사용할 수 있는 형태로 제작</p>

<p>식각에 사용되는 가스: SF6, CF4 등 불소가스</p>

<p>노광에 사용되는 엑시머 레이저 생성을 위해서는 네온, 아르곤 등 비활성 기체가 필요하다.</p>

<p>실리콘 주요 생산국은 중국, 노르웨이
웨이퍼 생산은 일본, 한국, 대만</p>

<p>네온: 우크라이나</p>

<p>반도체 재료 분야는 일본 기업들이 잡고 있다.</p>

<p>설계: 팹리스
전공정: 파운드리
후공정: OSAT
설계제조판매: IDM</p>

<p>1970~1980년대에는 NEC(일본전기), 후지쯔, 도시바, 미쓰비시전기, 히타치제작소, 소니, 마쓰시타전기산업(현 파나소닉) 등의 경쟁이 치열했다. 모두 IDM이었다.</p>

<p>얘네가 잘하던건 특정 용도에 최적화된 반도체를 만들고,
그게 들어가는 전자기기까지 만드는 것이었다.</p>

<p>근데 전자제품 시장이 디지털화 되면서, 이젠 특정 용도에만 쓰이는 반도체가 아니라 다양한 디지털 제품들에 필요한 반도체를 만들어야 하게 됐다.</p>

<p>설비 비용도 증가해서, 우리 회사에서만 쓰는 반도체를 위해 수천억 들이붓는것도 위험부담이 커졌다.</p>

<p>그래서 팹리스 파운드리 OSAT처럼 분업이 필요해졌다</p>

<p>팹리스: 거금의 투자가 필요 없다.
파운드리: 다수의 고객을 유치할 수 있다.</p>

<p>인터넷도 발달해서, 다른 나라 파운드리에 설계도 보내고 피드백 받으며 소통하는게 가능해졌다.</p>

<p>일본 반도체 제조사들은 전력반도체 분야에서는 아직 경쟁력이 있다.</p>

<p>SiC, GaN이 쓰이기 시작했지만, 아직 너무 비싸다.</p>

<p>차세대 전력반도체로는 GaO도 주목받고 있다.</p>

<p>진공관은 고음질 오디오, 위성 통신 등에서 지금도 쓰인다.</p>

<p>진공관 라디오 이전에는 광석 라디오가 쓰였다.</p>

<p>처음 발명된 트랜지스터는 게르마늄 트랜지스터였다.
근데 게르마늄은 녹는 점이 낮아 열에 취약했다.</p>

<p>게르마늄 녹는점 938도
실리콘 녹는점 1410도
철 녹는점 1583도</p>

<p>근데 왜 게르마늄을 썼냐?
다른 재료보다 가공이 용이해서.</p>

<p>1960년대 후반~70년대 후반까지는 전자계산기 시장이 엄청 활발하게 굴러갔다. 기업들이 엄청 많이 썼다.</p>

<p>TI, 카시오계산기, 소니, 제2정공사(현 세이코 인스트루먼트), 샤프 등이 만들었다.</p>

<p>전자계산기 시정에 의해:
군수, 우주 산업 등에 의해 굴러가던 반도체 산업에 민간 수요가 더해지며 반도체 수요가 크게 증가했다.
집적회로의 집적도가 크게 올라갔다.
액정, 태양전지 등 다양한 제품들이 상용화되었다.
세계 최초 마이크로프로세서인 인텔4004가 개발됐다.</p>

<p>1969년, 일본의 비지콤은 인텔에 8가지 IC 설계를 의뢰한다.
이 IC들은 모두 비지콤의 전자계산기에 들어갈 예정이었다.</p>

<p>하지만 당시 인텔은 규모가 작은 회사였기에, 새로운 IC를 8개나 만들 여력이 없었다.</p>

<p>인텔의 테드 호프가 범용 컴퓨터 개념을 제안한다.</p>

<p>CPU를 하나만 만들고, 프로그램을 바꿔넣어 다른 기능을 하게 만드는 것이다.</p>

<p>비지콤은 이를 수용하고, 그렇게 1971년 11월 인텔4004가 탄생한다.</p>

<p>인텔 4001~4003도 있다.
4001: 비휘발성 ROM
4002: RAM
4003: I/O 회로 제어
4004: CPU</p>

<p>원하는 프로그램을 ROM에 담아두고 실행시키면 됐다.</p>

<p>범용 컴퓨터는 원래 자기 코어 메모리라는걸 사용했다.
근데 자기코어메모리는 크기도 크고 수작업으로 제조해야 했다.</p>

<p>그래서 인텔, IBM등이 반도체메모리 개발을 시도한다.</p>

<p>1973년, TI에서 4K비트 DRAM을 개발한다.</p>

<p>후지쯔, 히타치제작소, 미쓰비시전기, 일본전기, 도쿄시바우라전기(지금 도시바) 연구조합이 1976년 16k비트 dram 개발</p>

<p>1960년대는 전자계산기 경쟁이었지만,
1970년대부터는 가전제품에도 IC가 들어가게 됐다.
1970~1980년대동안 일본의 반도체(히노마루 반도체)가 세계 반도체 시장을 주도했다</p>

<p>1985년에는 인텔이 dram산업에서 철수했다</p>

<p>일본 회사들은 dram만 만든게 아니라 완제품까지 만들었다. 각 제품에 맞는 ic까지 설계해서 넣었었다</p>

<p>당시 일제 tv, 라디오 등은 인기가 많았다.
1979년에 소니가 출시한 워크맨은 정말 인기가 많았다.</p>

<p>그러다가 미일반도체협정으로 꺾였다</p>

<p>그리고 다른 전자제품보다도 컴퓨터 위주로 시장이 굴러가면서 인텔 등 회사가 살아났고, 일본 반도체의 고가 과대품질보다는 다른 dram들이 쓰이면서 일본 회사들이 망했다</p>

<p>1972: 인텔8008 개발 - 8비트 마이크로프로세서
1978: 인텔8088 개발 - 16비트 마이크로프로세서
1985: 인텔80386 개발 - 32비트 마이크로프로세서</p>

<p>70년대 후반부터, 일본 기업들도 pc 시장에 뛰어들었고, pc들이 몇개 출시됐다</p>

<p>1992: 윈도우 3.1 출시
이전까지는 문자 중심 인터페이스 (CUI)였던게 여기부터 그래픽 중심 인터페이스(GUI)가 됐다</p>

<p>Pc 시장이 커지면서 주변 기기에 쓰이는 시장도 커졌다
이더넷 트랜시버 ic, 시리얼 통신 ic 등</p>

<p>114~119 보고 역사 정리</p>

<p>한동안, LED가 백열전구, 형광등을 대체할것이라는 이야기가 있었다
1990년대 초, 빨간색 LED는 나왔는데 초록색, 파란색 LED가 안나온 상태였다. RGB가 다 있어야 하얀 빛이 나올텐데 말이다.</p>

<p>연구는 되고 있었다. 재료 후보로 2개가 있었다.
ZnSe, GaN이 후보였다</p>

<p>당시 ZnSe의 전망이 더 밝아보였다.
ZnSe는 p형, n형 모두 개발되어 있었는데
GaN은 n형만 개발되어있고 p형은 개발되기 전이었다.</p>

<p>전세계 대기업들은 ZnSe를 가지고 초록색, 파란색 LED를 만들려고 노력했다.
일본의 소니, 마쓰시타전기산업(현재 파나소닉), 미국의 3M, 네덜란드의 필립스 등이 ZnSe를 가지고 연구하고 있었다.</p>

<p>근데 1993년 11월, 일본의 니치아화학공업이 GaN으로 파란색 LED를 만들었다고 발표했다. 그래서 다들 LED를 GaN으로 만들게 되었다.</p>

<p>결국 ZnSe는 역사 속으로 사라졌다.</p>

<p>2014년 노벨물리학상이 이거 개발한 사람들에게 갔다.</p>

<p>전자기기 많이 쓸수록 전력 소비량이 늘어나는데, 전력반도체를 쓰면 에너지 절약 효과가 클 것으로 다들 예상한다</p>

<p>반도체 업황:
2000년 활황
2001 불황
2008 리먼으로 불황
2017,2018 데이터센터, 스마트폰의 플래시메모리로 활황
2019 불황
2020 코로나로 대불장</p>

<p>Si: 중국, 노르웨이에서 생산
웨이퍼 가공: 한국, 일본</p>

<p>최근에는 반도체 제조에 사용되는 헬륨, 네온, 아르곤 등 비활성기체의 대부분을 우크라이나에서 생산하고 있어 문제가 됐다</p>

<p>설계는 미국, 전공정은 대만, 후공정은 말레이시아에서 하는 경우가 많다</p>

<p>반도체는 특정 회사 점유율이 높아 갑자기 공급이 끊길 위험이 많다</p>

<p>웨이퍼: 노부코시화학공업 세계점유율이 30%, SUMCO 점유율이 20%</p>

<p>포토마스크는 대일본인쇄, 돗판인쇄</p>

<p>포토레지스트: 도쿄응화공업, JSR등 일본기업 91%</p>

<p>웨이퍼 세정장치: SCREEN 홀딩스 40%
세정도 전세정 후세정이 있다</p>

<p>코터 디벨로퍼(포토레지스트 도포, 현상장치): 도쿄일렉트론 90%</p>

<p>반도체 기판: 이비덴, 신코전기공업
얘네 없으면 CPU 패키지 기판 못만들고, 서버용 프로세서도 못만든다</p>

<p>그래서 회사에 문제생기면 큰일날 수 있다</p>

<p>1993년 7월, 스미토모화학공업(현재 스미토모화학)의 에히메 공장에서 화재가 발생했다.</p>

<p>근데 스미토모화학공업은 반도체 패키징에 필요한 에폭시 수지의 세계점유율이 63%였다</p>

<p>화재때문에 공장이 1달 멈췄고, 전세계 반도체 제조사는 에폭시 수지 확보를 위해 노력해야 했다</p>

<p>그 뒤 동일본 대지진, 구마모토 지진, 코로나19 등 사태는 위태로운 반도체 공급망을 잘 보여줬다</p>

<p>2021년 2월: 텍사스 한파로 정전 발생, 텍사스에 공장 있던 반도체 제조사들 공장이 멈춰ㅛ다</p>

<p>2022년 1월: ASML 독일 공장에서 화재, 노광장치 생산 늦어져 모든게 다 늦어졌다</p>

<p>반도체 상사:
영업 마케팅, 재고 물류, 금융 기능을 한다
고객사 연결해주는 역할 등 한다</p>

<p>미국 애로우일렉트로닉스, 애브넷,
대만 WPG 홀딩스 등</p>

<p>반도체 수요가 컴퓨터에서 핸드폰으로 바뀔 쯤부터, 반도체 업계는 수직 통합형에서 수평 분업형으로 바뀌었다</p>

<p>EPC: GaN 전력반도체
AI칩: 세레브라스시스템즈, 삼바노바시스템즈</p>

<p>덴소: 도요타 산하 차량용 반도체 기업</p>

<p>176페이지 보고 일본기업들 정리</p>

<p>차량용 반도체는 최첨단보다는 몇단계 전 세대 공정을 많이 쓴다. 신뢰성이 너무나도 중요하니까</p>

<p>반도체 말고 전자부품은 일본 기업들이 상당히 선전한다
무라타, TDK, 다이요유덴이 MLCC 50%이상 먹었다</p>

<p>반도체는 나중에는 1930년대, 2차대전때처럼 블록 경제화될수도 있다</p>

<p>현대에 큰 전쟁이 없어서 반도체 수평분업화가 가능하기도 했다. 전쟁나면 어려워질 수 있다</p>

<p>실리콘 원자 지름이 0.1나노미터정도 된다.
지금 공정이 2나노쯤 되니까, 거의 한계까지 왔다</p>]]></content><author><name></name></author><category term="History" /><summary type="html"><![CDATA[SEMI: 국제반도체장비재료협회]]></summary></entry><entry><title type="html">차세대 반도체 내용</title><link href="http://localhost:4000/history/2023/10/04/History-%EC%B0%A8%EC%84%B8%EB%8C%80-%EB%B0%98%EB%8F%84%EC%B2%B4-%EB%82%B4%EC%9A%A9.html" rel="alternate" type="text/html" title="차세대 반도체 내용" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/history/2023/10/04/History%20-%20%EC%B0%A8%EC%84%B8%EB%8C%80%20%EB%B0%98%EB%8F%84%EC%B2%B4%20%EB%82%B4%EC%9A%A9</id><content type="html" xml:base="http://localhost:4000/history/2023/10/04/History-%EC%B0%A8%EC%84%B8%EB%8C%80-%EB%B0%98%EB%8F%84%EC%B2%B4-%EB%82%B4%EC%9A%A9.html"><![CDATA[<p>차세대 반도체</p>

<p>최신 반도체 집적회로 설계 동향:</p>

<p>아날로그회로는 생산과정에 최신공정이 필요하지는 않지만, 설계 노하우가 굉장히 중요하다.
자세한 설계도가 없으면 모방하기도 어렵다.</p>

<p>엔비디아의 그레이스 호퍼 슈퍼칩:
이전까지는 cpu gpu가 별개 칩이었지만, 여기서는 하나의 패키지 안에 들어간다.
여기 Cpu gpu 통신에 적용된 기술은 NVlink라는 엔비디아 독점기술이다</p>

<p>보톤 서버, 슈퍼컴퓨터 cpu에는 x86이 많이 쓰이는데, 그레이스 호퍼 슈퍼칩은 arm cpu를 썼다</p>

<p>X86은 라이센스 비용이 비싸고 인텔 영향을 벗어나지 못한다는 단점이 있다. 그러니 arm을 서버나 슈퍼컴퓨터에 쓴다는건 인텔 영향력을 벗어나 cpu를 쓰겠다는 엔비디아의 의도를 보여준다</p>

<p>Dram:
DDRx 데스크톱, 클라우드 컴퓨터용 (double data rate)
LPDDRx 스마트폰, 노트북등 모바일 기기용 (low power ddr)
GDDR gpu용</p>

<p>Dram 밀도를 높이기 위해 미세공정은 당연히 쓰고,
3d 집적도 쓴다</p>

<p>Dram 안에 로직 회로를 넣는 pim 기술도 있다</p>

<p>Pim 구조에서는 dram이 로직 칩과 독립적으로 일부 연산을 수행할 수 있다.
그러면 로직 칩으로 데이터를 전송하기 전에 dram에서 데이터 크기를 줄일 수 있고,
Dram과 로직 칩 사이에 필요한 데이터 전송량과 횟수도 줄어든다. 결국, 전력소비를 아낄 수 있다</p>

<p>3d nand flash: 메모리 셀을 쌓은거
4d nand flash: 메모리 셀 외에 주번회로들까지 몽땅 쌓은거. 단면적을 더 줄였다</p>

<p>요즘 자체 칩 생산 많이한다. 애플이 주도하는 질서다</p>
<ol>
  <li>설계 최적화 가능</li>
  <li>맞춤 기능 구현 가능</li>
  <li>공급망 관리(scm, supply chain management) 유리
요즘은 빅테크 기업들이 자체 칩 생산을 많이 한다</li>
</ol>

<p>폰노이만 구조의 한계를 뛰어넘는 차세대 아키텍처 설계:</p>

<p>폰노이만 구조는 미리 저장해둔 프로그램을 바탕으로 컴퓨터가 작동한다
그래서, 하나의 하드웨어로 소프트웨어만 교체하면 다양한 기능을 구현할 수 있다</p>

<p>폰 노이만 구조 도입 전, 예를 들어 에니악에서는 다른 기능으로 바꾸려면 작업자들이 전선 연결을 일일이 다 바꿔야 했다.</p>

<p>폰 노이만 구조의 특징: 
로직과 메모리가 분리되어 있다
그래서, 연산이 일어나는 장소와 데이터가 저장되는 장소가 다르다.
그래서, ALU와 메모리 사이에서 data access 과정이 자주 발생한다.
연산을 하는 ALU는 필요한 정보를 메모리에서 불러오고, 결과는 메모리로 내보내 저장한다.
그런데, 이 데이터 액세스에 소모되는 시간과 에너지가 너무 많다. 연산 자체보다 에너지 훨씬 많이 먹는다.</p>

<p>Cpu, gpu 안에서 일어나는 on-chip data access는 연산 자체보다 10~100배,</p>

<p>산술논리장치(ALU)와 주기억장치(DRAM) 사이의 off-chip access는 연산 자체보다 100~1000배 에너지를 먹는다.</p>

<p>이게 폰 노이만 구조의 문제 중 하나다.</p>

<p>폰 노이만 구조의 핵심이자 최대 강점은 범용성 추구다.
그래서 컴퓨터 발전은 모든 일을 잘하는 컴퓨터를 향해 발전해왔다</p>

<p>2000년대 초까지는 그냥 공정 미세화로 트랜지스터 밀도 올리면 성능이 잘 증가해왔다</p>

<p>근데, 트랜지스터 미세화가 한계에 가까워져서 이 방법으로는 성능을 올리기 어렵게 되었다.
이제는 범용성을 포기하면서 성능을 올려야 하는 시대가 되었다</p>

<p>다양한 비 폰노이만 구조가 나오고 있다
비 폰노이만 구조 하드웨어를 accelerator 또는 ASIC라 부른다.</p>

<p>LLM등 특수 목적에 최적화된 구조를 만든다거나,
Programmability(프로그램 가능성)을 제한하는 대신 성능과 효율을 개선한다거나,
데이터 액세스 과정에서 소모되는 에너지 줄이기 위해 최대한 온칩 메모리만 써서 연산한다거나,
로직과 메모리의 구분을 없앤 IMC(In Memory Computing)구조를 쓴다거나</p>

<p>다양한 시도들이 이뤄지고 있다</p>

<p>폰노이만 구조: 데이터를 하나 가져와서 연산하고 내보내고, 또 하나 가져와서 연산하고 내보낸다</p>

<p>인메모리 컴퓨팅 구조: 데이터를 병렬로 가져와서 연산하고 병렬로 내보낸다</p>

<p>이 구조를 하드웨어로 구현할 때는:
저항, 캐패시턴스를 쓰는 AMS(analog mixed signal) 방식,
Compressor, adder tree를 쓰는 디지털 방식이 있다</p>

<p>MACC-SRAM: multistep accumulation capacitor-coupling in memory computing sram</p>

<p>DIMCA: digital in-memory computing with approximate hardware</p>

<p>등을 석민구 교수님 연구실에서 개발했다
모두 sram 기반</p>

<p>인메모리 하드웨어 성능을 평가할때 살펴보는 지표는 4가지다</p>

<p>에너지 효율[TOPS/W], 1W전력으로 수행할 수 있는 초당 연산 횟수</p>

<p>연산 밀도[TOPS/mm^2], 1mm^2 면적에서 수행할 수 있는 초당 연산 횟수</p>

<p>저장 밀도[um^2/kb], 1킬로비트당 필요한 Si 면적</p>

<p>정확도[%], 말 그대로 얼마나 정확한지</p>

<p>Hardware reuse: 동일한 하드웨어 요소를 여러 연산에 걸쳐 공유</p>

<p>특정 알고리즘을 위한 하드웨어들도 많다
석민구 교수님이 개발한 SNN(스파이킹 신경망)을 위한 전용 하드웨어가 예시
이거밖에 못하지만, 이거는 높은 효율로 해낸다</p>

<p>대부분 연산에 온칩 메모리를 활용하고, dram 등 오프칩 메모리는 최소한으로 사용
Neurosynaptic core,
Neuron block,
Synapse block 등 다양한 온칩 요소를 활용해 데이터를 잘 분산해서 저장</p>

<p>메모리 접근 방식은 원래는 synchronous, asynchronous sequencing이 있었는데, 여기서는 둘을 결합해 신경망 연산에 최적화된 방식을 만들었다</p>

<p>인간 뇌 뉴런은 860억개 정도라고 한다
인간 뇌는 20와트 전력을 소모한다고 한다
나누면 뉴런 1개당 230피코와트 소모다</p>

<p>Time sharing: 시분할. 이거 했다가 저거 했다가 하는 식으로 여러 동작에 하나 장치를 공유하는거</p>

<p>ALU는 time sharing이 가능한데, 메모리는 새로운 정보를 저장하려면 기존 정보를 지워야 해서 안된다.</p>

<p>따라서 메모리 용량을 늘리는건 더 많은 공간을 확보해서 셀 꾸겨넣는것밖에 방법이 없다</p>

<p>Dram같은 오프칩 메모리에 엄청난 데이터를 저장할 수 있긴 하지만, 연산장치 입장에서 데이터를 보냈다가 불러오는게 너무 비효율적이니 연산장치 안에 메모리를 얼마나 내장할 수 있는지가 중요해졌다</p>

<p>로직 안에 메모리를 집어넣는 기술중에는 내장형 sram이 지금 가장 많이 쓰이고 있다</p>

<p>메모리 공간 확보를 위해, 그냥 칩 크기를 키우는 방식도 있다. 이런 접근을 웨이퍼 스케일 컴퓨팅이라 부른다.
이렇게 하면 당연히 sram 많이 박아서 큰 용량 온칩메모리를 구현할 수 있다.</p>

<p>하지만, 칩 크기가 커질수록 수율이 떨어진다. 칩이 커질수록 그 안에 들어있는 모든 트랜지스터와 회로가 완벽히 작동할 확률이 내려가니까.</p>

<p>그래서, 어디 문제생기면 그걸 대체하기 위한 여분 부품이나 우회 회로를 추가하고, 제조 후 칩을 검수해서 제대로 된 부분끼리 연결하는 calibration 등 여러 방법을 쓰고 았다.</p>

<p>그리고 칩이 크면 전력소모도 많고 열이 많이 발생한다.
그래서 패키징이나 보드 설계 과정에서 전력공급, 냉각을 신경쓰고, 수냉시스템이나 냉간판(cold plate)를 활용하는 등 다양한 방법이 있다</p>

<p>차세대 메모리 기술:
내장형 sram 말고도 다양한 기술들이 나오고 있다
강유전체를 쓰는 FeRAM,
저항변화를 쓰는 ReRAM,
물질의 상 변화를 쓰는 PCRAM,
스핀 전류를 쓰는 STT-MRAM,
스핀 궤도를 쓰는 SOT-MRAM 등</p>

<p>비트셀 크기는 F^2라는 단위로도 많이 표현한다
여기서 F는 최소 배선 폭(minimum feature size)을 의미한다
Sram은 일반 로직 소자를 이용해서 F값이 아주 작다
하지만 dram의 f값은 sram보다 몇배 크다</p>

<p>신기술들의 F값들도 현재 기술 수준으로 꽤 작기 때문에, 상용화가 가능해진다면 상당히 가능성이 있다</p>

<p>물론 문제도 있다.
내구성(endurance): 데이터를 쓰고 지우고를 반복할 수 있는 횟수
강건성(robustness): 온도, 전압 등 외부 환경 변화에 노출돼도 정상적으로 동작하는 정도
차세대 기술들은 아직 이게 부족하다</p>

<p>하드웨어 가격은 제조과정 복잡도, 수율에 영향받는다
아직 차세대 메모리 기술들은 기존 메모리 기술 수준이 안된다</p>

<p>패키징에서는 메모리 공간을 확보하기 위해 어떤 노력을 하고 있는가?</p>

<p>상용화된 가장 유명한 패키징 방식은 2.5D다.
Pcb 위에 Interposer, 그 위에 로직칩, 메모리칩이 올라가 있다</p>

<p>Interposer는 금속 배선만 들어있는 칩이다.다른 칩들을 연결해주는 역할을 하는데, 배선 밀도가 굉장히 높고 parasitic capacitance도 적기 때문에, 인터포저를 쓰면 pcb 위에 바로 칩을 연결하는것보다 훨씬 높은 에너지 효율과 빠른 통신 속도를 얻을 수 있다</p>

<p>55페이지 보고 패키징 방식들 정리해놓기</p>

<p>칩을 싹 다 수직으로 쌓을 경우, 전력을 옆이 아닌 위 또는 아래에서 공급해야 한다. 근데 이건 층이 높아질수록 어려운 일이다.
또, 발생하는 열들이 빠져나가지 못해 칩이 뜨거워진다
그래서 전력공급, 열 방출을 위한 구조물을 만드는 데에 또 실리콘 면적을 할애해야 한다
그래서 아직 상용화가 안됐다</p>

<p>Pim
신경망 모델처럼 복잡한 대규모 연산을 수행할 때, 일부 연산을 메모리가 끝낸 후 연산장치에 보내주자는 거다</p>

<p>이러면 연산장치와 메모리 사이 데이터 액세스가 감소할 것이다
이렇게 해서 연산장치와 메모리가 연산을 분담하자는건데,
문제는 하나의 workload를 서로 독립적인 2개 workload로 나누는 것이 상당히 어렵다는 점이다.</p>

<p>Cpu 멀티코어에서 코어별로 일 분담하게 하기 어려운거랑 비슷한 이야기다.</p>

<p>그래서 차세대 하드웨어는 cpu gpu dram을 모두 합친 형태가 될 것이라고 석민구 교수님은 생각하고 있다
이정도는 돼야 장치간 데이터 액세스 시간 에너지를 확실하게 줄일 수 있을 것이다</p>

<p>신창환 교수, 반도체 기술:
전공정(front end process): 웨이퍼에 회로 형성
후공정(back end process): 패키징</p>

<p>전공정은 fab공정이라고도 한다.
전공정은 feol, beol공정으로 또 나뉜다
Feol: 웨이퍼 기판 위에 반도체 소자 형성
Beol: 반도체 소자 사이에 금속 배선 형성
Front end of line, back end of line</p>

<p>Stress engineering:
트랜지스터의 채널, 즉 source와 drain 영역 사이 공간에 존재하든 실리콘 원자들에 기계적 stress를 가하면 mobility가 증가한다?
그래서 SiGe를 쓰면 그냥 Si를 쓸 때보다 채널 속 실리콘 원자에 더 큰 stres가 가해져 hole의 mobility가 증가했다</p>

<p>HKMG: high k metal gate
45nm부터 적용된 기술이다
Gate, 웨이퍼 사이 SiO2층을 점점 더 얇게 만들다보니, 전류가 계속 새는 문제가 생겼다.</p>

<p>물리적으로는 얇고, 전기적으로는 누설전류가 없는 얇은 절연막을 어떻게 만들 수 있을까?</p>

<p>결과적으로, SiO2보다 유전상수(dielectric constant, k)가 더 큰 high-k dielectric material인 HfO2를 절연막에 쓰게 됐다.</p>

<p>최근에는 란타넘 기반 산화물도 절연막 소재로 연구되고 있다</p>

<p>인텔이 세계 최초로 high k metal gate기술을 도입했고, 거의 모든 제조사가 32나노부터 기 기술을 도입했다</p>

<p>High k 유전체 절연막과 함께, 게이트 소재도 폴리실리콘에서 metal로 대체했다
메탈은 열에 약하기 때문에, gate를 soure, drain 이후에 생성하는 gate last 방식이 새롭게 적용됐다
그 전에는 gate first 방식이었다
현재는 대부분의 제조사가 gate last 방식을 쓰고 있다</p>

<p>반도체 기술이 계속 고도화되고 있지만, supply voltage를 낮추는건 어려운 일이다.</p>

<p>20년 전 90나노에서 스던 전압이 1.2볼트, 3나노에서는 0.65볼트를 쓰고 있다. 얼마 못내려간거다. 왜 supply voltage는 못 내리고 있을까?
-&gt;볼츠만 한계(boltzmann’s tyranny)라고 부른다
이걸 바꾸기 위한 노력, 즉 더 낮은 Vgs로도 더 큰 전류를 얻기 위한 노력이 계속되고 있다</p>

<p>솔리다임: 인텔 낸드사업부 하이닉스가 인수한거</p>

<p>첨단 낸드 생산에 쓰이는 HARC(high aspect ratio contact)장비: 가로 세로 비율이 아주 큰 구멍을 만드는 식각 장비</p>

<p>Saqp: self aligned quadruple patterning,
초미세 패턴을 한번에 만들 수 있는 기술이 여의치 않을때, 기존 패터닝 기술을 여러번 써서 초미세 패턴을 구현하는 방식</p>

<p>고난이도 식각 공정을 위해서는 새로운 불화탄화수소(hydrofluorocarbon, CxHyFz) 계열 가스를 개발할 필요가 있다.</p>

<p>신호 전달 속도를 올리기 위해 Mo(몰리브덴)을 많이 쓸 것이다
메모리 칩의 신뢰성을 올리기 위해 중수소(deuterium, D2)도 많이 쓸 것이다</p>

<p>중국은 전세계 형석(Calcium Fluoride, CaF2) 채취량의 57%를 차지하고 있다.
중국은 몰리브덴 원석 채취량의 56%를 차지하고 잇다.
그래서 중국이 반도체 공급망을 흔들 우려가 있다.</p>

<p>인도는 전세계 생산의 64%를 담당하고 있다.
그래서 인도가 낸드플래시 메모리 공급망의 주요 국가가 될 가능성이 있다</p>

<p>D램 메모리 반도체의 혁신 방향은:
3D구조를 도입한 적층, 메모리 셀 용량 극대화, 초미세 패턴 구현</p>

<p>D램이 3D구조로 넘어간다면, Si, SiGe 이중층(bilayer) 구조를 여러 단으로 쌓는 데에 필요한 사수소화저마늄(Germanium Tetrahydride, GeH4) 가스가 중요해질 것이다.</p>

<p>메모리 셀 용량을 높이는 과정에서 negative capacitance를 활용할 수 있는 Zr, Hf 사용량도 더 많아질 예정이다.</p>

<p>D램 공정에도 초미세패턴이 중요해지고 있어 EUV 공정을 사용하는 추세이고, 공정에 필요한 원재료에 대한 의존도가 높아지고 있다.</p>

<p>중국은 전세계 저마늄(Ge) 원석 채취량의 65%를 차지하고 있기에, 또 반도체 공급망 혼란을 야기할 가능성이 있다.</p>

<p>지르코늄, 하프늄을 분리하는 공정 및 공급체게를 장악한 나라는 러시아, 캐나다다.
얘네 때문에 공급망 문제가 생길수도 있다.</p>

<p>EUV용 blank mask, quartz, 포토레지스트는 일본이 꽉 잡고있다.</p>

<p>첨단 로직반도체는 GAAFET 등 기술과 함께 3D반도체 소자 구조로 넘어가고 있다.
로직반도체에서는 heterogeneous integration(제조사, 기능, 규격 등이 다른 부품을 조립해 하나의 반도체 칩을 만드는 기술), 첨단 패키징 기술 등이 주목받고 있다.</p>

<p>그래서, 복잡한 3차원 구조를 구현하는데에 필요한 특수식각가스 GeH4, 몰리브덴, EUV용 마스크, 포토레지스트 등이 중요해지고 있다.</p>

<p>그래서 D램과 유사한 공급망 문제가 로직 반도체에서도 발생할 수 있다.</p>

<p>airliquid: 프랑스 특수가스 회사</p>

<p>권석준 교수 차세대 반도체 기술의 핵심:</p>

<p>AI accelerator: GPU, TPU, NPU 경쟁중</p>

<p>메모리: HBM-PIM(HBM 안에 로직코어 넣어서 PIM), GDDR(GPU 전용 DRAM), Word line 수를 극대화하기 위한 다양한 3D NAND Flash 구조</p>

<p>소자: 지금은 FinFET이 DRAM, 로직반도체에서 주로 쓰인다.
앞으로는 GAAFET, MBCFET, CFET(Complementary FET, NMOS랑 PMOS를 수직으로 붙여 면적 최소화한 구조)</p>

<p>센서: multi-band 또는 hyperspectral 이미지 센서, 오감을 모방하는 멀티모달 센서</p>

<p>과거에는 설계, 전공정에 비해 패키징, 테스트 등 후공정에 대한 관심이 부족했는데,
반도체 소자 구조 고도화와 전공정 미세화가 한계에 이르면서 후공정 산업에도 관심이 많아지고 있다.</p>

<p>CoWoS(Chip on Wafer on Substrate), 이종 집적, 다중 대역 분광학을 이용한 첨단 계측 및 품질검사 등이 후공정의 혁신을 주도할거다.</p>

<p>최근, 첨단 파운드리 업계에서는 디자인하우스의 역할도 주목받고 있다.
칩 설계를 완료하고 생산하기 전에 설계 의도를 구현할 최적의 공정 조합을 제공함으로써 팹리스 기업에 많은 도움이 되기 때문이다.</p>

<p>더불어, 설계와 공정의 연결고리이니 설계-기술 공동 최적화(design-technology co-optimization,PRCO, 반도체 개발 초기 단계부터 설계와 공정의 호환성을 고려해 설계에서 발생하는 문제를 최소화하고 공정 조합을 최적화하는 기법)도 앞으로 많이 중요해질거다.</p>

<p>소재: 제조기술이 EUV로 넘어가면서, 기존에는 화학 증폭형 레지스트(Chemically amplified resist, CAR)을 썼는데, 이제는 CAR의 한계를 보완한 비화학 증폭형 레지스트(non-CAR)가 논의되고 있다.
이게 의미하는 것은, 유기물 기반 포토레지스트보다 무기물 또는 유/무기물 복합체에 기반한 첨단 소재가 더 중요해질 것이라는 거다.</p>

<p>장기적으로는, 노광을 대체할 방식을 찾고 있다.
고속 전자빔(High-energy electron beam) 노광, 2차원 반도체 기반의 자기 조립(2D semiconductor self-assembly) 등이 후보다.</p>

<p>물리학과에서 연구하는 저차원(low-dimensional)물질, 위상 절연체(topological insulator), 준입자(quasi-particle)을 비롯한 비전자(non-electron) 신호 전달 체계에 대한 연구가 진행되고 있다.</p>

<p>언제까지 전자를 쓸 수 있을 것인가에 대해 부정적인 견해가 많이 제기되고 있어서 그렇다.
전자는 아주 작긴 하지만 질량이 있고,
무엇보다도 전하를 가지기 때문에 매우 작은 스케일에서는 신호 손실과 전기 저항의 영향이 커지기 때문이다.</p>

<p>그래서, 전자와 달리 정지질량이 없으며 저항이 통제되는 광자(photon), 플라스몬(plasmon), 포논(phonon), 엑시톤(exiton), 스핀(spin), 스커미온(skyrmion), 폴라론(polaron), 폴라리톤(polariton) 등 다양한 신호 전달 매체로 쓸 수 있는 준입자들에 대한 기초과학 연구도 앞으로 계속 할거다</p>

<p>구글의 Sycamore, IBM의 Kookaburra: 양자컴퓨터 전용 반도체 프로세서</p>

<p>AI시대, 한국 반도체 산업 전략에 대한 토론 (전동석, 석민구, 신창환, 권석준):</p>

<p>지금은 인공지능 한번 학습시키는데에 전력이 진짜진짜 많이 든다. 즉, 전기 비용이 정말 많이 든다.더 효율적인 칩으로 전력을 아낄 필요가 있다.</p>

<p>PIM 만드는데에 어려움은 없는가?
로직과 메모리를 병합하는 기술이 상용화되려면, 3차원집적과 패키징에서 풀어야 할 문제가 많다.</p>]]></content><author><name></name></author><category term="History" /><summary type="html"><![CDATA[차세대 반도체]]></summary></entry><entry><title type="html">하루만에 내용</title><link href="http://localhost:4000/history/2023/10/04/History-%ED%95%98%EB%A3%A8%EB%A7%8C%EC%97%90-%EB%82%B4%EC%9A%A9.html" rel="alternate" type="text/html" title="하루만에 내용" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/history/2023/10/04/History%20-%20%ED%95%98%EB%A3%A8%EB%A7%8C%EC%97%90%20%EB%82%B4%EC%9A%A9</id><content type="html" xml:base="http://localhost:4000/history/2023/10/04/History-%ED%95%98%EB%A3%A8%EB%A7%8C%EC%97%90-%EB%82%B4%EC%9A%A9.html"><![CDATA[<p>진짜 하루만에 이해하는 반도체 산업 내용정리:<br />
2극 진공관: 1905년 플레밍, 다이오드 역할<br />
3극 진공관: 1907년 포리스트, 트랜지스터 역할<br />
<br />
1946년 개발된 ENIAC에는 18000개의 진공관과 1500개의 기계식 스위치가 사용됐다. 무게는 30톤, 소비전력은 180kW<br />
<br />
처음에는 반도체에 Ge를 썼는데, 노이스와 무어가 Ge 대신 Si를 쓰기 시작했다. Si는 구하기도 쉽고, 산소랑 반응시키면 SiO2라는 훌륭한 절연체가 된다.<br />
<br />
킬비가 집적회로 처음 만들었을 때만 해도 Ge기반 반도체였고, 사람이 일일이 다 납땜해야 했다. 재료가 Si로 바뀌면서 배선 방식도 바뀌어 자동화가 가능해졌다.<br />
<br />
1960년에는 MOSFET이 나왔다. MOSFET은 구조가 간단해 BJT 하나 들어갈 자리에 MOSFET 50~80개를 넣을 수 있었고, BJT에 비해 공정 단계가 40%정도 적어 만들기 쉬웠고, BJT보다 소비 전력이 낮았고, 스위칭 속도도 빨랐다.<br />
<br />
적층 공정을 할 경우, 각 층을 전기적으로 연결하기 위한 수직 구멍인 via를 뚫는 공정이 추가돼서 가격이 올라간다.<br />
<br />
칩 크기가 작아지면 한 웨이퍼에서 만들 수 있는 칩 수도 늘어나고, 웨이퍼에서 버려지는 부분도 줄어든다.<br />
<br />
Planar FET: 한 방향 전기장으로 leakage 차단<br />
FinFET: 세 방향 전기장으로 leakage 차단<br />
GAAFET: 네 방향 전기장으로 leakage 차단<br />
<br />
컴퓨터에서 엑셀을 실행할 경우:<br />
일단 엑셀 로고가 뜨며 로딩이 된다. 이 시간동안, CPU는 ROM에 있는 엑셀 프로그램 데이터 중 필요한 데이터들을 RAM으로 옮긴다. ROM은 속도가 느리기 때문에 옮겨서 쓰려는거다.<br />
<br />
데이터가 RAM으로 다 옮겨지면 엑셀이 실행된다. 엑셀 칸에 1, 2 등을 적으면 이게 다 RAM에 저장되고, 수식을 넣어 계산시키면 CPU가 엑셀 프로그램 데이터에 적혀있는대로 계산해준다.<br />
<br />
엑셀에서 저장 버튼을 누르면 RAM에 임시로 저장됐던 정보들이 다 ROM으로 보내진다. 그냥 끄면 ROM으로 안가고 날아가버린다.<br />
<br />
그래서 RAM이 커야 프로그램을 여러 개 띄워둘 수 있다. 프로그램에 적힌 계산방식은 CPU가 다 실행해준다. ROM은 흔히 보는 C드라이브다.<br />
<br />
CPU는 크게 제어 유닛, ALU(Arithmetic Logic Unit), 아주 작은 용량의 메모리 유닛(캐시메모리, 레지스터)로 이루어져 있다.<br />
제어유닛: 사용자의 명령을 해석하고, 각 유닛 사이 통신을 제어<br />
ALU: 제어 유닛이 해석한 명령을 수행하는 연산 기능<br />
메모리 유닛: 입력이나 출력 기억<br />
(사진)<br />
CPU는 계산만 해주는 애고, 기억은 RAM에서 하는데 캐시메모리가 왜 필요한가?<br />
앞에서 ROM이 너무 느리니 데이터를 RAM으로 옮겨 CPU와 작업시킨다고 했는데, 사실 CPU 입장에서는 RAM도 너무 느리다. 그래서 RAM에 있는 데이터를 RAM보다 더 빠른 캐시메모리와 레지스터로 옮겨 작업한다.<br />
그럼 캐시메모리와 레지스터를 RAM 대신 쓰면 안되나?<br />
그러기에는 캐시메모리와 레지스터가 용량 대비 가격이 너무 비싸다.<br />
그래서 CPU에 넣을 때도 꼭 필요한 만큼만 넣는다.<br />
<br />
CPU의 성능은 Core, Thread, Clock, Architecture에 의해 결정된다.<br />
Core는 ALU의 개수, Thread는 각 ALU가 동시에 몇가지 작업을 처리할 수 있는지 알려준다.<br />
<br />
Clock은 초당 몇 개의 instruction이 실행되는지 보여주고, architecture는 cpu가 어떤 명령어 체계를 사용하고, 해석과 연산을 어떻게 분배할 것인지, CPU와 주변 부품들의 구성과 동작에 대한 구조를 의미한다.<br />
<br />
그래서 같은 core, clock수를 갖고 있어도 architecture에 따라 CPU의 성능이 달라진다.<br />
<br />
CPU의 동작을 요리에 비유할 수 있다.<br />
ROM: 냉장고, RAM: 조리대, CPU: 요리사, 연산: 요리, 데이터: 식재료<br />
<br />
냉장고에서 식재료를 꺼내 조리대에 올려놓는다. 요리사가 요리를 할거다.<br />
<br />
이때 요리사의 수: Core 수, 각 요리사들의 팔 개수: Thread 수, 요리사의 요리 속도: clock 수<br />
<br />
그래서 core<em>thread</em>clock이 연산 속도를 보여주게 된다.<br />
<br />
Architecture: 주방 구조, 업무 체계<br />
어떻게 보면 architecture가 가장 중요할 수도 있다. 요리사가 아무리 훌륭해도 업무 체계가 구데기면 일 못한다.<br />
<br />
2021년 기준, CPU시장 점유율은 인텔 63%, 암드 37%<br />
<br />
이번엔 GPU에 대해 알아보자.<br />
그래픽은 픽셀들이 모여서 만들어지는데, 기술이 발전할수록 당연히 픽셀 수도 증가한다. 요즘 쓰이는 4k 모니터는 약 830만개 픽셀을 갖고 있으며, 각 픽셀이 2^24~=1678만개 색을 낼 수 있다.<br />
<br />
우리가 모니터로 보는 화면은 수백만개의 픽셀이 저마다의 색깔을 아주 짧은 시간동안 만들어내면서 생긴다. 각 순간의 화면을 모니터에 구현하기 위해, 컴퓨터는 각 픽셀이 가져야 하는 값을 모두 계산해야 한다. 여기에 쓰이는게 GPU다.<br />
<br />
CPU는 어려운 일이든 쉬운 일이든 한번에 하나밖에 못하지만, GPU는 동시에 여러 작업을 할 수 있다.<br />
CPU는 프로그램에 설명만 되어 있다면 복잡한 연산도 다 해주는데, 그런 CPU를 그래픽 연산에 쓰는건 아주 비효율적인 일이다.<br />
그래픽 연산은 계산량이 많지만, 복잡한 계산을 필요로 하지는 않는다.<br />
<br />
GPU는 코어 성능이 CPU보다 떨어지지만, 대신 코어를 몇천개씩 갖고 있다.<br />
CPU의 ALU가 요리사 4명이면, GPU의 ALU는 알바 5천명이다.<br />
<br />
clock에도 차이가 있다. CPU clock은 4GHz정도 되는데, GPU clock은 1.5~2GHz정도 된다.<br />
<br />
GPU에 들어가는 단순 연산용 ALU는 Stream Processor라는 애들이다.<br />
<br />
GPGPU: General-Purpose computing on GPU<br />
<br />
GPU가 코인 채굴에 사용되면서 인기를 많이 끌었다.<br />
코인 채굴은 거래 기록을 사슬처럼 이어붙이는 단순 작업이라 GPU가 많이 쓰였다.<br />
<br />
이렇게, GPU를 여러 작업들에 활용하는 기술을 GPGPU라 부른다. 요즘은 인공지능 연산에 GPU가 많이 쓰인다.<br />
(사진)<br />
GPU는 이렇게, 제어 유닛과 메모리 유닛이 수많은 ALU들을 보조하는 형태로 구성되어 있다.<br />
<br />
2021년 기준, GPU 시장의 56%를 엔비디아가, 26%를 암드가, 나머지 18%를 인텔이 점유하고 있다. *인텔 GPU가 있나?<br />
<br />
AP, SoC:<br />
2000년대 후반, 반도체 산업을 다시 한번 부흥시키는 제품이 등장한다. 스마트폰이다.<br />
스마트폰에는 AP(Application Processor)가 들어있어서, 소프트웨어를 설치하고 실행할 수 있다.<br />
<br />
컴퓨터 메인보드를 보면 CPU, GPU, 통신모뎀, RAM, ROM, 전력관리, 오디오, I/O 장치들이 장착되어 있다. 이런걸 다 핸드폰에 넣기 위해 하나의 IC로 만든게 AP다. AP처럼 여러 부품들이 하나에 들어가면 이걸 SoC(System on Chip)이라 부른다.<br />
(사진)<br />
AP시장에 가장 먼저 뛰어든건 삼성전자, 퀄컴, TI, Marvell이었다. 애플 아이폰, 아이팟 터치에도 삼성전자의 AP가 들어갔다. 그러다가 TI와 Marvell이 AP시장에서 철수했다.<br />
<br />
이때쯤 재정난을 겪던 AMD는 모바일 그래픽 사업부를 퀄컴에 매각했다. 퀄컴은 그 뒤 모바일 그래픽 분야에서도 강점을 보이며 계속 잘나갔다.<br />
<br />
초기 스마트폰 시장은 고성능 위주였으나, 시장이 포화된 후에는 중저가 스마트폰 시장이 열렸다. 대만의 MediatTek이 중저가 스마트폰 시장을 잘 파고들었다.<br />
<br />
2010년 이후로는 스마트폰 제조사들이 AP를 직접 설계하기 시작했다.<br />
애플은 2010년, 반도체 설계 업체 Intrinsity를 인수하고, 삼성전자의 AP를 버리고 직접 AP를 만들기 시작한다.<br />
<br />
화웨이도 스마트폰에 들어가는 AP를 그들의 자회사인 HiSilicon에서 만든다. 결국 삼성전자, 애플, 화웨이 모두 AP를 직접 만들고 있다.<br />
<br />
아니 컴퓨터 제조사들은 CPU, RAM, ROM 직접 안만드는데 왜 스마트폰 제조사들은 AP를 직접 만드는건가?<br />
1. 제품 출시 일정이 AP회사 일정에 끌려다녀서.<br />
2. 넣고 싶은 기능 맘대로 넣고싶어서.<br />
<br />
현재 AP는 저전력 동작을 위해 많이 노력하고 있다.<br />
<br />
2022년 1분기, AP시장 점유율은 미디어텍 38%, 퀄컴 30%, 애플 15%, 유니 SoC 11%, 삼성전자 5%, HiSilicon 1%<br />
<br />
NPU:<br />
Machine Learning은 인공지능의 한 분야고, Deep Learning은 Machine Learning에서 활용되는 알고리즘이다.<br />
<br />
요즘 인공지능 반도체라고 NPU라는게 나오는데, 왜 GPU를 두고 NPU라는걸 새로 만든걸까?<br />
1. 내장 AI 알고리즘, 2. 데이터 전송 속도, 3. 가격<br /></p>

<p>GPGPU로는 연산만 가능하고, 연산 결과를 분석하려면 별도의 소프트웨어가 필요하다.
그리고 AI연산을 위한 고속 데이터 전송 구조(아키텍처)가 없어 속도가 느리다.</p>

<p>그래서, AI 알고리즘이 탑재되어있고, 대규모 병렬 연산 후 고속 데이터 전송까지 가능한 NPU가 만들어지게 된다.
AI 알고리즘이 회로로 구현되어 있어 속도가 빠르다.</p>

<p>가격 관점에서는, GPU가 갖고 있던 그래픽 처리 관련 하드웨어를 다 버려서 그만큼 경제적이 되었다.</p>

<p>NPU는 다수의 CPU가 한 칩에 탑재된 구조다. 각 CPU는 독립적으로 연산을 처리하면서도 서로의 연산 결과에 영향을 준다.</p>

<p>NPU 만드는 기업으로는 애플, 화웨이, 삼성전자, 퀄컴 등이 있다.
다들 AP 만드는 업체들인데, 지들 AP 안에 NPU를 넣어 성능을 테스트함과 도시에 데이터를 수집하고 있다.</p>

<p>시리, 빅스비, 구글 어시스턴트 모두 AP에 NPU 들어간 후 성능이 향상됐고,
카메라 자동보정도 NPU 도입 이후 성능이 좋아졌다.</p>

<p>FPGA:
FPGA에는 코어의 구조를 원하는 대로 프로그래밍할 수 있는 프로그래머블 코어가 쓰인다.
그래서 코어의 구조를 연산에 맞게 최적화된 형태로 만들 수 있다.</p>

<p>코어가 연산에 맞게 최적화되면 명령어 해석 단계가 짧아진다.
그래서 특정 연산에서 FPGA가 CPU보다 빠를 수 있다.</p>

<p>FPGA는 특정 용도로 사용하기 위한 반도체를 개발할 때, 최적의 설계를 찾기 위해 반도체 개발자들이 많이 사용한다.</p>

<p>하드웨어 가속: 소프트웨어를 거치지 않고 하드웨어에서 바로 처리하는 것</p>

<p>ASIC:
FPGA를 통해 특정 상황에 최적화된 반도체를 설계했다면, 그 형태를 고정해서 대량생산하는 것으로 제작 단가를 낮출 수 있다.
이런 방식으로 만든 반도체가 ASIC다.</p>

<p>ASIC는 구성만 보면 CPU랑 비슷해 보이지만, 특정 연산만 수행하도록 만들어졌다는 차이가 있다.</p>

<p>ASIC는 특화된 기능만 수행하기에, 동작 속도가 빠르고, 크기가 작고, 소비전력이 적다.
물론 CPU에 비해 범용성은 매우 떨어진다.</p>

<p>그래서 ASIC가 코인 채굴에 많이 쓰였다.</p>

<p>2021년 기준으로, FPGA 시장은 약 8조원(69.6억 USD)규모로 전체 시스템 반도체 시장의 2.3%를 차지했다.</p>

<p>FPGA 1위 기업은 점유율 50%를 먹은 Xilinx고, 2위는 업계 2위였던 Altera를 2015년 인수한 인텔로, 30~40% 점유율을 차지하고 있다.</p>

<p>그리고 2020년에는 AMD가 Xilinx를 인수했다.</p>

<p>ASIC는 어떤 기업이든 지들 필요한 대로 알아서 만들어 쓰는거라, 대표 기업을 말하기 어렵다.</p>

<p>AI반도체는 현재 1세대, 2세대, 3세대로 구분한다.
1세대: GPU 병렬연산능력 + CPU, 소프트웨어로 AI알고리즘 구현
2세대: GPU에서 필요없는 기능 빼고, 하드웨어로 AI 알고리즘 구현
3세대: 뉴로모픽(Neuromorphic) 반도체</p>

<p>CPU는 데이터들을 순차적으로 처리하는데, 이러면 2차원 정보에 대한 패턴 분석이나, 실시간 데이터 처리같은 영역에서 데이터 처리 지연이 발생한다.</p>

<p>그래서 신경망을 모방해, 메모리 기록과 데이터 연산을 동시에 진행하는 뉴로모픽 반도체가 연구되고 있다.</p>

<p>최근에는 memristor를 뉴로모픽 반도체에 적용해 소비전력을 획기적으로 줄이려고 하고 있다.</p>

<p>memristor는 여기서는 메모리 특성을 갖는 저항 소자를 말한다.</p>

<p>FPGA = Field Programmable Gate Array
ASIC = Application-Specific IC</p>

<p>이미지 센서에는 크게 2가지 방식이 있다.
1. CCD(Charge Coupled Device)
2. CIS(CMOS Image Sensor)</p>

<p>CCD:
초반에 많이 쓰였다. Noise가 적어 초기 디지털카메라 등에 쓰였다.
하지만 화소를 고화소로 집적하거나 주변 회로들을 만드는게 어려웠다.
그리고, 신호 처리 방식이 아날로그라 전력을 많이 소비했다.</p>

<p>CIS:
반도체 만들던대로 만들면 돼서, 화소 집적의 어려움과 소비 전력을 해결할 수 있었고, 부가 회로들도 하나의 칩에 같이 만들 수 있었다.
단점은 noise였는데, 이건 센서 기술과 소프트웨어 기반 이미지 후처리 기술을 통해 보완했다.
결국 CIS가 CCD를 제치고 시장을 먹었다.</p>

<p>CIS 개발은 미국이 주도했지만, CCD를 많이 하던 일본이 CIS 분야까지 먹어버렸다.
소니, 캐논, 니콘 등이 자사 카메라에 활용하기 위한 이미지센서 개발을 위해 노력했다.</p>

<p>스마트폰 이전에는, 핸드폰 카메라가 30만~300만 화소면 충분했다.
근데 이제는 화소도 훨씬 많아졌고, 카메라도 핸드폰 하나에 4개씩 들어가는 시대가 됐다.
그래서 CIS가 많이 필요해졌다.</p>

<p>삼성전자는 2016년 디지털카메라 생산 중단, 2017년 판매 중단으로 디지털카메라를 손절하고, 그 기술을 스마트폰 카메라에 적용했다.</p>

<p>고화소 사진을 찍으려면 핸드폰 안에 더 넓은 공간이 필요하다.
즉 이미지센서의 크기가 커져야 한다.
근데 이러면 카메라가 튀어나온 디자인으로 이어진다.</p>

<p>Small pixel 기술을 통해, 작은 이미지센서로 많은 화소를 집적할 수 있다.</p>

<p>DSP:
Digital Signal Processor다. ADC, ALU, DAC의 3개 단계로 이루어져 잇다. 통신용 DSP, 음성용 DSP 등 종류가 많은데, 그중에서 이미지용 DSP는 ISP라 부른다.
ISP = Image Signal Processor.</p>

<p>DDI:
Display Driving IC. 디스플레이 구현을 위해 사용하는 회로라서, 디스플레이가 있는 모든 곳에 들어간다.</p>

<p>GPU가 디스플레이에 들어갈 화면을 디지털 신호로 계산하면, DDI는 그 디지털 신호를 아날로그 신호로 바꿔 디스플레이 서브픽셀에 전달한다.
즉, GPU와 디스플레이를 연결하는 역할을 하는게 DDI다.</p>

<p>서브픽셀이 많을수록 DDI도 많이 필요해서, 스마트폰에는 DDI 1개, FHD 모니터에는 6개, 4k TV에는 최대 48개가 들어간다.</p>

<p>PMIC:
Power Management IC. 안정적인 전압을 공급하는 데에 쓰인다.</p>

<p>컴퓨터 내 부품들은 다들 다른 전압을 필요로 한다.
CPU, GPU, DRAM은 1~1.1V, 통신 모듈은 3.3~5V, NAND플래시메모리는 10~15V를 필요로 한다.</p>

<p>콘센트에서 220V를 받아서, 그 전압이 잘 변환되어 각 제품에 들어가게 하는게 PMIC다.</p>

<p>MCU: Microcontroller Unit이다. 간단한 기능을 위한 컴퓨터같은 거다.
MCU에는 낮은 사양의 CPU, I/O port, 메모리, 통신 장치가 들어간다.</p>

<p>MCU가 요즘 많이 쓰이는 곳이 차량용 반도체 분야다. CPU가 각 장치들에 명령을 내리면, 장치마다 붙어있는 MCU가 해당 명령을 수행한다.</p>

<p>인텔: 컴퓨터용이니까, 저전력보다 고성능
ARM: 핸드폰용이니까, 고성능보다 저전력</p>

<p>RAM:
Random Access Memory
순차접근과 임의접근이 있는데, 임의접근(Random Access)이면 정보가 어느 셀에 있든 불러오기까지 시간이 똑같이 걸린다.</p>

<p>RAM에는 SRAM(Static RAM)과 DRAM(Dynamic RAM)이 있다.
둘 다 전원 꺼지면 데이터가 날아가지만, 회로 구성과 동작 속도에 차이가 있다.</p>

<p>SRAM은 트랜지스터 6개로 구성되어 있고, DRAM은 트랜지스터 1개와 캐패시터 1개로 구성되어 있다.</p>

<p>DRAM은 bit를 저장하기 위한 메모리 셀의 회로 구성이 단순해서, SRAM에 비해 큰 용량을 저렴하게 만들 수 있다.
SRAM은 비싸긴 하지만, DRAM보다 100배 이상 빠르다.
그래서 SRAM은 꼭 필요한 용량만 만들어 CPU 캐시메모리로 쓴다.</p>

<p>DRAM:
MOSFET을 켜면 캐패시터에 전하가 충전되고, 끄면 전하가 거기 저장된다.</p>

<p>근데, 캐패시터에 저장된 전하는 시간이 지나면 조금씩 새어나간다
그래서 주기적으로 전하를 채워주는 Refresh 작업이 필요하다.</p>

<p>Refresh 작업때문에 ‘Dynamic’ RAM이라는 이름이 붙었다.
SRAM은 Refresh가 필요 없어서 ‘Static’ RAM이다.</p>

<p>SDR: Single Data Rate. Rising edge에서만 데이터 송신
DDR: Dual Data Rate. Rising/Falling edge 모두에서 데이터 송신</p>

<p>당연히, SDR 방식보다 DDR 방식이 2배 빠르다.
그래서 지금 나오는 DRAM들은 모두 DDR 방식을 쓰고 있다
DDR 방식도 하나만 있는게 아니고, DDR1부터 DDR5까지 있다.</p>

<p>DDR 버전이 올라갈수록 데이터 처리 속도는 빨라지고, 전력소모는 줄어든다.
보통 버전이 하나 올라가면 2배정도 빨라진다.</p>

<p>ROM은 원래 읽기만 가능한 메모리라는 뜻이지만, 요즘은 그냥 기억장치 뜻하는 말로 쓰인다.
천공테이프 -&gt; 자기테이프 -&gt; HDD -&gt; SSD</p>

<p>HDD = Hard Disk Drive,
SSD = Solid State Drive</p>

<p>SSD에는 NAND 플래시 메모리가 쓰인다. 플래시메모리는 Floating gate라는 구조를 통해 데이터를 저장한다.
Refresh는 필요 없지만, 속도는 DRAM보다 훨씬 느리다.</p>

<p>플래시메모리 셀들을 어떻게 연결하는지에 따라 NAND 플래시 메모리와 NOR 플래시 메모리로 분류된다.</p>

<p>NAND플래시: 셀들을 직렬로 연결해서 String을 만들고, 그 String들을 병렬로 연결
NOR플래시: 셀들을 그냥 병렬로 연결</p>

<p>읽는 속도: NOR &gt; NAND
NAND플래시에서는 몇번 String의 몇번 Cell에서 읽어와라
NOR플래시에서는 그냥 몇번 Cell 읽어와라
라서 NOR가 더 빠르다.</p>

<p>쓰는 속도: NAND &gt; NOR
NAND플래시에서는 이 string에 데이터 저장. 하고 끝낼 수 있는데,
NOR플래시에서는 일일이 어떤어떤cell에 데이터 저장할지 정해줘야 한다.</p>

<p>근데 왜 NAND플래시를 많이 쓰나?
String으로 연결하는게 더 집적하기 쉬워서, 결국 같은 용량이면 NAND플래시가 더 저렴하다.</p>

<p>NAND플래시는 용량을 위해 위로 쌓는다.
2013년에 삼성전자가 최초로 24단을 쌓으며 3D 플래시메모리가 시작됐다.
지금은 200단도 넘게 쌓는다.</p>

<p>WDC: Western Digital Corporation
Solidigm: NAND플래시 하는 하이닉스 자회사</p>

<p>레지스터 - 캐시메모리 - RAM - ROM
왼쪽으로 갈수록 비싸고, 작고, 빠르다.</p>

<p>메모리반도체 치킨게임은 3개정도를 든다.
1.
1971년, IBM에서 발명한 DRAM을 인텔이 상용화해 큰 돈을 번다.
근데 1973년 1차 오일쇼크로 경제가 박살나자 미국 반도체 기업들이 투자를 축소했고, 일본 기업들은 그 사이에 공장에 크게 투자해 생산단가를 낮춘다.</p>

<p>일본 기업들이 공격적으로 가격을 내려서, 인텔은 시장 점유율도 잃고 적자도 1년만에 2억달러정도 내면서 메모리 사업을 접는다.</p>

<p>일본이 이긴 데에는 일본 정부의 보조금, 엔저로 인한 수출 경쟁력 등이 있었다.</p>

<p>2.
1986년, 미일 반도체협정으로 일본이 견제당하는 동안 한국 기업들이 커지기 시작했다.</p>

<p>80년대부터 PC가 보급되기 시작했는데, 일본 DRAM 품질이 좋긴 했지만 삼성전자 DRAM이 훨씬 싸서 훨씬 많이 팔렸다.</p>

<p>일본 생각: 고품질 제품이 수요를 창출한다.
한국 생각: 중간품질 싸게 찍어내자.</p>

<p>PC가 보급되기 시작하던 시대에, 일본보다는 한국 생각이 더 적합했다. 그래서 삼성전자가 돈을 꽤 벌었다.</p>

<p>그리고, 삼성전자는 2번의 결정으로 큰 이득을 보게 됐다.</p>

<p>1. Stack 기술 채택.
1M DRAM까지는 DRAM Cell들을 하나의 평면에 넣을 수 있었지만, 4M DRAM부터는 여러 층으로 Cell을 쌓아야 했다.</p>

<p>Cell을 쌓는 데에는 2가지 방법이 있었다.
웨이퍼를 파내서 층을 만드는 Trench 방식과, 웨이퍼 위에 층을 쌓는 Stack 방식이 있었다.</p>

<p>Trench 방식을 쓰면 크기가 작은 고성능 반도체 소자를 만들 수 있었지만, 공정이 까다롭고 불량 분석이 어려웠다.
Stack 방식은 소자 성능은 떨어졌지만, 생산이 용이하고 불량분석이 쉬웠다.</p>

<p>Trench 방식 : NEC, 도시바, TI, IBM, 현대전자, 금성반도체
Stack 방식: 히타치, 미쓰비시, 마츠시타, 삼성전자</p>

<p>Trench 방식은 4M DRAM 수율이 많이 떨어졌기 때문에, 결국 승자는 stack 방식을 채택한 회사들이었다.
그렇게, 삼성전자는 3년간의 적자를 1988년 단 1년의 이익으로 복구한다.</p>

<p>2. 200mm 웨이퍼 공정으로 전환(8인치 웨이퍼)
1990년대 초에는 두번째 결정을 한다.
당시 업계 표준은 6인치(150mm)웨이퍼였다. 삼성전자는 엄청난 리스크를 감수하고 8인치 웨이퍼 공정을 시작했는데, 이게 큰 성공을 거둬 삼성전자가 호황을 누리게 된다.</p>

<p>삼성전자는 16M DRAM 부터는 해외 기업들에 밀리지 않는 속도로 개발했고, 1994년에는 256M DRAM을, 1996년에는 1G DRAM을 세계 최초로 개발했다.</p>

<p>그러나 90년대 후반이 되어 IMF 외환위기가 온다. 한국경제는 박살이 났고, 정부의 구조조정 명령으로 현대전자와 금성반도체가 합병해 하이닉스가 된다.</p>

<p>일본도 적자 누적때문에 구조조정에 들어간다. NEC, 히타치 메모리반도체 부문, 미츠비시 DRAM부문이 합쳐져 Elpida가 된다.</p>

<p>한국이 휘청거리는 동안, 대만 기업들이 메모리 반도체 시장에 진입한다.
그리고 2007년부터 2번째 메모리반도체 치킨게임이 시작됐는데, 하필 2008년에 금융위기가 터져서 DRAM 가격이 치킨게임+금융위기로 박살난다.</p>

<p>512M DRAM은 2006년에 6.8달러, 2009년에 0.5달러였다.</p>

<p>두번째 치킨게임 결과, 대만 기업들(난야, 이노테라)은 메모리반도체 시장에서 철수한다.
독일의 Qimonda는 파산했고, 일본의 Elpida는 정부지원+은행대출 2조원으로 연명한다.
결국 삼성전자, 하이닉스, 마이크론이 살아남았다고 보면 된다.</p>

<p>3.
2번째 치킨게임에서 연명하는 정도였던 대만과 일본의 반도체 기업들은 남은 돈을 모두 공장 증설에 털어넣고 3번째 치킨게임을 건다.
삼성전자 하이닉스도 얘네 아주 조져버리려고 DRAM 생산량을 늘린다.</p>

<p>2010년 5월에는 1G DRAM이 2.7달러였는데, 12월에는 1달러 아래로 떨어졌다.</p>

<p>엘피다는 이때 누적적자 6조원을 찍고 파산하며, 이후 마이크론에 인수된다.</p>

<p>대만 기업들도 고용량 고성능 DRAM을 완전히 포기하고, 가전제품 등에 쓰이는 저용량 DRAM으로 도망간다.</p>

<p>이 3번째 치킨게임에서 살아남은 회사들이 삼성전자, 하이닉스, 마이크론이고, 이 회사들이 10년 넘게 해먹고 있다.</p>

<p>그리고, 메모리 반도체 기업들은 이 과정에서 만들어진 DRAM 공장들 중 오래된 곳을 시스템반도체 생산에 활용하기 시작한다.</p>

<p>메모리반도체 사이클:
일단 시스템반도체는 종류가 다양하고, 사용처가 정해져 있다.
PC에는 CPU, 스마트폰에는 AP, 채굴/인공지능에는 GPU, 전력관리에는 PMIC 이런 식으로 다 정해져 있다.
그리고 생산 전에 사용처와 계약을 먼저 체결하고, 사용처가 원하는 대로 제품을 만들어준다.</p>

<p>근데, 메모리반도체는 DRAM, NAND플래시 두가지밖에 없고, 규격화도 되어 있어 아무거나 끼워 쓰면 된다.
그래서 만드는 쪽에서도 일단 생산을 한다. 그 다음에 어떻게든 파는 식이다.
그렇기에 많은 재고를 안고 가게 되고, 가격 변동성에 크게 노출된다.
이게 메모리반도체 사이클의 원인이다.</p>

<p>시스템반도체의 경쟁력: 특화기업
메모리반도체의 경쟁력: 규모의 경제</p>

<p>SSD 안에는 NAND플래시 메모리, DRAM, 컨트롤러가 들어간다. NAND플래시가 DRAM보다 훨씬 느리기 때문에, 외부에서 들어온 데이터는 일단 DRAM에 저장된 후 NAND플래시에 저장된다.</p>

<p>그래서, SSD 안의 NAND플래시와 DRAM을 제어해줄 시스템반도체가 필요하다. 이걸 컨트롤러라고 부른다.
컨트롤러는 단순 제어 뿐 아니라 NAND플래시 메모리 셀에 발생하는 오류를 없애고, 셀을 골고루 사용해서 특정 셀만 수명이 깎이는 일이 없도록 한다.</p>

<p>그래서, 컨트롤러가 뛰어난 알고리즘으로 작동해야 SSD의 성능과 수명이 개선된다.</p>

<p>시스템반도체는 사실 우리나라에서만 쓰는 용어다. 해외에서는 로직 반도체라고 부른다.</p>

<p>IP기업: 특허를 제공하고 돈을 받는 기업이다. chipless기업이라고도 부른다. ARM, Synopsys, Cadence등.</p>

<p>ARM은 저전력 코어 설계도 특허를 팔고,
Synopsys, Cadence는 EDA(Electronic Design Automation) tool을 판다.</p>

<p>Fabless 기업은 설계, 판매, 유통은 하지만 제조는 외주맡긴다.
퀄컴, 미디어텍, 엔비디아, AMD, 애플 등</p>

<p>Design House 기업: 보통 디자인 하우스는 하나의 파운드리랑만 일한다.
팹리스 기업에서 준 설계도를 공정에 맞게 최적화하는 일을 한다.
파운드리들이 기밀 유출을 꺼려서 여러 파운드리랑은 일하지 않는 것으로 보인다.</p>

<p>파운드리 기업에는 TSMC, 삼성전자, UMC, 글로벌파운드리즈, SMIC, 인텔 등이 있다.</p>

<p>IDM(Integrated Device Manufacturer)기업:
자체 설계 자체 생산. 인텔 말고는 대부분 메모리반도체 기업이다.
삼성전자, 인텔, 하이닉스, 마이크론, 키오시아, WDC 등.</p>

<p>OSAT(Outsourced Semiconductor Assembly &amp; Test)기업:
테스트랑 패키징 해주는 회사. IDM이나 파운드리가 패키징이랑 테스트까지 할 때도 있지만,
기술이 부족할 경우 OSAT에 외주를 맡긴다.</p>

<p>OSAT 기업으로는 ASE, 앰코 테크놀로지, JCET, SPIL, 파워텍 등이 있다.</p>

<p>파운드리 기업은 디자인 하우스에게 PDK(Process Design Kit)를 제공한다.
PDK에는 생산되는 반도체의 특성과 설계 방법 등이 적혀 있어, 디자인 하우스가 파운드리 기업의 공정 스펙을 알 수 있다.</p>

<p>디자인 하우스 없이 진행하는 경우에는 파운드리 기업이 팹리스 기업에게 PDK를 제공하고, 팹리스 기업은 PDK에 기반한 설계도를 만들어 파운드리 기업에 보낸다.</p>

<p>TI: MCU, PMIC, 아날로그 또는 디지털 신호처리 반도체에 특화되어 있다. 공학용 계산기도 만든다.</p>

<p>키오시아: 도시바 NAND플래시 사업부가 분사되어 생긴 기업인 만큼, 만드는게 SSD 등 NAND플래시 쓰는 제품뿐이다.
키오시아도 나름 IDM이다.</p>

<p>차량용 반도체 big7:
Infineon, NXP, 르네사스, ST microelectronics, TI, Bosch, Onsemiconductor정도가 있다.</p>

<p>차량용 반도체로는 두뇌 역할을 하는 ECU(Electronic Control Unit), MCU, 센서(거리, 초음파, RADAR, LiDAR), 통신용 반도체 칩 등이 있다.</p>

<p>IP 기업들을 좀 알아보자.
ARM: AP, MCU Core IP (영국)
Synopsys: EDA, 반도체 IP (미국)
Cadence: EDA, 반도체 IP (미국)
Imagination Technologies: GPU IP (영국)
SST: 임베디드 메모리IP (미국)
Ceva: DSP IP (미국)
Verisilicon: DSP, 통신, NPU IP (중국)
Alphawave: AI, 통신 IP (영국)
eMemory Technology: 임베디드 메모리 IP (대만)
Rambus: 메모리 컨트롤러 IP (미국)</p>

<p>Rambus는 속도가 빠른 RDRAM(Rambus DRAM)의 IP와 DDR DRAM 컨트롤러 IP의 일부를 갖고 있다.
RDRAM 특허로 DRAM 기업들한테 맨날 시비털고 다니면서 ‘Patent Troll’이라는 별명도 얻었다.
이러다보니 메모리반도체 기업들이 빡쳐서 RDRAM을 쓰지 않게 됐고, DDR DRAM이 시장의 주류가 되었다.</p>

<p>그니까, IP 기업의 IP가 아무리 좋아도, 팹리스 기업과 IDM의 선택의 받지 못하면 망한다.</p>

<p>팹리스 기업:
Qualcomm: AP, 통신반도체 등 (미국)
NVDIA: GPU 등 (미국)
Broadcom: DSP, 통신반도체 등 (미국)
MediaTek: AP, DSP, 통신반도체 등 (대만)
AMD: GPU, CPU 등 (미국)
Novatek: DDI 등 (대만)
Marvell: 메모리컨트롤러, 통신반도체 (미국)
Realtek: 사운드, 네트워크 반도체 (대만)
Xilinx: FPGA (미국)
Himax : 디스플레이, ISP 등 (대만)</p>

<p>Broadcom: Ethernet, Wi-Fi, 블루투스 특화</p>

<p>Mediatek: 스마트폰 AP 만든다. 주요 제품으로는 Helios가 있다.</p>

<p>Realtek: 통신, 오디오 칩 외에도 IPTV용 SoC, SSD 컨트롤러, GloNass(위성항법장치 중 하나)의 수신 칩 등을 만든다.</p>

<p>애플도 설계 많이 한다.
스마트폰/태블릿에 들어가는 A시리즈 AP,
고성능 태블릿/PC에 들어가는 M 시리즈 CPU,
Wearable에 들어가는 S시리즈와 W시리즈를 직접 설계한다.</p>

<p>파운드리 기업:
TSMC, 삼성전자, UMC, Global Foundries, SMIC, DB하이텍
SMIC는 중국 정부가 지분을 갖고 있다.</p>

<p>반도체 칩 자체의 성능 향상이 물리적 한계에 가까워지며, OSAT 기업의 중요성이 커지고 있다.
ASE(대만), Amkor(미국), JCET(중국), SPIL(대만), Powertech(대만), TFME(중국), TSHT(중국), KYEC(대만), 칩모스(대만), 칩본드(대만)</p>

<p>OSAT 기업들 중에는 대만 기업들이 이렇게 많다.</p>

<p>Amkor의 전신은 한국의 아남반도체다. 2005년에 본사를 미국으로 옮기며 미국 기업이 되었다.</p>

<p>JCET은 중국에서 가장 큰 OSAT 기업이다.
싱가포르의 OSAT 기업인 STATS chipPAC을 인수하며 회사가 커졌다.</p>

<p>반도체 장비 기업:
AMAT: 증착 장비, CMP 장비, 에칭 장비(미국)
ASML: 노광장비(EUV, ArFi, ArF) (네덜란드)
Lam Research: 에칭 장비, 증착 장비, 세정 장비 (미국)
TEL: 에칭 장비, 증착 장비, 세정 장비 (일본)
KLA: 웨이퍼, 포토마스크, 패키지 등의 측정/분석 장비 (미국)
Advantest: 웨이퍼, 패키지 등의 테스트 장비 (일본)
SCREEN: 노광 공정 주변 장비 등 (일본)
Teradyne: 웨이퍼, 패키지 등의 테스트 장비들 (미국)
Hitachi: SEM(전자현미경), 에칭 장비 (일본)
ASM: 증착장비 등 (네덜란드)</p>

<p>국가별로 분류하면,
미국: 시스템반도체, 장비
일본: 소재, 부품
한국: 메모리반도체
대만: 파운드리, OSAT, 시스템반도체
중국: 파운드리, OSAT</p>

<p>Si, Ge처럼 한 종류 원소로 이루어진 반도체를 Elemental Semiconductor라 부르고,
두 종류 이상의 원소로 이루어진 반도체는 Compound Semiconductor라 부른다.</p>

<p>반도체 산업에서는 Si 기반 반도체를 쓰지만, 광학 반도체나 전력 반도체 분야에서는 Compound Semiconductor가 나을 때가 있다.</p>

<p>GaN은 LED에 쓰이고, SiC는 고전압 반도체에 쓰인다.
대표적인 Compound Semiconductor로는 GaAs, ZnSe 등이 있다.</p>

<p>반도체에 쓰이는 실리콘: Silicon
말랑말랑 실리콘: Silicone. 이건 R2SiO다. R = 탄화수소</p>

<p>즉, Silicone은 Si, C, H, O가 수없이 많이 연결되어있는 고분자(Polymer) 물질이다.</p>]]></content><author><name></name></author><category term="History" /><summary type="html"><![CDATA[진짜 하루만에 이해하는 반도체 산업 내용정리: 2극 진공관: 1905년 플레밍, 다이오드 역할 3극 진공관: 1907년 포리스트, 트랜지스터 역할 1946년 개발된 ENIAC에는 18000개의 진공관과 1500개의 기계식 스위치가 사용됐다. 무게는 30톤, 소비전력은 180kW 처음에는 반도체에 Ge를 썼는데, 노이스와 무어가 Ge 대신 Si를 쓰기 시작했다. Si는 구하기도 쉽고, 산소랑 반응시키면 SiO2라는 훌륭한 절연체가 된다. 킬비가 집적회로 처음 만들었을 때만 해도 Ge기반 반도체였고, 사람이 일일이 다 납땜해야 했다. 재료가 Si로 바뀌면서 배선 방식도 바뀌어 자동화가 가능해졌다. 1960년에는 MOSFET이 나왔다. MOSFET은 구조가 간단해 BJT 하나 들어갈 자리에 MOSFET 50~80개를 넣을 수 있었고, BJT에 비해 공정 단계가 40%정도 적어 만들기 쉬웠고, BJT보다 소비 전력이 낮았고, 스위칭 속도도 빨랐다. 적층 공정을 할 경우, 각 층을 전기적으로 연결하기 위한 수직 구멍인 via를 뚫는 공정이 추가돼서 가격이 올라간다. 칩 크기가 작아지면 한 웨이퍼에서 만들 수 있는 칩 수도 늘어나고, 웨이퍼에서 버려지는 부분도 줄어든다. Planar FET: 한 방향 전기장으로 leakage 차단 FinFET: 세 방향 전기장으로 leakage 차단 GAAFET: 네 방향 전기장으로 leakage 차단 컴퓨터에서 엑셀을 실행할 경우: 일단 엑셀 로고가 뜨며 로딩이 된다. 이 시간동안, CPU는 ROM에 있는 엑셀 프로그램 데이터 중 필요한 데이터들을 RAM으로 옮긴다. ROM은 속도가 느리기 때문에 옮겨서 쓰려는거다. 데이터가 RAM으로 다 옮겨지면 엑셀이 실행된다. 엑셀 칸에 1, 2 등을 적으면 이게 다 RAM에 저장되고, 수식을 넣어 계산시키면 CPU가 엑셀 프로그램 데이터에 적혀있는대로 계산해준다. 엑셀에서 저장 버튼을 누르면 RAM에 임시로 저장됐던 정보들이 다 ROM으로 보내진다. 그냥 끄면 ROM으로 안가고 날아가버린다. 그래서 RAM이 커야 프로그램을 여러 개 띄워둘 수 있다. 프로그램에 적힌 계산방식은 CPU가 다 실행해준다. ROM은 흔히 보는 C드라이브다. CPU는 크게 제어 유닛, ALU(Arithmetic Logic Unit), 아주 작은 용량의 메모리 유닛(캐시메모리, 레지스터)로 이루어져 있다. 제어유닛: 사용자의 명령을 해석하고, 각 유닛 사이 통신을 제어 ALU: 제어 유닛이 해석한 명령을 수행하는 연산 기능 메모리 유닛: 입력이나 출력 기억 (사진) CPU는 계산만 해주는 애고, 기억은 RAM에서 하는데 캐시메모리가 왜 필요한가? 앞에서 ROM이 너무 느리니 데이터를 RAM으로 옮겨 CPU와 작업시킨다고 했는데, 사실 CPU 입장에서는 RAM도 너무 느리다. 그래서 RAM에 있는 데이터를 RAM보다 더 빠른 캐시메모리와 레지스터로 옮겨 작업한다. 그럼 캐시메모리와 레지스터를 RAM 대신 쓰면 안되나? 그러기에는 캐시메모리와 레지스터가 용량 대비 가격이 너무 비싸다. 그래서 CPU에 넣을 때도 꼭 필요한 만큼만 넣는다. CPU의 성능은 Core, Thread, Clock, Architecture에 의해 결정된다. Core는 ALU의 개수, Thread는 각 ALU가 동시에 몇가지 작업을 처리할 수 있는지 알려준다. Clock은 초당 몇 개의 instruction이 실행되는지 보여주고, architecture는 cpu가 어떤 명령어 체계를 사용하고, 해석과 연산을 어떻게 분배할 것인지, CPU와 주변 부품들의 구성과 동작에 대한 구조를 의미한다. 그래서 같은 core, clock수를 갖고 있어도 architecture에 따라 CPU의 성능이 달라진다. CPU의 동작을 요리에 비유할 수 있다. ROM: 냉장고, RAM: 조리대, CPU: 요리사, 연산: 요리, 데이터: 식재료 냉장고에서 식재료를 꺼내 조리대에 올려놓는다. 요리사가 요리를 할거다. 이때 요리사의 수: Core 수, 각 요리사들의 팔 개수: Thread 수, 요리사의 요리 속도: clock 수 그래서 corethreadclock이 연산 속도를 보여주게 된다. Architecture: 주방 구조, 업무 체계 어떻게 보면 architecture가 가장 중요할 수도 있다. 요리사가 아무리 훌륭해도 업무 체계가 구데기면 일 못한다. 2021년 기준, CPU시장 점유율은 인텔 63%, 암드 37% 이번엔 GPU에 대해 알아보자. 그래픽은 픽셀들이 모여서 만들어지는데, 기술이 발전할수록 당연히 픽셀 수도 증가한다. 요즘 쓰이는 4k 모니터는 약 830만개 픽셀을 갖고 있으며, 각 픽셀이 2^24~=1678만개 색을 낼 수 있다. 우리가 모니터로 보는 화면은 수백만개의 픽셀이 저마다의 색깔을 아주 짧은 시간동안 만들어내면서 생긴다. 각 순간의 화면을 모니터에 구현하기 위해, 컴퓨터는 각 픽셀이 가져야 하는 값을 모두 계산해야 한다. 여기에 쓰이는게 GPU다. CPU는 어려운 일이든 쉬운 일이든 한번에 하나밖에 못하지만, GPU는 동시에 여러 작업을 할 수 있다. CPU는 프로그램에 설명만 되어 있다면 복잡한 연산도 다 해주는데, 그런 CPU를 그래픽 연산에 쓰는건 아주 비효율적인 일이다. 그래픽 연산은 계산량이 많지만, 복잡한 계산을 필요로 하지는 않는다. GPU는 코어 성능이 CPU보다 떨어지지만, 대신 코어를 몇천개씩 갖고 있다. CPU의 ALU가 요리사 4명이면, GPU의 ALU는 알바 5천명이다. clock에도 차이가 있다. CPU clock은 4GHz정도 되는데, GPU clock은 1.5~2GHz정도 된다. GPU에 들어가는 단순 연산용 ALU는 Stream Processor라는 애들이다. GPGPU: General-Purpose computing on GPU GPU가 코인 채굴에 사용되면서 인기를 많이 끌었다. 코인 채굴은 거래 기록을 사슬처럼 이어붙이는 단순 작업이라 GPU가 많이 쓰였다. 이렇게, GPU를 여러 작업들에 활용하는 기술을 GPGPU라 부른다. 요즘은 인공지능 연산에 GPU가 많이 쓰인다. (사진) GPU는 이렇게, 제어 유닛과 메모리 유닛이 수많은 ALU들을 보조하는 형태로 구성되어 있다. 2021년 기준, GPU 시장의 56%를 엔비디아가, 26%를 암드가, 나머지 18%를 인텔이 점유하고 있다. *인텔 GPU가 있나? AP, SoC: 2000년대 후반, 반도체 산업을 다시 한번 부흥시키는 제품이 등장한다. 스마트폰이다. 스마트폰에는 AP(Application Processor)가 들어있어서, 소프트웨어를 설치하고 실행할 수 있다. 컴퓨터 메인보드를 보면 CPU, GPU, 통신모뎀, RAM, ROM, 전력관리, 오디오, I/O 장치들이 장착되어 있다. 이런걸 다 핸드폰에 넣기 위해 하나의 IC로 만든게 AP다. AP처럼 여러 부품들이 하나에 들어가면 이걸 SoC(System on Chip)이라 부른다. (사진) AP시장에 가장 먼저 뛰어든건 삼성전자, 퀄컴, TI, Marvell이었다. 애플 아이폰, 아이팟 터치에도 삼성전자의 AP가 들어갔다. 그러다가 TI와 Marvell이 AP시장에서 철수했다. 이때쯤 재정난을 겪던 AMD는 모바일 그래픽 사업부를 퀄컴에 매각했다. 퀄컴은 그 뒤 모바일 그래픽 분야에서도 강점을 보이며 계속 잘나갔다. 초기 스마트폰 시장은 고성능 위주였으나, 시장이 포화된 후에는 중저가 스마트폰 시장이 열렸다. 대만의 MediatTek이 중저가 스마트폰 시장을 잘 파고들었다. 2010년 이후로는 스마트폰 제조사들이 AP를 직접 설계하기 시작했다. 애플은 2010년, 반도체 설계 업체 Intrinsity를 인수하고, 삼성전자의 AP를 버리고 직접 AP를 만들기 시작한다. 화웨이도 스마트폰에 들어가는 AP를 그들의 자회사인 HiSilicon에서 만든다. 결국 삼성전자, 애플, 화웨이 모두 AP를 직접 만들고 있다. 아니 컴퓨터 제조사들은 CPU, RAM, ROM 직접 안만드는데 왜 스마트폰 제조사들은 AP를 직접 만드는건가? 1. 제품 출시 일정이 AP회사 일정에 끌려다녀서. 2. 넣고 싶은 기능 맘대로 넣고싶어서. 현재 AP는 저전력 동작을 위해 많이 노력하고 있다. 2022년 1분기, AP시장 점유율은 미디어텍 38%, 퀄컴 30%, 애플 15%, 유니 SoC 11%, 삼성전자 5%, HiSilicon 1% NPU: Machine Learning은 인공지능의 한 분야고, Deep Learning은 Machine Learning에서 활용되는 알고리즘이다. 요즘 인공지능 반도체라고 NPU라는게 나오는데, 왜 GPU를 두고 NPU라는걸 새로 만든걸까? 1. 내장 AI 알고리즘, 2. 데이터 전송 속도, 3. 가격]]></summary></entry><entry><title type="html">스마트폰 - 카메라 - AF</title><link href="http://localhost:4000/market/2023/10/04/Market-AF.html" rel="alternate" type="text/html" title="스마트폰 - 카메라 - AF" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/market/2023/10/04/Market%20-%20AF</id><content type="html" xml:base="http://localhost:4000/market/2023/10/04/Market-AF.html"><![CDATA[<p>AF는 스마트폰 카메라에 적용되는 기술이다.<br />
<br />
<br />
<br />
카메라 초점을 잡는 방식에는 MF와 AF가 있다.<br />
MF = Manual Focus, 사용자가 직접 초점을 설정<br />
AF = Auto Focus, 카메라가 알아서 초점을 설정<br />
<br />
피쳐폰 시대에는 핸드폰 카메라에 AF기능이 굳이 들어가지 않았다.<br />
그러나 스마트폰 시대가 오면서, 아무리 저가형 핸드폰이라도 AF 기능이 필요하게 되었다.<br />
핸드폰 카메라로 QR코드, 바코드 등을 인식해야 하는 시대가 되었기 때문이다.<br />
<br />
<br />
초점을 조정하기 위해서는 렌즈를 앞뒤로 움직일 필요가 있다.<br />
AF 렌즈 제어에는 Open loop 방식과 Closed loop 방식이 있다.<br />
<br />
Open Loop 방식:<br />
코일에 전류를 인가해 렌즈를 움직인다.<br />
렌즈 이동 후 위치를 피드백받을 수가 없다.<br />
정밀한 제어는 어렵지만, 생산단가가 저렴하다.<br />
<br />
Closed Loop 방식:<br />
코일에 전류를 인가해 렌즈를 움직인다.<br />
렌즈 이동 후에는 Hall Sensor를 통해 렌즈 위치를 피드백받아 정밀하게 제어할 수 있다.<br />
생산단가는 비싸지만, 더 정밀한 제어가 가능하다.<br />
<br />
<img src="/public/img/AF.png" alt="alt text" /><br />
*VCM(Voice Coil Motor): 코일과 자석으로 이루어진 구조다. 전류가 인가되면 움직인다.<br />
<br />
<br />
AF 렌즈 제어를 실행하는 반도체 칩이 AF Driver IC다.<br />
Open Loop 방식으로 렌즈를 제어하는 AF Driver IC를 OLAF(Open-Loop AF) Driver IC,<br />
Closed Loop 방식으로 렌즈를 제어하는 AF Driver IC를 CLAF(Closed-Loop AF) Driver IC라 부른다.<br />
<br />
Driver IC 구조는 OLAF가 가장 간단하고 CLAF가 그 다음, OIS가 가장 복잡하다.<br />
Driver IC 수율은 OLAF가 가장 높고 CLAF가 그 다음, OIS가 가장 낮다.<br />
Driver IC 가격은 OLAF가 가장 낮고 CLAF가 그 다음, OIS가 가장 높다.<br />
Driver IC 마진은 OLAF가 가장 낮고 CLAF가 그 다음, OIS가 가장 높다.<br /></p>]]></content><author><name></name></author><category term="Market" /><summary type="html"><![CDATA[AF는 스마트폰 카메라에 적용되는 기술이다. 카메라 초점을 잡는 방식에는 MF와 AF가 있다. MF = Manual Focus, 사용자가 직접 초점을 설정 AF = Auto Focus, 카메라가 알아서 초점을 설정 피쳐폰 시대에는 핸드폰 카메라에 AF기능이 굳이 들어가지 않았다. 그러나 스마트폰 시대가 오면서, 아무리 저가형 핸드폰이라도 AF 기능이 필요하게 되었다. 핸드폰 카메라로 QR코드, 바코드 등을 인식해야 하는 시대가 되었기 때문이다. 초점을 조정하기 위해서는 렌즈를 앞뒤로 움직일 필요가 있다. AF 렌즈 제어에는 Open loop 방식과 Closed loop 방식이 있다. Open Loop 방식: 코일에 전류를 인가해 렌즈를 움직인다. 렌즈 이동 후 위치를 피드백받을 수가 없다. 정밀한 제어는 어렵지만, 생산단가가 저렴하다. Closed Loop 방식: 코일에 전류를 인가해 렌즈를 움직인다. 렌즈 이동 후에는 Hall Sensor를 통해 렌즈 위치를 피드백받아 정밀하게 제어할 수 있다. 생산단가는 비싸지만, 더 정밀한 제어가 가능하다. *VCM(Voice Coil Motor): 코일과 자석으로 이루어진 구조다. 전류가 인가되면 움직인다. AF 렌즈 제어를 실행하는 반도체 칩이 AF Driver IC다. Open Loop 방식으로 렌즈를 제어하는 AF Driver IC를 OLAF(Open-Loop AF) Driver IC, Closed Loop 방식으로 렌즈를 제어하는 AF Driver IC를 CLAF(Closed-Loop AF) Driver IC라 부른다. Driver IC 구조는 OLAF가 가장 간단하고 CLAF가 그 다음, OIS가 가장 복잡하다. Driver IC 수율은 OLAF가 가장 높고 CLAF가 그 다음, OIS가 가장 낮다. Driver IC 가격은 OLAF가 가장 낮고 CLAF가 그 다음, OIS가 가장 높다. Driver IC 마진은 OLAF가 가장 낮고 CLAF가 그 다음, OIS가 가장 높다.]]></summary></entry><entry><title type="html">스마트폰 - 햅틱</title><link href="http://localhost:4000/market/2023/10/04/Market-Haptic.html" rel="alternate" type="text/html" title="스마트폰 - 햅틱" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/market/2023/10/04/Market%20-%20Haptic</id><content type="html" xml:base="http://localhost:4000/market/2023/10/04/Market-Haptic.html"><![CDATA[<p>햅틱은 사용자가 어떤 행동을 하면, 진동을 통해 촉각을 느낄 수 있게 하는 기술이다.<br />
<br />
<br />
<br />
핸드폰 업계에서, 햅틱이란 단어는 삼성전자에서 2009년에 출시한 연아의 햅틱 시절부터 계속 등장해 왔다.<br />
<br />
스마트폰에서는 아이폰 7부터 누르는 홈버튼이 없어지며 햅틱 기능이 들어갔다.<br />
스마트폰 화면을 터치하면, 스마트폰이 진동해서 진짜로 버튼을 누른 느낌을 주는 방식이다.<br />
그 후 다른 스마트폰들에도 햅틱 기능이 들어가게 되었다.<br />
<br />
<br />
햅틱 시장은 점점 커지고 있으며, 스마트폰 외 다른 제품군에도 햅틱이 들어가고 있다.<br />
<br />
대표적으로 자동차 전장 분야가 있다.<br />
자동차 전장은 자동차 내부 전자 장치를 뜻한다.<br />
<br />
자동차 전장 분야에서는 자동차 내 버튼들을 터치스크린과 햅틱으로 대체하려는 움직임이 있다.<br />
자동차에서는 운전중 앞만 보며 화면을 터치해야 하기 때문에, 버튼이 눌렸으면 햅틱으로 그걸 확인시켜주는 것이 좋다.<br />
<br />
그 외에도 전자담배, 태블릿 펜 등 다양한 제품군에 햅틱이 활용된다.<br />
전자담배는 액상 끼우면 진동하는 식으로 동작하게 햅틱 기능을 넣고,<br />
태블릿 펜은 종이에 글씨 쓰는 느낌 나도록 햅틱을 넣는다.<br />
<br />
<br />
햅틱이 동작하기 위해서는 햅틱 드라이버 IC와 액추에이터가 필요하다.<br />
터치가 발생하면 그 정보가 햅틱 드라이버 IC로 전달되고, 햅틱 드라이버 IC는 그 정보대로 액추에이터를 동작시켜 진동이 발생한다.<br />
<br />
햅틱 드라이버 IC 업체들은 액추에이터를 어떻게 제어해야 진짜 버튼 눌렀을때랑 비슷한 느낌이 날 지를 연구한다.<br />
그래서, 햅틱 드라이버 IC 업체들은 액추에이터 업체들과 많은 협력을 한다.<br />
진짜 버튼을 눌렀을 때 생기는 가속도를 기록하고, 액추에이터가 똑같이 진동하도록 설계하는 등의 노력을 한다.<br />
<br />
<br />
햅틱 평가요소에는 소음, 진동시간, 진동력이 있다.<br />
소음은 적어야 하고, 진동시간은 짧아야 하고, 진동력은 적당해야 한다.<br />
<br />
진동시간이 짧아야 한다는 이야기는, 진동 후 잔진동이 남으면 안된다는 뜻이다.<br />
액추에이터 제어 기술을 통해 남은 진동을 잘 잡아줘야 한다.<br />
<br />
진동력이 적당해야 한다는 이야기는, 햅틱이 적용되는 제품에 따라 적당한 세기로 진동해야 한다는 뜻이다.<br />
당연히, 자동차에서는 핸드폰보다 햅틱 진동이 더 강해야 할 것이다.<br />
그래서 차에 쓰이는 액추에이터는 핸드폰에 들어가는 액추에이터보다 훨씬 크고, 가격도 비싸다.<br />
<br />
<br />
햅틱 시장에는 수많은 회사들이 참여하고 있지만, 대표적인 회사가 몇 곳 있다.<br />
<br />
햅틱의 원천 특허를 갖고 있는 회사는 미국의 이머전(Immersion)이라는 회사다.<br />
그래서, 햅틱이 들어간 제품을 만드는 회사들은 모두 이머전에게 돈을 내야 한다.<br />
<br />
현재 햅틱 기술이 가장 우수하다고 평가되는 회사는 애플이다.<br />
애플 제품들에 들어가는 햅틱 드라이버 IC는 시러스 로직(Cirrus Logic)에서 만든다.<br />
애플 제품들에 들어가는 햅틱 액추에이터는 애플이 직접 개발하며, Taptic Engine이라는 이름을 갖고 있다.<br />
애플의 Taptic Engine은 다른 회사들의 햅틱 액추에이터보다 훨씬 비싸다.<br />
<br />
<br />
햅틱의 진동 주파수는 300Hz정도 되는데, 회사마다 조금씩 차이가 있다.<br />
예를 들어 애플의 Taptic Engine은 진동시 150Hz -&gt; 300Hz -&gt; 180Hz로 진동 주파수가 변한다.<br />
햅틱도 소리랑 비슷하게, 나이가 들면 잘 못느끼게 된다.<br />
<br />
<br />
햅틱 시장은 분명 넓어지고 있지만, 모든 분야에서 햅틱이 환영받는 것은 아니다.<br />
예를 들어, 모바일 게임 회사들은 햅틱에 소극적인 경향이 있다.<br />
핸드폰마다 진동하는 방식이 다르니, 모든 핸드폰에서 같은 느낌을 줄 수가 없기 때문이다.<br /></p>]]></content><author><name></name></author><category term="Market" /><summary type="html"><![CDATA[햅틱은 사용자가 어떤 행동을 하면, 진동을 통해 촉각을 느낄 수 있게 하는 기술이다. 핸드폰 업계에서, 햅틱이란 단어는 삼성전자에서 2009년에 출시한 연아의 햅틱 시절부터 계속 등장해 왔다. 스마트폰에서는 아이폰 7부터 누르는 홈버튼이 없어지며 햅틱 기능이 들어갔다. 스마트폰 화면을 터치하면, 스마트폰이 진동해서 진짜로 버튼을 누른 느낌을 주는 방식이다. 그 후 다른 스마트폰들에도 햅틱 기능이 들어가게 되었다. 햅틱 시장은 점점 커지고 있으며, 스마트폰 외 다른 제품군에도 햅틱이 들어가고 있다. 대표적으로 자동차 전장 분야가 있다. 자동차 전장은 자동차 내부 전자 장치를 뜻한다. 자동차 전장 분야에서는 자동차 내 버튼들을 터치스크린과 햅틱으로 대체하려는 움직임이 있다. 자동차에서는 운전중 앞만 보며 화면을 터치해야 하기 때문에, 버튼이 눌렸으면 햅틱으로 그걸 확인시켜주는 것이 좋다. 그 외에도 전자담배, 태블릿 펜 등 다양한 제품군에 햅틱이 활용된다. 전자담배는 액상 끼우면 진동하는 식으로 동작하게 햅틱 기능을 넣고, 태블릿 펜은 종이에 글씨 쓰는 느낌 나도록 햅틱을 넣는다. 햅틱이 동작하기 위해서는 햅틱 드라이버 IC와 액추에이터가 필요하다. 터치가 발생하면 그 정보가 햅틱 드라이버 IC로 전달되고, 햅틱 드라이버 IC는 그 정보대로 액추에이터를 동작시켜 진동이 발생한다. 햅틱 드라이버 IC 업체들은 액추에이터를 어떻게 제어해야 진짜 버튼 눌렀을때랑 비슷한 느낌이 날 지를 연구한다. 그래서, 햅틱 드라이버 IC 업체들은 액추에이터 업체들과 많은 협력을 한다. 진짜 버튼을 눌렀을 때 생기는 가속도를 기록하고, 액추에이터가 똑같이 진동하도록 설계하는 등의 노력을 한다. 햅틱 평가요소에는 소음, 진동시간, 진동력이 있다. 소음은 적어야 하고, 진동시간은 짧아야 하고, 진동력은 적당해야 한다. 진동시간이 짧아야 한다는 이야기는, 진동 후 잔진동이 남으면 안된다는 뜻이다. 액추에이터 제어 기술을 통해 남은 진동을 잘 잡아줘야 한다. 진동력이 적당해야 한다는 이야기는, 햅틱이 적용되는 제품에 따라 적당한 세기로 진동해야 한다는 뜻이다. 당연히, 자동차에서는 핸드폰보다 햅틱 진동이 더 강해야 할 것이다. 그래서 차에 쓰이는 액추에이터는 핸드폰에 들어가는 액추에이터보다 훨씬 크고, 가격도 비싸다. 햅틱 시장에는 수많은 회사들이 참여하고 있지만, 대표적인 회사가 몇 곳 있다. 햅틱의 원천 특허를 갖고 있는 회사는 미국의 이머전(Immersion)이라는 회사다. 그래서, 햅틱이 들어간 제품을 만드는 회사들은 모두 이머전에게 돈을 내야 한다. 현재 햅틱 기술이 가장 우수하다고 평가되는 회사는 애플이다. 애플 제품들에 들어가는 햅틱 드라이버 IC는 시러스 로직(Cirrus Logic)에서 만든다. 애플 제품들에 들어가는 햅틱 액추에이터는 애플이 직접 개발하며, Taptic Engine이라는 이름을 갖고 있다. 애플의 Taptic Engine은 다른 회사들의 햅틱 액추에이터보다 훨씬 비싸다. 햅틱의 진동 주파수는 300Hz정도 되는데, 회사마다 조금씩 차이가 있다. 예를 들어 애플의 Taptic Engine은 진동시 150Hz -&gt; 300Hz -&gt; 180Hz로 진동 주파수가 변한다. 햅틱도 소리랑 비슷하게, 나이가 들면 잘 못느끼게 된다. 햅틱 시장은 분명 넓어지고 있지만, 모든 분야에서 햅틱이 환영받는 것은 아니다. 예를 들어, 모바일 게임 회사들은 햅틱에 소극적인 경향이 있다. 핸드폰마다 진동하는 방식이 다르니, 모든 핸드폰에서 같은 느낌을 줄 수가 없기 때문이다.]]></summary></entry><entry><title type="html">자동차 전장 - 카메라, 레이더, 라이다</title><link href="http://localhost:4000/market/2023/10/04/Market-LiDAR.html" rel="alternate" type="text/html" title="자동차 전장 - 카메라, 레이더, 라이다" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/market/2023/10/04/Market%20-%20LiDAR</id><content type="html" xml:base="http://localhost:4000/market/2023/10/04/Market-LiDAR.html"><![CDATA[<p>자율주행 자동차에서는 멀리 보기 위해 레이더, 고해상도를 위해 라이다, 색을 구분하기 위해 카메라를 사용한다.<br />
<br />
<br />
레이더의 장점:<br />
멀리 있는 물체를 인식할 수 있다.<br />
날씨의 영향을 적게 받는다.<br />
전파가 투과할 수 있는 물체라면, 그 뒤에 있는 물체까지 인식할 수 있다.<br />
<br />
레이더의 단점:<br />
작은 물체 식별이 불가능하다.<br />
최대 측정 가능 거리가 늘어나면 시야각이 좁아진다.<br />
<br />
<br />
라이다의 장점:<br />
작은 물체도 식별할 수 있다.<br />
정밀한 3D 형태 인식이 가능하다.<br />
<br />
라이다의 단점:<br />
탐지거리가 짧다.<br />
날씨에 많은 영향을 받는다.<br />
가격이 비싸다.<br />
<br />
<br />
카메라의 장점:<br />
색상을 인지할 수 있다.<br />
가격이 싸다.<br />
<br />
카메라의 단점:<br />
날씨에 많은 영향을 받는다.<br />
물체와의 거리를 알기 어렵다.<br />
<br />
<br />
단거리 물체 인식에는 라이다 외에도 초음파센서를 쓰기도 한다.<br />
장거리 물체 인식에는 레이더를 사용한다.<br />
<br />
<br />
레이더에서는 전파 주파수로 24GHz, 77GHz를 주로 사용한다.<br />
<br />
24GHz 레이더의 장점:<br />
부품이 저렴하다.<br />
QFN등 표준 플라스틱 패키지를 쓸 수 있다.<br />
Sampling rate가 비교적 낮아 baseband 처리가 용이하고, 활용이 쉽다.<br />
<br />
24GHz 레이더의 단점:<br />
큰 센서 안테나가 필요하다.<br />
대역폭이 좁아 해상도가 낮다.<br />
<br />
<br />
77GHz 레이더의 장점:<br />
작은 센서 안테나로 구현할 수 있다.<br />
대역폭이 넓어 해상도가 높다.<br />
<br />
77GHz 레이더의 단점:<br />
부품이 비싸다.<br />
parasitic inductance가 적은 eWLB(embedded Wafer Level Ball grid array) 패키지를 써야 한다.<br />
Sampling rate가 비교적 높아 baseband 처리가 힘들고, 활용이 어렵다.<br /></p>]]></content><author><name></name></author><category term="Market" /><summary type="html"><![CDATA[자율주행 자동차에서는 멀리 보기 위해 레이더, 고해상도를 위해 라이다, 색을 구분하기 위해 카메라를 사용한다. 레이더의 장점: 멀리 있는 물체를 인식할 수 있다. 날씨의 영향을 적게 받는다. 전파가 투과할 수 있는 물체라면, 그 뒤에 있는 물체까지 인식할 수 있다. 레이더의 단점: 작은 물체 식별이 불가능하다. 최대 측정 가능 거리가 늘어나면 시야각이 좁아진다. 라이다의 장점: 작은 물체도 식별할 수 있다. 정밀한 3D 형태 인식이 가능하다. 라이다의 단점: 탐지거리가 짧다. 날씨에 많은 영향을 받는다. 가격이 비싸다. 카메라의 장점: 색상을 인지할 수 있다. 가격이 싸다. 카메라의 단점: 날씨에 많은 영향을 받는다. 물체와의 거리를 알기 어렵다. 단거리 물체 인식에는 라이다 외에도 초음파센서를 쓰기도 한다. 장거리 물체 인식에는 레이더를 사용한다. 레이더에서는 전파 주파수로 24GHz, 77GHz를 주로 사용한다. 24GHz 레이더의 장점: 부품이 저렴하다. QFN등 표준 플라스틱 패키지를 쓸 수 있다. Sampling rate가 비교적 낮아 baseband 처리가 용이하고, 활용이 쉽다. 24GHz 레이더의 단점: 큰 센서 안테나가 필요하다. 대역폭이 좁아 해상도가 낮다. 77GHz 레이더의 장점: 작은 센서 안테나로 구현할 수 있다. 대역폭이 넓어 해상도가 높다. 77GHz 레이더의 단점: 부품이 비싸다. parasitic inductance가 적은 eWLB(embedded Wafer Level Ball grid array) 패키지를 써야 한다. Sampling rate가 비교적 높아 baseband 처리가 힘들고, 활용이 어렵다.]]></summary></entry><entry><title type="html">스마트폰 - 카메라 - OIS</title><link href="http://localhost:4000/market/2023/10/04/Market-OIS.html" rel="alternate" type="text/html" title="스마트폰 - 카메라 - OIS" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/market/2023/10/04/Market%20-%20OIS</id><content type="html" xml:base="http://localhost:4000/market/2023/10/04/Market-OIS.html"><![CDATA[<p>OIS는 스마트폰 카메라에 적용되는 기술이다.<br />
OIS = Optical Image Stabilization<br />
<br />
<br />
<br />
OIS는 외부 진동을 상쇄하는 방향으로 카메라 렌즈를 움직여 초점을 유지하고, 센서로 들어오는 광량을 유지하는 기술이다. 즉, 손떨림 보정 기술이다.<br />
OIS가 적용된 카메라에서는 손떨림의 영향이 없는 깨끗한 사진을 찍을 수 있다.<br />
<br />
카메라의 확대 배율이 커질수록 손떨림의 영향도 커진다.<br />
따라서, 스마트폰 카메라가 발전할수록 OIS도 함께 중요해지고 있다.<br />
<br />
<br />
OIS 렌즈 제어를 실행하는 반도체 칩이 OIS Driver IC다.<br />
OIS Driver IC는 자이로센서로 움직임을 파악해 렌즈를 움직인다.<br />
렌즈를 움직이는 방식에는 ball guide 방식과 spring 방식이 있다.<br />
<br />
Spring 방식:<br />
스프링이 렌즈를 움직이게 하는 방식이다.<br />
장점: 높은 경제성, 높은 생산성, 제어기 tuning의 용이함<br />
단점: 낮은 내구성(스프링이 늘어날 수 있음)<br />
<br />
Ball guide 방식:<br />
작은 구슬들이 렌즈를 움직이게 하는 방식이다.<br />
장점: 빠른 구동속도, 높은 정확성, 높은 내구성, 높은 배터리 효율<br />
단점: 제어기 tuning의 어려움<br />
<br />
<br />
렌즈를 움직이는 방식에는 4축제어와 5축제어가 있다.<br />
4축제어: xtilt, ytilt, xmove, ymove<br />
5축제어: 4축제어 + rotation<br />
<br />
근데 사실 4축제어나 5축제어나 성능에 큰 차이는 없다.<br />
손떨림이 회전하는 방향으로는 잘 안일어나기 때문이다.<br />
<br />
<br />
원래 OIS는 렌즈를 움직여서 손떨림을 보정하는 기술인데,<br />
스마트폰이 발전할수록 렌즈가 점점 무거워져서 렌즈를 움직이기 어려워졌다.<br />
<br />
결국 아이폰 12 pro max, 화웨이 P60 pro부터는 렌즈가 아닌 센서를 움직이게 되었다.<br />
이때, 렌즈 대신 센서를 움직이기 위해 핸드폰 생산 라인을 크게 바꿔야 했다고 한다.<br />
<br />
<br />
OIS는 대부분의 스마트폰에 들어갈 정도로 중요한 기술이지만, 단점도 존재한다.<br />
<br />
OIS의 단점:<br />
OIS Driver IC는 의도하지 않은 흔들림과 의도된 움직임을 구분할 수 없다.<br />
따라서, 의도적으로 카메라를 움직일 경우 반대방향으로 보정이 들어간다.<br />
결국, 화면이 느리게 따라온다.<br />
<br />
영상에서 OIS를 켜놓을 경우에는 더 이질적으로 보인다.<br />
그래서 영상을 촬영할 경우에는 OIS를 사용하지 않는다.<br />
<br />
<br />
손떨림 방지 기술이 OIS만 있는 것은 아니다. OIS 말고도 DIS, EIS가 있다.<br />
DIS(Digital Image Stabilization)은 흔들린 이미지를 소프트웨어로 보정하는 기술이다.<br />
EIS(Electrical image Stabilization)은 자이로센서로 흔들림을 인식하고, 그 정보를 반영해 소프트웨어로 이미지를 보정하는 방식이다.<br />
<br />
비용은 DIS가 가장 싸고 EIS가 그 다음, OIS가 가장 비싸다.<br />
품질은 OIS가 가장 좋고 EIS가 그 다음, DIS가 가장 안좋다.<br />
<br />
DIS를 쓰면 필요한 장치들이 줄어들어 하드웨어 비용을 아낄 수 있긴 하지만, 그만큼 소프트웨어, 고속 인터페이스 성능이 중요해진다.<br />
또한, DIS를 쓰기 위해서는 이미지 센서의 화소 수가 상당히 높아야 한다.<br /></p>]]></content><author><name></name></author><category term="Market" /><summary type="html"><![CDATA[OIS는 스마트폰 카메라에 적용되는 기술이다. OIS = Optical Image Stabilization OIS는 외부 진동을 상쇄하는 방향으로 카메라 렌즈를 움직여 초점을 유지하고, 센서로 들어오는 광량을 유지하는 기술이다. 즉, 손떨림 보정 기술이다. OIS가 적용된 카메라에서는 손떨림의 영향이 없는 깨끗한 사진을 찍을 수 있다. 카메라의 확대 배율이 커질수록 손떨림의 영향도 커진다. 따라서, 스마트폰 카메라가 발전할수록 OIS도 함께 중요해지고 있다. OIS 렌즈 제어를 실행하는 반도체 칩이 OIS Driver IC다. OIS Driver IC는 자이로센서로 움직임을 파악해 렌즈를 움직인다. 렌즈를 움직이는 방식에는 ball guide 방식과 spring 방식이 있다. Spring 방식: 스프링이 렌즈를 움직이게 하는 방식이다. 장점: 높은 경제성, 높은 생산성, 제어기 tuning의 용이함 단점: 낮은 내구성(스프링이 늘어날 수 있음) Ball guide 방식: 작은 구슬들이 렌즈를 움직이게 하는 방식이다. 장점: 빠른 구동속도, 높은 정확성, 높은 내구성, 높은 배터리 효율 단점: 제어기 tuning의 어려움 렌즈를 움직이는 방식에는 4축제어와 5축제어가 있다. 4축제어: xtilt, ytilt, xmove, ymove 5축제어: 4축제어 + rotation 근데 사실 4축제어나 5축제어나 성능에 큰 차이는 없다. 손떨림이 회전하는 방향으로는 잘 안일어나기 때문이다. 원래 OIS는 렌즈를 움직여서 손떨림을 보정하는 기술인데, 스마트폰이 발전할수록 렌즈가 점점 무거워져서 렌즈를 움직이기 어려워졌다. 결국 아이폰 12 pro max, 화웨이 P60 pro부터는 렌즈가 아닌 센서를 움직이게 되었다. 이때, 렌즈 대신 센서를 움직이기 위해 핸드폰 생산 라인을 크게 바꿔야 했다고 한다. OIS는 대부분의 스마트폰에 들어갈 정도로 중요한 기술이지만, 단점도 존재한다. OIS의 단점: OIS Driver IC는 의도하지 않은 흔들림과 의도된 움직임을 구분할 수 없다. 따라서, 의도적으로 카메라를 움직일 경우 반대방향으로 보정이 들어간다. 결국, 화면이 느리게 따라온다. 영상에서 OIS를 켜놓을 경우에는 더 이질적으로 보인다. 그래서 영상을 촬영할 경우에는 OIS를 사용하지 않는다. 손떨림 방지 기술이 OIS만 있는 것은 아니다. OIS 말고도 DIS, EIS가 있다. DIS(Digital Image Stabilization)은 흔들린 이미지를 소프트웨어로 보정하는 기술이다. EIS(Electrical image Stabilization)은 자이로센서로 흔들림을 인식하고, 그 정보를 반영해 소프트웨어로 이미지를 보정하는 방식이다. 비용은 DIS가 가장 싸고 EIS가 그 다음, OIS가 가장 비싸다. 품질은 OIS가 가장 좋고 EIS가 그 다음, DIS가 가장 안좋다. DIS를 쓰면 필요한 장치들이 줄어들어 하드웨어 비용을 아낄 수 있긴 하지만, 그만큼 소프트웨어, 고속 인터페이스 성능이 중요해진다. 또한, DIS를 쓰기 위해서는 이미지 센서의 화소 수가 상당히 높아야 한다.]]></summary></entry><entry><title type="html">스마트폰 - ToF</title><link href="http://localhost:4000/market/2023/10/04/Market-ToF.html" rel="alternate" type="text/html" title="스마트폰 - ToF" /><published>2023-10-04T19:31:29+09:00</published><updated>2023-10-04T19:31:29+09:00</updated><id>http://localhost:4000/market/2023/10/04/Market%20-%20ToF</id><content type="html" xml:base="http://localhost:4000/market/2023/10/04/Market-ToF.html"><![CDATA[<p>ToF(Time of Flight)는 3차원 공간 인식 방식 중 하나다.<br />
<br />
<br />
<br />
3차원 공간 인식 방식에는 3가지 대표적인 방법이 있다.<br />
<br />
Stereo Vision:<br />
카메라 2개로 3차원 공간을 인식한다.<br />
<br />
Structured Light:<br />
패턴 있는 빛을 뿌리고, 그 반사패턴을 통해 3차원 공간을 인식한다.<br />
<br />
ToF:<br />
여러 가닥의 레이저를 발사하고, 반사되어 돌아오는데에 걸리는 시간을 통해 3차원 공간을 인식한다.<br />
<br />
<br />
ToF 카메라가 다른 카메라들과 다른 점은, 카메라에 광원이 있다는 점이다.<br />
ToF 카메라의 광원은 VCSEL(Vertical Cavity Surface Emitting Laser, 웨이퍼 수직 방향으로 레이저 쏘는 다이오드)이다.<br />
ToF 드라이버 IC는 ToF 카메라의 VCSEL을 제어하는 IC다.<br />
<br />
<br />
ToF에는 2가지 방식이 있다.<br />
<br />
dToF(direct Time-of-Flight):<br />
몇 개 point에 강한 레이저를 발사한다.<br />
먼 거리까지 인식할 수 있지만, 중간에 비는 부분들은 알고리즘으로 채워넣어야 한다.<br />
<br />
dToF는 아이폰, 로봇청소기, 라이다 등에 쓰인다.<br />
<br />
애플 dToF는 600개 point에 레이저를 발사한다.<br />
애플은 아이폰 12 프로부터 스마트폰에 ToF 카메라를 넣고 있다.<br />
애플 안면인식이 dToF 센서로 3D 센싱을 하는 방식이다.<br />
<br />
<br />
iToF(indirect Time-of-Flight):<br />
여러개 point에 약한 레이저를 발사한다.<br />
먼 거리는 인식할 수 없지만, 발사하는 point가 많아 해상도가 높다.<br />
<br />
아이폰을 제외한 핸드폰 등에 쓰인다.<br />
<br />
삼성전자 iToF는 4500개 point에 레이저를 발사한다.<br />
삼성전자는 현재는 스마트폰 제품에 ToF 카메라를 넣지 않고 있다.<br />
<br />
<br />
iToF, dToF 쏘는건 물총 쏘는거랑 비슷하게 생각하면 된다.<br />
분무기가 iToF, 물총이 dToF다.<br />
<br />
iToF 센서와 dToF 센서는 다르게 생겼는데, 이 중 dToF 센서는 작게 만들기 어렵다는 특징이 있다.<br />
dToF 센서는 소니에서 만든다.<br />
<br />
dToF 방식은 알고리즘으로 빈칸을 채워야 해서 소프트웨어 처리가 많이 필요하다.<br />
그래서 이게 가능한 애플만 dToF를, 나머지는 iToF를 쓴다.<br />
<br />
<br />
dToF는 아이폰 외에도 형체만 인식하는 곳들에 쓰인다.<br />
매장 입구에 인원 파악을 위해 설치되기도 하는데, 이 경우 사람의 형체만 보이기 때문에 사생활을 보호할 수 있다.<br />
<br />
dToF는 스마트폰 외 제품군에도 적용된다.<br />
라이다의 핵심 원리가 dToF다.<br /></p>]]></content><author><name></name></author><category term="Market" /><summary type="html"><![CDATA[ToF(Time of Flight)는 3차원 공간 인식 방식 중 하나다. 3차원 공간 인식 방식에는 3가지 대표적인 방법이 있다. Stereo Vision: 카메라 2개로 3차원 공간을 인식한다. Structured Light: 패턴 있는 빛을 뿌리고, 그 반사패턴을 통해 3차원 공간을 인식한다. ToF: 여러 가닥의 레이저를 발사하고, 반사되어 돌아오는데에 걸리는 시간을 통해 3차원 공간을 인식한다. ToF 카메라가 다른 카메라들과 다른 점은, 카메라에 광원이 있다는 점이다. ToF 카메라의 광원은 VCSEL(Vertical Cavity Surface Emitting Laser, 웨이퍼 수직 방향으로 레이저 쏘는 다이오드)이다. ToF 드라이버 IC는 ToF 카메라의 VCSEL을 제어하는 IC다. ToF에는 2가지 방식이 있다. dToF(direct Time-of-Flight): 몇 개 point에 강한 레이저를 발사한다. 먼 거리까지 인식할 수 있지만, 중간에 비는 부분들은 알고리즘으로 채워넣어야 한다. dToF는 아이폰, 로봇청소기, 라이다 등에 쓰인다. 애플 dToF는 600개 point에 레이저를 발사한다. 애플은 아이폰 12 프로부터 스마트폰에 ToF 카메라를 넣고 있다. 애플 안면인식이 dToF 센서로 3D 센싱을 하는 방식이다. iToF(indirect Time-of-Flight): 여러개 point에 약한 레이저를 발사한다. 먼 거리는 인식할 수 없지만, 발사하는 point가 많아 해상도가 높다. 아이폰을 제외한 핸드폰 등에 쓰인다. 삼성전자 iToF는 4500개 point에 레이저를 발사한다. 삼성전자는 현재는 스마트폰 제품에 ToF 카메라를 넣지 않고 있다. iToF, dToF 쏘는건 물총 쏘는거랑 비슷하게 생각하면 된다. 분무기가 iToF, 물총이 dToF다. iToF 센서와 dToF 센서는 다르게 생겼는데, 이 중 dToF 센서는 작게 만들기 어렵다는 특징이 있다. dToF 센서는 소니에서 만든다. dToF 방식은 알고리즘으로 빈칸을 채워야 해서 소프트웨어 처리가 많이 필요하다. 그래서 이게 가능한 애플만 dToF를, 나머지는 iToF를 쓴다. dToF는 아이폰 외에도 형체만 인식하는 곳들에 쓰인다. 매장 입구에 인원 파악을 위해 설치되기도 하는데, 이 경우 사람의 형체만 보이기 때문에 사생활을 보호할 수 있다. dToF는 스마트폰 외 제품군에도 적용된다. 라이다의 핵심 원리가 dToF다.]]></summary></entry></feed>